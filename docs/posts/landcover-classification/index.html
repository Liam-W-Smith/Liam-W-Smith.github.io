<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Liam Smith">
<meta name="dcterms.date" content="2022-05-01">
<meta name="description" content="Comparing different image sources for object-oriented landcover classification.">

<title>High Resolution Landcover Classification – Liam Smith</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-03ce2e201f5a56ff0ffe1814f9cd2be0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #f2f2f2;
      }

      .quarto-title-block .quarto-title-banner {
        color: #f2f2f2;
background-image: url(../../img/panorama5.jpg);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Liam Smith</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../portfolio.html"> 
<span class="menu-text">Portfolio</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Liam-W-Smith"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/liam-smith-b159791b3/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">High Resolution Landcover Classification</h1>
                  <div>
        <div class="description">
          Comparing different image sources for object-oriented landcover classification.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ArcGIS Pro</div>
                <div class="quarto-category">Remote Sensing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Liam Smith </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 1, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The Lemon Fair Insect Control District is a municipal project of Bridport, Cornwall, and Weybridge seeking to control local mosquito populations. By applying larvicides in areas with high concentrations of mosquito eggs, the Lemon Fair Insect Control District reduces the presence of these pests and mitigates associated health risks. In this portfolio problem, we analyzed imagery of one section of the Lemon Fair River in order to assist the Lemon Fair Insect Control District with two tasks:</p>
<ol type="1">
<li>Identifying areas that ought to be targeted for mosquito prevention</li>
<li>Identifying the optimal source of imagery for this type of work</li>
</ol>
</section>
<section id="image-acquisition-comparison" class="level2">
<h2 class="anchored" data-anchor-id="image-acquisition-comparison">Image Acquisition &amp; Comparison</h2>
<p>In this portfolio problem, I comparatively analyzed 4 sources of imagery of the Lemon Fair River: aerial photos from Vermont’s Open Data Portal, satellite images taken by Planet Labs, drone footage taken in 2021, and drone footage taken in 2022.</p>
<p>I first downloaded images from VT Open Data Portal’s website. Using the Vermont Orthoimagery Finder, I found the most recent high resolution color imagery taken in April. On the main page, I selected the appropriate dataset and downloaded 6 adjacent images taken in April 2017, cross-referencing with the provided shapefile of the study area to ensure that the images covered the entire region.</p>
<p>Next, I downloaded imagery from Planet Labs. To do so, I navigated to Planet Explorer, specified my area of interest by uploading a shapefile of the study area, and specified the date range of April 2021 using the left sidebar. From the list of suitable images and image collections, I chose a single image that covered the entire study site. This eliminated any preprocessing work associated with stitching images together, and Planet Labs actually clipped the provided image to my study area.</p>
<p>The other two images were generated by flying drones directly over the study site. The drone image from 2021 was generated and provided by Bill Hegman using a borrowed drone. The drone image from 2022 was generated using the Geography Department’s drone during last week’s lab, when Bill Hegman took our class to the study site for a hands-on field trip. After lab, Bill stitched the many images taken by the drone into one image. I downloaded both drone images directly from our class Canvas page.</p>
<p>In order to efficiently compare the value of my four image sources, I generated the following table displaying important characteristics of each image.</p>
<table class="caption-top table">
<caption>Table 1. Spatial, spectral, and temporal characteristics of each source of imagery.</caption>
<colgroup>
<col style="width: 14%">
<col style="width: 28%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 14%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Source</th>
<th>Spatial Extent</th>
<th>Resolution</th>
<th>Year</th>
<th>Bands</th>
<th>Frequency of Imaging</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VT Open Data Portal</td>
<td>Small, but merged images to cover entire study area.</td>
<td>30 cm</td>
<td>2017</td>
<td>RGB + NIR</td>
<td>Once every 5 years, when VT commissions a plane.</td>
</tr>
<tr class="even">
<td>Planet Labs</td>
<td>Large. Came from Planet Labs clipped to study area.</td>
<td>3 m</td>
<td>2021</td>
<td>RGB + NIR</td>
<td>Daily (not always great quality)</td>
</tr>
<tr class="odd">
<td>Drone 2021</td>
<td>Small. Does not cover study area.</td>
<td>6 cm</td>
<td>2021</td>
<td>RGB + NIR + 8 more</td>
<td>As needed, with borrowed drone.</td>
</tr>
<tr class="even">
<td>Drone 2022</td>
<td>Even smaller. Does not cover study area.</td>
<td>2.1 cm</td>
<td>2022</td>
<td>RGB</td>
<td>As needed, easy because we own the drone.</td>
</tr>
</tbody>
</table>
<p>My initial impression is that drones would work best for this work, because they offer the best spatial and spectral properties and if you own one, you can fly it whenever works best. While the aerial imagery from VT Open Data Portal has good technical specifications, they only photograph the region every 5 years. Flooded areas of standing water change from year to year depending on differing flood conditions, so aerial imagery would often fail to provide the most up-to-date information. While Planet Labs photographs the entire earth every day, their images are often covered by clouds and with a 3 meter pixel resolution, they might fail to pick up on small areas of standing water.</p>
<p>Let’s see how each image source performs in a classification model before deciding which one is best-suited for the Lemon Fair Insect Control District.</p>
</section>
<section id="pre-processing-analysis-performed" class="level2">
<h2 class="anchored" data-anchor-id="pre-processing-analysis-performed">Pre-Processing &amp; Analysis Performed</h2>
<p>The first step of my analysis was to create a new map in ArcGIS Pro and import all of my images into the project. Next, I merged the six VT Open Data Portal images together into one image suitable for classification using the mosaic to raster tool. At this point, I was ready to perform my analysis. Because Mosquitoes tend to lay their eggs in standing and slow-moving water, I chose to identify locations of still water, which most often appeared in my images as irrigation channels, ponds, and the flooding of the Lemon Fair.</p>
<p>One at a time, I selected each image in ArcGIS Pro and navigated to the “classification wizard”. I configured my classifier as a supervised, object-oriented analysis and modified the default schema to include only the classes relevant to each image, which wound up being some combination of standing water, farmland, trees with leaves, flowing water, trees without leaves, shrubs, and wetlands. After using the default parameters to generate the segmented image, I identified 20 training samples for each class. I selected the Support Vector Machine as my classifier, using all segment attributes in my model. I saved my classified dataset as a new layer, and I chose not to merge or reclassify any regions so that I could fairly compare the performance of each image’s classification.</p>
</section>
<section id="results-and-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="results-and-interpretation">Results and Interpretation</h2>
<p>Below, each classified layer is displayed next to its corresponding true color image, followed by a reflection on the strengths and/or weaknesses of that image source for classification. The legend next to the first set of maps applies to all classified figures.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="drone2022.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1. True color image from 2022 drone footage (left) and corresponding classified image (right).</figcaption>
</figure>
</div>
<p>As you can tell, the 2022 drone footage performed extremely poorly. Classification does not appear to be any better than random chance. Classification of regions is both blocky and pixelated, and no areas were correctly identified as standing water.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="drone2021.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2. True color image from 2021 drone footage (left) and corresponding classified image (right).</figcaption>
</figure>
</div>
<p>In stark contrast, the 2021 drone imagery performed remarkably well. Most ponds and irrigation channels were correctly classified as standing water. One aspect I found impressive was that flowing water was never erroneously classified as standing water; I, personally, would imagine that it would be difficult to distinguish between those two classes of water. That said, the model did make some errors. The L-shaped lake in the middle of the image was misclassified as flowing water, and some irrigation channels were misclassified as trees without leaves (although to be fair, there were trees without leaves along these irrigation channels). Overall, I am very impressed with the performance of this drone, especially juxtaposed with the drone from 2022.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vtopendata.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3. True color mosaicked image from VT Open Data (left) and corresponding classified image (right).</figcaption>
</figure>
</div>
<p>The VT Open Data image performed better than the 2022 drone footage, but worse than the 2021 drone footage. Unfortunately, several agricultural fields and shrub areas were misclassified as standing water. Perhaps they were relatively wet areas and thus shared similar spectral characteristics. Additionally, several sections of road and several sections of the Lemon Fair River are misclassified as standing water, perhaps because of object shapes similar to standing water, and one pond was misclassified as flowing water. That said, many irrigation channels and ponds were correctly classified, and this classification would be salvageable were I to go in with the reclassification tool to correct mistakes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="planet.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4. True color image from Planet Labs (left) and corresponding classified image (right).</figcaption>
</figure>
</div>
<p>Finally, despite its courser spatial resolution, the Planet Labs image performed better than the 2022 drone image. It correctly classified portions irrigation channels and some ponds, but failed to capture other parts. Additionally, one section of the forest in the southwest corner of the study region was misclassified as standing water, as well as parts of the flowing Lemon Fair River. While this image far outperformed the 2021 drone image, I believe that its 3m pixel resolution was inadequate to identify some narrow parts of irrigation channels.</p>
</section>
<section id="recommendations-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="recommendations-conclusion">Recommendations &amp; Conclusion</h2>
<p>Despite performing an identical analysis on each image, the performance of each image differed greatly, ranging from results that look basically random to results that are highly accurate. The clear winner in terms of classification accuracy is the 2021 drone footage, which correctly identifies almost all instances of standing water.</p>
<p>With a pixel resolution of mere centimeters and 12 multispectral bands, the 2021 drone simply has the best technical specifications for the job, allowing it to pick up on small objects that are essentially invisible at coarser resolutions. Drones also have the benefit of versatility: if you own a drone, then you can fly it whenever you want, which is far preferable to chartering a plane.</p>
<p>That said, drones are not without their drawbacks. First, operating a drone requires some degree of technical expertise. Second, it can take a long time, and many batteries, to photograph a study site. And third, the technical specifications vary widely between drones, allowing some drones to perform far better than others (as we saw with the 2022 drone footage). My recommendation to the Lemon Fair Insect Control District is to carefully evaluate their drone options and select one that minimizes these drawback and works best for them.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>