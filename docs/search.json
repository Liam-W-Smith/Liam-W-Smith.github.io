[
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Water Chemistry R Package\n\n\n\n\n\n\nR\n\n\n\nAn R package to process water chemistry data into a standardized, machine readable format.\n\n\n\n\n\nJan 6, 2025\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Final Project\n\n\n\n\n\n\nGIScience\n\n\nMachine Learning\n\n\nPython\n\n\n\nPredicting population density with land cover using linear regression and spatial autoregression.\n\n\n\n\n\nMay 17, 2024\n\n\nManuel Fors, Liam Smith, Yide (Alex) Xu\n\n\n\n\n\n\n\n\n\n\n\n\nRoad Segmentation\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\nRemote Sensing\n\n\n\nAn image processing project using k-means clustering, random forest classification, and the U-Net CNN architecture for road segmentation.\n\n\n\n\n\nMay 15, 2024\n\n\nAlex Oh, Aiden Pape, and Liam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nCartography Final Project\n\n\n\n\n\n\nAdobe Illustrator\n\n\nCartography\n\n\n\nA map of rock climbing opportunities in South Africa’s Western Cape.\n\n\n\n\n\nMay 14, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernelized Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nExtending logistic regression to a sparse version that detects nonlinear decision boundaries.\n\n\n\n\n\nMay 5, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nA comparison between two optimization algorithms for logistic regression.\n\n\n\n\n\nApr 21, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nImplementing logistic regression using gradient descent with momentum.\n\n\n\n\n\nApr 3, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nImplementing one of the oldest machine learning models from scratch.\n\n\n\n\n\nApr 1, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\nBlack box classification with the Palmer Penguins dataset.\n\n\n\n\n\nMar 7, 2024\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nMathematics Thesis\n\n\n\n\n\n\nMathematics\n\n\n\nThe capstone project for my mathematics major, this paper investigates two applications of spectral graph theory.\n\n\n\n\n\nDec 16, 2023\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGIScience Summer Research\n\n\n\n\n\n\nGIScience\n\n\nPython\n\n\n\nA reproduction and reanalysis of Spielman et al.’s critique of the Social Vulnerability Index (SoVI).\n\n\n\n\n\nAug 3, 2023\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nHabitat Blocks and Connectivity\n\n\n\n\n\n\nConservation\n\n\nPython\n\n\n\nMethodologies for defining habitat blocks and their implications on connectivity.\n\n\n\n\n\nMay 15, 2023\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Risk and Conservation\n\n\n\n\n\n\nConservation\n\n\nGoogle Earth Engine\n\n\n\nIdentifying priority parcels for flood hazard and conservation buy-out programs.\n\n\n\n\n\nApr 18, 2023\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nScenic Viewsheds\n\n\n\n\n\n\nConservation\n\n\nGoogle Earth Engine\n\n\nQGIS\n\n\n\nSensitivity of viewshed analyses to different Digital Elevation Models.\n\n\n\n\n\nMar 10, 2023\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nFuzzy C-Means Clustering\n\n\n\n\n\n\nMachine Learning\n\n\nR\n\n\n\nUnderstanding a popular algorithm for soft clustering.\n\n\n\n\n\nMay 18, 2022\n\n\nYifei Luo, Liam Smith, Xingze Wang\n\n\n\n\n\n\n\n\n\n\n\n\nHigh Resolution Landcover Classification\n\n\n\n\n\n\nArcGIS Pro\n\n\nRemote Sensing\n\n\n\nComparing different image sources for object-oriented landcover classification.\n\n\n\n\n\nMay 1, 2022\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Versus Global Classification\n\n\n\n\n\n\nGoogle Earth Engine\n\n\nRemote Sensing\n\n\n\nTraining a land classifier in Cameroon and comparing results with a well-known global classifier.\n\n\n\n\n\nApr 1, 2022\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nDeforestation in Protected Areas\n\n\n\n\n\n\nGoogle Earth Engine\n\n\nRemote Sensing\n\n\n\nDetecting and comparing deforestation across parks with varying levels of protection in Malawi.\n\n\n\n\n\nMar 1, 2022\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nGreen Space and Heat Anomalies\n\n\n\n\n\n\nCartography\n\n\nGoogle Earth Engine\n\n\nRemote Sensing\n\n\n\nA remote sensing project and interactive visualization assessing changes in green space and land surface temperature anomalies in Cape Town over time.\n\n\n\n\n\nFeb 4, 2022\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study with Spatial Regression\n\n\n\n\n\n\nGIScience\n\n\nR\n\n\n\nUsing geographically weighted regression to analyze social predictors and spatial variation of pediatric dental extractions in the United Kingdom.\n\n\n\n\n\nDec 18, 2021\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nHurricane Ida Spatial Twitter Analysis\n\n\n\n\n\n\nGIScience\n\n\nR\n\n\n\nAnalyzing spatial patterns in Twitter activity during a natural disaster.\n\n\n\n\n\nNov 28, 2021\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Accessibility of Healthcare Resources\n\n\n\n\n\n\nGIScience\n\n\nPython\n\n\n\nReproducing and revising a study on accessibility of healthcare resources necessary to address COVID-19 in Chicago.\n\n\n\n\n\nNov 8, 2021\n\n\nLiam Smith\n\n\n\n\n\n\n\n\n\n\n\n\nFlood Risk and Food Security\n\n\n\n\n\n\nGIScience\n\n\nSQL\n\n\n\nUnderstanding the impact of flooding on food accessibility in Dar Es Salaam.\n\n\n\n\n\nOct 7, 2021\n\n\nLiam Smith\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/palmer-penguins/index.html",
    "href": "posts/palmer-penguins/index.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#abstract",
    "href": "posts/palmer-penguins/index.html#abstract",
    "title": "Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset contains anatomical measurements for \\(344\\) Penguins of \\(3\\) different species and is commonly used when introducing students to new data science concepts. In this blog post, we construct our first classification model in Python, attempting to accurately predict the species of a given observation in the Palmer Penguins dataset. Constrained to one qualitative and two quantitative predictors, we create our model using the scikit-learn package’s out-of-the-box logistic regression functionality. We select features by conducting 5-fold cross-validation on logistic regression models constructed from every possible combination of one qualitative and two quantitative variables. We implement our two models with the highest cross-validation score, selecting one that achieves 100% test data accuracy as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-preparation",
    "href": "posts/palmer-penguins/index.html#data-preparation",
    "title": "Palmer Penguins",
    "section": "Data Preparation",
    "text": "Data Preparation\nMost of the code in this section was conveniently provided by Professor Chodrow.\n\n# Load packages \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport warnings\n\n# Change pd display parameter\npd.set_option('display.max_colwidth', 100)\n\n# Supress warnings to improve web aesthetics\nwarnings.filterwarnings(\"ignore\")\n\n# Load training data\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n# Data preparation\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  x = df.drop([\"Species\"], axis = 1)\n  x = pd.get_dummies(x)\n  return x, y, df\n\nX_train, y_train, viz_train = prepare_data(train)"
  },
  {
    "objectID": "posts/palmer-penguins/index.html#data-exploration",
    "href": "posts/palmer-penguins/index.html#data-exploration",
    "title": "Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore constructing a classification model, it is often helpful to get a better feel for the data you are working with. For this reason, we begin our analysis by creating and analyzing a few figures.\n\n# Display plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\n\nsns.scatterplot(viz_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", hue = \"Species\", ax = ax[0]);\nsns.move_legend(ax[0], \"lower right\")\nsns.scatterplot(viz_train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\", hue = \"Species\", ax = ax[1]);\nsns.move_legend(ax[1], \"lower right\")\n\n\n\n\n\n\n\n\nIn the first scatterplot, we have almost perfectly separated the three species of penguin using just two variables: flipper length and culmen length. The Adelie penguins tend to have low values of both variables, the Chinstrap penguins tend to have large culmen lengths but small flipper lengths, and the Gentoo penguins tend to have medium culmen lengths and large flipper lengths. One can imagine drawing straight lines in this graph that almost perfectly separate the three species of penguin. The fact that we can visually distinguish between the different species of penguins makes me very confident about our prospects for classification.\nIn the second scatterplot, the Gentoo penguins tend to have small culmen depths and large body masses, separating them from the other two species with some white-space to spare. However, the distributions of Adelie and Chinstrap points look almost identical, occupying the same lower right corner of the plot. If our goal was simply to predict whether a penguin is a Gentoo penguin, then I would imagine that these two variables could help make a strong classifier. However, since we seek to distinguish between all three species, these variables might not be the best choice.\nWhile body mass (or body mass and culmen depth) alone is insufficient to distinguish between the species, perhaps accounting for a categorical variable like sex will reveal some patterns. To explore this possibility, we include the following table of median body mass.\n\n# Display table\nviz_train.groupby(['Sex', 'Species'])[['Body Mass (g)']].median().unstack()\n\n\n\n\n\n\n\n\nBody Mass (g)\n\n\nSpecies\nAdelie\nChinstrap\nGentoo\n\n\nSex\n\n\n\n\n\n\n\nFEMALE\n3350.0\n3525.0\n4700.0\n\n\nMALE\n4025.0\n4050.0\n5500.0\n\n\n\n\n\n\n\nFrom this table, it is clear that the median body masses of all three species vary by sex, with the males tending to weigh more than the females. Additionally, while we already knew from our scatterplot that the Gentoo penguins tend to weigh more than the other species, this table reinforces that conclusion and shows it to be true for both males and females. Unfortunately, accounting for sex appears to do little to further distinguish between Adelie and Chinstrap penguins. There is roughly a 200 gram difference between the median mass of female Adelie and Chinstrap penguins, so perhaps sex in can help distinguish between the two species given that a penguin is female. However, the median mass of male Adelie and Chinstrap penguins differs only by 25 grams. A classifier would almost certainly need more information in order to make accurate predictions."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-construction",
    "href": "posts/palmer-penguins/index.html#model-construction",
    "title": "Palmer Penguins",
    "section": "Model Construction",
    "text": "Model Construction\nSince the Palmer Penguins dataset is relatively small and we are given constraints on the number of features we may include in our model, we will select features by fitting a classifier for every possible combination of one qualitative and two quantitative variables. There may be more robust and less computationally expensive methods of feature selection, but for our purposes, this works.\nWe implement logistic regression using scikit-learn’s out-of-the-box logistic-regression classifier.\n\n# Choosing features\nfrom itertools import combinations\n\n# Distinguish between qualitative and quantitative variables\nall_qual_cols = ['Clutch Completion', 'Sex', 'Island', 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n# Define classifier\nLR = LogisticRegression(max_iter=100000000)\n\n# Initialize dataframe to store data\nmodels = pd.DataFrame(columns = [\"CV Score\", \"Columns\"])\n\n# Find best-performing classifier\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv = 5)\n    cv_score = cv_scores_LR.mean()\n    \n    model = pd.DataFrame({\"CV Score\": [cv_score], \"Columns\": [cols]})\n\n    models = pd.concat([models, model])\n\n\n# Sort by CV Score and display best-performing models\nmodels = models.sort_values(\"CV Score\", ascending=False)\nmodels.head()\n\n\n\n\n\n\n\n\nCV Score\nColumns\n\n\n\n\n0\n0.988311\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.988311\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\n\n\n0\n0.980543\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Body Mass (g)]\n\n\n0\n0.968854\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\n\n\n0\n0.968778\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Delta 13 C (o/oo)]\n\n\n\n\n\n\n\nLet’s fit the model with the highest cross-validation score! There are actually two models with equal performance, so we will try both.\n\n# Test the model\n\n# Import data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Adjust species label\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n# Data preparation\nX_test, y_test, viz_test = prepare_data(test)\n\n# Fit and output the performance of the model\nmodel1 = LR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\nmodel1_score = LR.score(X_test[models.iloc[[0]]['Columns'][0]], y_test)\nmodel1_score\n\n1.0\n\n\nVoila! Logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Island features correctly predicts every observation in our test data, achieving our goal.\n\n# Test the next model\nmodel2 = LR.fit(X_train[models.iloc[[1]]['Columns'][0]], y_train)\nmodel2_score = LR.score(X_test[models.iloc[[1]]['Columns'][0]], y_test)\nmodel2_score\n\n0.9852941176470589\n\n\nOn the other hand, logistic regression using the Culmen Length (mm), Culmen Depth (mm), and Sex features does not achieve 100% accuracy on the test data. Since our first model achieved 100% test accuracy, we select it as our final model."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#model-evaluation",
    "href": "posts/palmer-penguins/index.html#model-evaluation",
    "title": "Palmer Penguins",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nAt this point, we have constructed a well-performing model, so we proceed to model evaluation. First, we visualize our model’s decision regions using the plot_regions() function provided by Professor Chodrow.\n\n# Plotting decision regions function, generously provided by Professor Chodrow\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (10, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout(rect = (0,0,.95,.95))\n\n\n# Reverse the order of columns for the purposes of visualization\nmodels.iloc[[0]]['Columns'][0].reverse()\n\n# Refit model\nLR.fit(X_train[models.iloc[[0]]['Columns'][0]], y_train)\n\n# Plot decision regions on training data\nplot_regions(LR, X_train[models.iloc[[0]]['Columns'][0]], y_train)\nplt.suptitle(\"Decision Regions and Training Data\", fontsize = 13)\n\n# # Plot decision regions on test data\nplot_regions(LR, X_test[models.iloc[[0]]['Columns'][0]], y_test)\nplt.suptitle(\"Decision Regions and Test Data\", fontsize = 13);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the upper row of figures, we plot the training data in our decision regions, while in the lower row, we plot the test data in the decision regions. We trained a model on our training data and implemented it on our test data, so the decision regions are identical in the two rows of figures. The only difference between the rows is the points within the decision regions. Since most observations are part of the training data, there are many more points in the first row of figures. It is interesting to note that Torgersen Island only contains Gentoo penguins, Dream Island does not contain any Adelie penguins, and Biscoe Island does not contain any Chinstrap penguins. These patterns makes it substantially easier to predict a given penguin’s species. One could imagine that the model would always predict Gentoo penguins on Torgersen Island. Similarly, one could imagine the model never predicting Adelie penguins on Dream Island and never predicting Chinstrap penguins on Biscoe Island. Interestingly, the model actually fits decision regions for all three penguin varieties in each island. The decision region for a given species tends to be larger on islands where we would expect to find them, but the model allows for the possibility that there might be a penguin on an island where we would not expect one. Overall, the logistic regression model does a great job of incorporating both qualitative and quantitative variables into its decision regions.\n\n# Predict\nmodel1_pred = model1.predict(X_test[models.iloc[[0]]['Columns'][0]])\n\n# Confusion matrix\nC = confusion_matrix(y_test, model1_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nSince all of the predictions are on the main diagonal of the confusion matrix, we conclude that we did not misclassify any observations – but we already knew this, since our model achieved 100% test accuracy! If our model had not perfectly classified the observations in our test data, a confusion matrix would be more helpful for assessing our model’s misclassification tendencies."
  },
  {
    "objectID": "posts/palmer-penguins/index.html#discussion",
    "href": "posts/palmer-penguins/index.html#discussion",
    "title": "Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nIn this blog post, we began by exploring the Palmer Penguins dataset. Our scatterplots and table illustrated that for some variables, there is substantial variation between different species of penguin, while for others, there is very little. Determining which variables would be most valuable for classification required more than simple visualizations. In order to robustly determine the best combination of one qualitative and two quantitative varibles, I fit a logistic regression model on every possible combination. This process was relatively slow and may not work at scale, but it was successful in generating a highly accurate model in this scenario. My final model used the Culmen Length (mm), Culmen Depth (mm), and Island features, achieving 100% accurate predictions on the test data.\nThis assignment furthered my understanding of the entire model-construction procedure, including data preparation, data exploration, feature selection, model fitting, and model evaluation. After completing this blog post, I feel more confident about my ability to implement standard machine learning procedures from Python packages like scikit-learn. While it is relatively easy to implement out-of-the-box classifiers from scikit-learn, it may be more difficult to understand their theoretical underpinnings. As I finish this assignment, I am eager to learn more about the mathematics happening under the hood."
  },
  {
    "objectID": "posts/riparian-report/index.html",
    "href": "posts/riparian-report/index.html",
    "title": "Flood Risk and Conservation",
    "section": "",
    "text": "Through the Flood Resilient Communities Fund (FRCF), the Vermont legislature has allocated $14,750,000 of American Rescue Plan Act funds for projects that reduce flood risk and promote community resilience for the fiscal year 2023 (VT DEC Watershed Management Division & VT Emergency Management, 2022). Focused on “buyouts of flood-vulnerable properties,” this funding presents communities throughout Vermont, including the Town of Middlebury, with an opportunity to mitigate local flood hazards by purchasing parcels that are particularly prone to flooding or valuable for floodwater storage (VT DEC Watershed Management Division & VT Emergency Management, 2022).\nFurthermore, by evaluating the overlap between flood-prone areas and priority conservation targets, communities may be able to realize conservation co-benefits by prioritizing purchasing parcels that are both at risk of flooding and home to wildlife habitats. In this analysis, I present one methodology for identifying priority parcels in Middlebury for buyout under FRCF, and I investigate the sensitivity of this methodology to the choice of input layers."
  },
  {
    "objectID": "posts/riparian-report/index.html#introduction",
    "href": "posts/riparian-report/index.html#introduction",
    "title": "Flood Risk and Conservation",
    "section": "",
    "text": "Through the Flood Resilient Communities Fund (FRCF), the Vermont legislature has allocated $14,750,000 of American Rescue Plan Act funds for projects that reduce flood risk and promote community resilience for the fiscal year 2023 (VT DEC Watershed Management Division & VT Emergency Management, 2022). Focused on “buyouts of flood-vulnerable properties,” this funding presents communities throughout Vermont, including the Town of Middlebury, with an opportunity to mitigate local flood hazards by purchasing parcels that are particularly prone to flooding or valuable for floodwater storage (VT DEC Watershed Management Division & VT Emergency Management, 2022).\nFurthermore, by evaluating the overlap between flood-prone areas and priority conservation targets, communities may be able to realize conservation co-benefits by prioritizing purchasing parcels that are both at risk of flooding and home to wildlife habitats. In this analysis, I present one methodology for identifying priority parcels in Middlebury for buyout under FRCF, and I investigate the sensitivity of this methodology to the choice of input layers."
  },
  {
    "objectID": "posts/riparian-report/index.html#background",
    "href": "posts/riparian-report/index.html#background",
    "title": "Flood Risk and Conservation",
    "section": "Background",
    "text": "Background\nVermont Conservation Design (VCD) is a governmental initiative that takes a “rigorous, science-based” approach to conservation efforts (Sorenson & Zaino, 2018). By developing statewide datasets of forest blocks, riparian areas, natural communities, wetlands, and more, VCD provides the data required to conduct informed and targeted conservation efforts (Sorenson & Zaino, 2018). Furthermore, through the online BioFinder mapping interface, the Vermont Agency of Natural Resources (ANR) provides ordinary citizens with the opportunity to explore these datasets (Creating BioFinder and Vermont Conservation Design, n.d.).\nTwo layers that are made publicly available through VCD are a “Surface Water and Riparian Areas - Highest Priority” layer, which includes all aquatic habitats in Vermont and the valley bottoms typically affected by flooding, and a “Riparian Wildlife Connectivity” layer, which includes riparian areas and habitat corridors for riparian wildlife (VT ANR Biofinder/VCD Team, 2019). The intersection of these two layers would provide us with the locations that are both flood-prone and important for riparian wildlife, which are precisely the conditions that we seek for identifying parcels for buyout under FRCF.\nUnfortunately, the VCD layers have a relatively coarse spatial resolution. As evidence of this, please see the figure on the following page, which shows a close-up look at the “Riparian Wildlife Connectivity” layer (Figure 1). Notice the pixelation and how parts of Otter Creek fail to be included in the layer; in reality, riparian wildlife certainly use the entire creek. While VCD’s layers provide valuable information at scale, datasets of finer spatial resolution may provide more reliable information when developing buy-out recommendations.\n\n\n\nFigure 1: A close-up look at VCD’s “Riparian Wildlife Connectivity” layer in Middlebury.\n\n\nAccording to Avelino et al., high resolution imagery can capture more detail but it can also introduce more noise, so the choice of spatial resolution should be specific to one’s use of the imagery (Avelino et al., 2016). In this analysis, I develop a higher resolution alternative to VCD’s layers, using a number of high-resolution layers introduced to us by Professor Howarth. To both layers, I apply a methodology for identifying properties for buy-out – calculating the percentage of each parcel in a desirable location for riparian conservation – in the hopes that I might determine which resolution is preferable for our purposes."
  },
  {
    "objectID": "posts/riparian-report/index.html#methods",
    "href": "posts/riparian-report/index.html#methods",
    "title": "Flood Risk and Conservation",
    "section": "Methods",
    "text": "Methods\nFirst, I identified locations that are both at risk of flooding and valuable for conservation efforts according to VCD by taking the intersection of VCD’s “Surface Water and Riparian Areas - Highest Priority” and “Riparian Wildlife Connectivity” layers.\nTo create a higher resolution alternative to this layer, I first created higher resolution alternatives to the two input layers. Specifically, as a replacement for VCD’s surface waters layer, I took the union of the ANR river corridors and small stream setbacks, Addison County’s wetlands, and Middlebury’s FEMA flood hazard areas. As an alternative to VCD’s riparian wildlife connectivity layer, I took the union of all water and tree cover landcover in Middlebury. Many thanks to Professor Howarth for putting together several of these layers in Earth Engine. Finally, I took the intersection of these two layers to serve as a higher resolution alternative to the intersection of the two VCD layers. The figure below illustrates the similarities and differences between the two layers (Figure 2). Notably, the VCD layer is far more pixelated and tends to include wider buffers around Otter Creek, Middlebury River, and the Muddy Branch, while the high-resolution alternative includes more wetlands and flood-prone regions not directly adjacent to rivers.\n\n\n\nFigure 2: Priority lands to help identify parcels to buy-out for flooding and conservation reasons.\n\n\nNext, I used a reducer to calculate the area of each of the two layers within each parcel in Middlebury. I used this statistic to calculate the percentage of each parcel’s area that is overlapped by the two different layers. All of that analysis is available in this script.\nFinally, I created a new script to produce the figures for my discussion. Specifically, I created a histogram and a choropleth map of the percentage of each parcel in a favorable location for riparian conservation for both the VCD layer and the high-resolution alternative. I also set a cut-off threshold of 80% and used that to generate an agreement and disagreement map of which parcels ought to be prioritized for buy-out under the two methodologies. This visualization script is available here, and the resulting figures are included below."
  },
  {
    "objectID": "posts/riparian-report/index.html#discussion",
    "href": "posts/riparian-report/index.html#discussion",
    "title": "Flood Risk and Conservation",
    "section": "Discussion",
    "text": "Discussion\nAlthough Figure 2 illustrates that the two methodologies for identifying priority lands for buy-out lead to substantially different layers, when we aggregate these layers to the parcel level, we find surprisingly similar results. To begin, let us evaluate the distribution of the percent area in each case.\n\n\n\nFigure 3: Frequency histogram of the percent area of parcels in VCD priority buy-out layer.\n\n\n\n\n\nFigure 4: Frequency histogram of percent area of parcels in the high-resolution buy-out layer.\n\n\nClearly, the distribution of parcels is fairly similar for the two methodologies. In both cases, the histogram is skewed right, with the vast majority of parcels having between 0% and 25% of their area in a desirable location for riparian conservation. There appear to be more parcels in the 25-50% category in the high-resolution layer, but fewer parcels in the 50-75% and 75-100% categories (Figures 3 and 4). While there is relatively little variation between the histograms, these graphs reveal nothing about any differences in spatial patterns between the two methodologies. To investigate any spatial differences, I created a choropleth map for each methodology (Figures 5 and 6).\n\n\n\nFigure 5: The percentage of each parcel’s land area in the VCD priority buy-out layer.\n\n\n\n\n\nFigure 6: The percentage of each parcel’s land area in the high-resolution priority buy-out layer.\n\n\nFor the most part, these maps reveal similar spatial patterns for the two methodologies. In both maps, parcels in the vicinity of Otter Creek, the Middlebury River, and the Muddy Branch tend to be colored in deeper red, indicating that they are the most valuable parcels for riparian conservation (Figures 5 and 6). This trend is slightly more pronounced in the VCD map, with more parcels colored deep red around the rivers (Figure 5). This makes sense given the tendency of the VCD layer to include wider buffers around the rivers, which we noticed earlier in the comparison map of priority lands for buyout (Figure 2).\nHowever, the higher resolution layer appears to have better captured the actual shape of the Middlebury River (Figure 6), perhaps because smaller pixels better capture the shapes of smaller features (Avelino et al., 2016). Additionally, further away from the rivers, there appears to be more white parcels in the VCD map (Figures 5 and 6). This may occur because the high-resolution layer includes more wetlands and flood-prone areas away from the rivers, as noted in the comparison map earlier (Figure 2).\nAs one might expect, these tendencies carried over to the classification of parcels for prioritization for buyout. As mentioned in the methodology section, I used a threshold of 80% to indicate whether one should prioritize a parcel for buyout. The choice of threshold was totally arbitrary and should be carefully evaluated if one were to actually use this method to propose the buyout of parcels under the FRCF.\n\n\n\nFigure 7: Agreement and disagreement map regarding parcels to be prioritized for buyout.\n\n\nAs we expect from the choropleth maps (Figures 5 and 6), both methodologies indicate that several properties in the vicinity of Otter Creek, the Middlebury River, and the Muddy Branch ought to be prioritized for conservation (Figure 7). The VCD methodology includes a few additional parcels in the vicinity of Otter Creek, while the high-resolution methodology better captures the Middlebury River (Figure 7)."
  },
  {
    "objectID": "posts/riparian-report/index.html#conclusions",
    "href": "posts/riparian-report/index.html#conclusions",
    "title": "Flood Risk and Conservation",
    "section": "Conclusions",
    "text": "Conclusions\nOverall, the results for the VCD and high-resolution methodologies were very similar. The high-resolution layer may represent a slight improvement because it better captures the habitat and flood-prone areas that are not situated in the vicinity of rivers. On the other hand, the VCD layer produced largely similar results, using far less computational effort due to the reduced number of pixels, so it may suffice for an analyst concerned about computational speed.\nThere are a couple of areas of weakness to my analysis that future work ought to address. First of all, the method I used to isolate the parcels in the Town of Middlebury also included any parcels bordering Middlebury. Although I clip these parcels out in the final figures, it would be computationally more efficient not to calculate percent area for these parcels at all. There are also a few cases where a parcel’s percent area is calculated to be slightly more than 100%. At 100.1% or 100.2%, I suspect this occurred because some raster cells were split over the boundary of the parcel, but this would certainly be worthwhile to investigate.\nAdditionally, while I used several of the layers presented in class, there are others that could provide additional insight, such as the grassland habitat blocks. Future work ought to refine the high-resolution layer to be as comprehensive yet noise-free as possible. Finally, further work should actually make recommendations for parcels to buy-out under FRCF. To make legitimate recommendations, one might use a methodology similar to the one in this report, and then cross-reference the high-priority parcels with information regarding the current land use and ownership of those parcels. Overall, this work provides a solid basis for identifying parcels to buy-out under FRCF, but there is substantial additional work that would be required to actually make those recommendations."
  },
  {
    "objectID": "posts/riparian-report/index.html#bibliography",
    "href": "posts/riparian-report/index.html#bibliography",
    "title": "Flood Risk and Conservation",
    "section": "Bibliography",
    "text": "Bibliography\nAvelino, A. F. T., Baylis, K., & Honey-Rosés, J. (2016). Goldilocks and the Raster Grid: Selecting Scale when Evaluating Conservation Programs. PLOS ONE, 11(12), e0167945. https://doi.org/10.1371/journal.pone.0167945\nCreating BioFinder and Vermont Conservation Design. (n.d.). VT Agency of Natural Resources. Retrieved April 17, 2023, from https://anr.vermont.gov/maps/biofinder/creating-and- design\nSorenson, E., & Zaino, R. (2018). Summary Report for Landscapes, Natural Communities, Habitats, and Species. Vermont Conservation Design. https://anr.vermont.gov/sites/anr/files/maps/biofinder/Vermont%20Conservation%20Desi gn%20-%20Summary%20Report%20-%20February%202018.pdf\nVT ANR Biofinder/VCD Team. (2019). BioFinder 3.0 Development Report. https://drive.google.com/file/d/1eiLfinDcBmC4skrvpNl3RVbkdLxSHzfj/view\nVT DEC Watershed Management Division & Vermont Emergency Management. (2022). Flood Resilient Communities Fund Program Overview – August 2022. https://vem.vermont.gov/sites/demhs/files/documents/Flood%20Resilient%20Communiti es%20Fund%20Overview_FY23.pdf"
  },
  {
    "objectID": "posts/spielman-reproduction/index.html",
    "href": "posts/spielman-reproduction/index.html",
    "title": "GIScience Summer Research",
    "section": "",
    "text": "Over the summer of 2023, I contributed to an open-science research effort seeking to address the reproducibility crisis in the field of geography by reproducing a published paper on social vulnerability to natural hazards. This involved reviewing all resources provided by the original authors, coding their analysis in Python, and critiquing the statistical methodology of the Social Vulnerability Index (SoVI).\nMy research advisor and I wrote the following report and delivered this presentation at the American Association of Geographers’ Annual Meeting in April 2024. Further documentation is available in our GitHub repository.\n\n\n\n\n  \n     Download PDF"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "In this blog post, we implement the perceptron, one of the first machine learning models ever invented, and perform a few experiments to highlight the model’s strengths and weaknesses. The perceptron algorithm is used to predict a binary outcome on data with a continuous and finite-dimensional feature space. Through our experiments, we visually illustrate that perceptron converges to a solution that completely differentiates between the outcome classes on linearly separable data. However, the algorithm does not converge to a solution on non-linearly separable data. We then implement a generalization of perceptron known as minibatch perceptron, which uses \\(k\\) observations in each iteration rather than \\(1\\). We perform experiments that illustrate that minibatch perceptron behaves similarly to perceptron when \\(k = 1\\) and continues to find a decision boundary on linearly separable data as we increase \\(k\\) towards \\(n\\). Finally, we show that when \\(k = n\\), minibatch perceptron can converge to a solution on non-linearly separable data, although this solution will not correctly classify every data point. To see my implementation of the two algorithms, please visit perceptron.py and minibatch_perceptron.py."
  },
  {
    "objectID": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "href": "posts/perceptron/index.html#part-a-implementing-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\nRecall from Professor Chodrow’s lecture notes that the perceptron algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nPick a random integer \\(i \\in \\{1,\\ldots,n\\}\\).\nCompute the score for the point \\(i\\): \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\).\nUpdate the decision boundary: \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\)\n\n\nI implement this entire process in perceptron.py, but for the sake of brevity, I will not discuss every line of code in this blog post. Instead, I focus exclusively on my implementation of the grad() function, which calculates \\(\\mathbb{1} [s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\n\n# grad() function from perceptron.py\ndef grad(self, X, y):\n        s = X@self.w\n        return (s*y &lt; 0)*X*y\n\nBy taking advantage of our knowledge of linear algebra, the implementation of grad() becomes quite short! In the first line of grad(), we calculate the inner product \\(s_i = \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle\\). The tensors X and self.w are shaped appropriately for us to compute this inner product with torch’s @ operator. In the second and final line of grad(), we use our result from the first line to calculate \\([s_i y_i &lt; 0] y_i \\mathbf{x}_i\\).\nTo verify that our implementation of perceptron was succussful, we run the “minimal training loop” provided in the assignment instructions. First, we need data to run the loop on, so we use some code generously provided by Professor Chodrow to generate and display our linearly separable data.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom minibatch_perceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nimport ipywidgets as wdg\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n# define function to create data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# define function to plot data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\n# plot linearly separable data\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\nVisually, it is clear that one could draw a line between the two different colors of points, so our data is linearly separable. This is important, because the perceptron algorithm will only converge to a solution with a loss of zero on linearly separable data. Since we have our data, we are now prepared to run the minimal training loop.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)\n\nThe minimal training loop terminates, so our model must have converged to a solution with a loss of 0."
  },
  {
    "objectID": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "href": "posts/perceptron/index.html#part-b-fitting-and-evaluating-perceptron-models",
    "title": "Implementing Perceptron",
    "section": "Part B: Fitting and Evaluating Perceptron Models",
    "text": "Part B: Fitting and Evaluating Perceptron Models\nNow that we have a functional implementation of perceptron, we perform experiments and generate illustrations to reveal the strengths and weaknesses of the perceptron model.\n\nPart B.1: Evolution of the Model on Linearly Separable Data\nFirst, we illustrate how the loss function changes between iterations of the minimal training loop.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs you can see, with our data and our randomness seed, the perceptron algorithm converges to a solution with zero loss after 49 iterations. Notice that throughout this process, the weight vector only changes 6 times. In all 43 other iterations, the randomly selected point was correctly classified by the model at that stage.\nTo gain some insight as to the location and orientation of the decision boundary throughout this process, we plot the changes to the weight vector in the figure below. We include a subplot for every change to the weight vector, letting the dashed lines represent the previous weight vector and the solid lines represent the current weight vector. In each subplot, the circled point corresponds to the point \\(i\\) that was misclassified, leading to an update to the weight vector.\nThank you to Professor Chodrow for providing the code for creating this visualization.\n\n# define line function\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# set seed\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X_ls, y_ls)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    i, local_loss = opt.step(X_ls, y_ls)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X_ls, y_ls, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X_ls, y_ls).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X_ls[i,0],X_ls[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y_ls[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThese figures illustrate how the model responds to the random selection of an incorrectly classified point. Generally, the correction slightly overcompensates for the misclassified point, leading the decision boundary to fluctuate between which class it incorrectly classifies, gradually adjusting until it classifies all points correctly.\n\n\nPart B.2: Evolution of the Model on Non-Linearly Separable Data\nUnfortunately, the perceptron algorithm will not converge to a decision boundary on data that is not linearly separable. To illustrate this, we first need to create data that cannot be perfectly divided using one separating line.\nTo create our non-linearly separable data, we use the same method as before but increase the amount of noise. As you can see in the graph below, the two classes have tendencies towards different regions in our resulting data, but it is impossible to draw a straight line that perfectly separates them.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create non-linearly separable data\nX_nls, y_nls = perceptron_data(n_points = 50, noise = 0.8)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\nax.set_title(\"Our Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nNow that we have created non-linearly separable data, we fit the perceptron model on our data. Since the perceptron will not converge to a solution on our data, we modify our code so the model terminates after 1000 iterations.\n\n# set seed\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_nls, y_nls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_nls, y_nls)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\nAs before, we begin by inspecting the change in loss over the iterations of our algorithm.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, our model clearly does not converge to a solution. Rather, the perceptron algorithm is so sensitive to the individual data point under consideration at any given moment that the loss fluctuates wildly throughout the entire process, ranging from less than 0.1 to more than 0.7.\nIn the linearly separable case, we visually inspected every change to the weight vector, but because there are so many changes to the model in this case, it would be burdensome to inspect every change. Instead, we display the decision boundary in the final iteration of our model.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 1.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThe decision boundary could be worse, but it is far from perfect. Perceptron’s inadequate performance on non-linearly separable data is one of its major weaknesses.\n\n\nPart B.3: A 5-Dimensional Example\nUp until this point, we have only trained perceptron models on 2-dimensional data. Working with 2-dimensional data facilitates understanding and communication surrounding the model, as we can easily visualize data in two dimensions. However, we wrote our model to work in any finite number of dimensions. To demonstrate that our implementation works on multidimensional data, we now fit a perceptron model on data with 5 features.\n\n# set seed\ntorch.manual_seed(1234)\n\n# create data\nX_5d, y_5d = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\n# define loss variable\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\n# for recording iteration number\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_5d, y_5d)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\nWhile we cannot visualize our 5 dimensional data and weight vector, we can inspect the changes to our loss function over time using the plot below.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn the figure above, it appears that the perceptron algorithm terminated after 60 iterations, achieving a loss value of zero. The perceptron algorithm terminates if and only if its training data is linearly separable; thus, our data must be linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\nIn this section, I implemented an extension to perceptron known as minibatch perceptron. The minibatch perceptron algorithm differs from the perceptron algorithm in that instead of updating the decision boundary using \\(1\\) random point, it updates the decision boundary using \\(k\\) random points. In more detail, the algorithm involves the following process:\n\nRandomly select an initial decision boundary \\(\\mathbf{w}^{(0)}\\).\nIteratively:\n\nSample \\(k\\) random integers \\(i_1, i_2, ..., i_k \\in \\{1,\\ldots,n\\}\\) without replacement.\nUpdate the decision boundary: \\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{k} \\sum_{j=1}^k \\mathbb{1} [\\langle \\mathbf{w}^{(t)}, \\mathbf{x}_{i_j} \\rangle y_{i_j} &lt; 0] y_{i_j} \\mathbf{x}_{i_j} \\]\n\n\nYou can view my implementation of the algorithm at minibatch_perceptron.py, but I will not discuss all of the implementation details here. From the user’s perspective, fitting a minibatch perceptron model is exactly the same as fitting the perceptron model, with the addition of two parameters: \\(k\\) and \\(\\alpha\\). The \\(k\\) parameter simply refers to the number of points used in each iteration, while the \\(\\alpha\\) parameter is a learning rate that affects how much \\(\\mathbf{w}\\) changes when updated.\nIn the rest of this section, we perform a few experiments to illustrate the functionality of our algorithm and its similarities and differences in comparison to the regular perceptron algorithm. The experiments we perform in parts C.1, C.2, and C.3 all involve relatively similar operations. To reduce the volume of code in each section, we define the experiment() function below.\n\ndef experiment(X, y, k, alpha):  \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    mb_p = MinibatchPerceptron() \n    mb_opt = MinibatchPerceptronOptimizer(mb_p)\n\n    # define loss variable\n    mb_loss = 1.0\n\n    # for keeping track of loss values\n    mb_loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while mb_loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        mb_loss = mb_p.loss(X, y) \n        mb_loss_vec.append(mb_loss)\n        \n        # perform a perceptron update using the random data point\n        mb_opt.step(X, y, k, alpha)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    # set seed\n    torch.manual_seed(1234567)\n\n    # instantiate a model and an optimizer\n    p = Perceptron() \n    opt = PerceptronOptimizer(p)\n\n    # define loss variable\n    loss = 1.0\n\n    # for keeping track of loss values\n    loss_vec = []\n\n    # for recording iteration number\n    iter = 0\n\n    while loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n        \n        # not part of the update: just for tracking our progress    \n        loss = p.loss(X, y) \n        loss_vec.append(loss)\n        \n        # perform a perceptron update using the random data point\n        opt.step(X, y)\n\n        # update iter\n        iter += 1\n\n        # maxiter condition\n        if iter &gt;= 1000:\n            break\n    \n    return loss_vec, mb_loss_vec, p.w, mb_p.w\n\n\nPart C.1: When k = 1\nTo illustrate the differences between the perceptron and minibatch perceptron algorithms, we begin by fitting the regular perceptron and the minibatch perceptron with \\(k = \\alpha = 1\\) on our linearly separable data.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 1, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (12, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels \nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 1, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nThe two algorithms generated incredibly similar results! In fact, if the random point considered at every iteration had been the same, these graphs would be identical. In the regular perceptron algorithm, we generate a random number between \\(0\\) and \\(n\\), but in the minibatch perceptron algorithm, we generate a random permutation of the numbers \\(0\\) through \\(n-1\\) and select the first element of that permutation. This difference appears to result in different points being considered at each iteration, explaining why these graphs differ slightly.\n\n\nPart C.2: When k = 10\nOne advantage of the minibatch perceptron algorithm is that we can tune our model to different values of \\(k\\) and \\(\\alpha\\). As our first adjustment, let’s try changing \\(k\\) to \\(10\\).\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_ls, y = y_ls, k = 10, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 10, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nIt appears that this adjustment allowed our model to converge to a decision boundary in far fewer iterations! While our regular perceptron model requires 49 iterations to achieve perfect classification, the minibatch model with \\(k = 10\\) requires only 8 iterations.\n\n\nPart C.3: Convergence on Non-Linearly Separable Data\nAnother advantage of the minibatch perceptron model is that it can converge to a decision threshold on data that is not linearly separable. This does not mean that the model will perfectly classify every data point, but rather that the model will converge to a decision boundary with a loss value that is less than 0.5.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 1)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 1$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nOh my! That certainly didn’t work. The minibatch perceptron model seems to have an even more chaotic loss function than the regular one! This is why we have the hyperparameter \\(\\alpha\\). Let’s try decreasing \\(\\alpha\\) to 0.01.\n\n# Fit models\nloss_vec, mb_loss_vec, p_w, mb_p_w = experiment(X = X_nls, y = y_nls, k = 50, alpha = 0.01)\n\n# Create plots\nfig, ax = plt.subplots(1, 2, figsize = (10, 5))\nax[0].plot(loss_vec, color = \"slategrey\")\nax[0].scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nax[1].plot(mb_loss_vec, color = \"slategrey\")\nax[1].scatter(torch.arange(len(mb_loss_vec)), mb_loss_vec, color = \"slategrey\")\n\n# Add labels\nax[0].set_title(\"Regular Perceptron\")\nax[1].set_title(\"Minibatch Perceptron with $k = 50, \\\\alpha = 0.01$\")\nax[0].set_xlabel(\"Perceptron Iteration\")\nax[0].set_ylabel(\"Loss\")\nax[1].set_xlabel(\"Perceptron Iteration\");\n\n\n\n\n\n\n\n\nPhew, that looks much better! Whereas the loss function of the regular perceptron algorithm would oscillate for an eternity, the loss function of the minibatch algorithm converged to a value between 0.1 and 0.15 by the 600th iteration. Let’s take a look at the resulting decision boundary.\n\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(mb_p_w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\n# ax.set(xlim = (-1, 2), ylim = (-1, 2))\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\nThis decision boundary looks great! Obviously, it is impossible to perfectly separate the two categories of data in this example with a straight line. The decision boundary that our minibatch perceptron algorithm converged to appears to be about as good as we can do."
  },
  {
    "objectID": "posts/perceptron/index.html#part-d-runtime-complexity",
    "href": "posts/perceptron/index.html#part-d-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "Part D: Runtime Complexity",
    "text": "Part D: Runtime Complexity\nWhat is the runtime complexity of perceptron and minibatch perceptron? To answer this question, let us consider the three lines of code that we run in each iteration of the model:\nloss = p.loss(X, y) \nloss_vec.append(loss)\nopt.step(X, y)\nThe first two lines store a record of the loss function for tracking purposes. They are not actually required for fitting the perceptron, so we exclude them from our consideration of the algorithm’s runtime complexity. Thus evaluating the runtime complexity of a single iteration of perceptron amounts to evaluating the runtime complexity of opt.step(X, y).\nIn our implementation of perceptron (see perceptron.py), opt.step() involves the following operations:\n\nn = X.size()[0]: determines the number of rows in X in constant time\ni = torch.randint(n, size = (1,)): selects a random integer in constant time\nx_i = X[[i],:] and y_i = y[i]: subsets X and y in constant time\ncurrent_loss = self.model.loss(X, y): calculates the current loss, which involves a dot product between two \\(1 \\times p\\) vectors, an operation with linear time complexity O(\\(p\\))\nself.model.w += torch.reshape(self.model.grad(x_i, y_i),(self.model.w.size()[0],)): updates \\(\\mathbf{w}\\) in linear time O(\\(p\\)) due to a dot product in the grad function\nnew_loss = self.model.loss(X, y): calculates the updated loss in linear time O(\\(p\\)) due to a dot product\nreturn i, abs(current_loss - new_loss): returns values for visualization in constant time\n\nSince the operations with the largest time complexity were O(\\(p\\)), the overall runtime complexity of a single iteration of the perceptron algorithm is O(\\(p\\)). This runtime depends on the number of features \\(p\\) rather than the number of observations \\(n\\), because the dot product is between the weight vector \\(\\mathbf{w}\\) and a single row of \\(X\\).\nIn our implementation of minibatch perceptron (see minibatch_perceptron.py), opt.step() involves largely similar operations. The only difference in runtime occurs in the grad() function. Whereas in the regular perceptron, this involved computing a single dot product between two \\(1 \\times p\\) vectors, in the minibatch perceptron, this involves computing a matrix product between \\(X^{k \\times p}\\) and \\(w^{p \\times 1}\\). Calculating this matrix product is equivalent to calculating \\(k\\) dot products, each of which are O(\\(p\\)). Thus the runtime of the matrix product, and therefore the big O runtime of the minibatch perceptron algorithm, is O(\\(kp\\))."
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we investigated the perceptron algorithm, implementing it from scratch within the object-oriented framework provided by Professor Chodrow. We illustrated that the algorithm converges to a solution on linearly separable data of any finite number of dimensions but fails to converge to a solution on non-linearly separable data. To address this shortcoming, we implemented the minibatch perceptron algorithm. We found that when \\(k = 1\\), minibatch perceptron is similar to regular perceptron, and as we increase \\(k\\), the algorithm continues to find decision boundaries on linearly separable data. Furthermore, we discovered that when \\(k = n\\) and our learning rate \\(\\alpha\\) is adjusted appropriately, minibatch perceptron can converge to a (albeit imperfect) solution on data that is not linearly separable. Overall, this assignment provided me with a great opportunity to write up my first machine learning model from scratch and investigate its strengths and weaknesses."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html",
    "href": "posts/habitat-connectivity/index.html",
    "title": "Habitat Blocks and Connectivity",
    "section": "",
    "text": "When planning for conservation, accurately representing habitat blocks is crucial. The size, shape, location, composition, and connectedness of habitat blocks all provide valuable information regarding the ways that flora and fauna use the space, and the way that we account for these factors impacts the conservation decisions we might make. In this report, I evaluate the options available for one facet of identifying habitat blocks: whether adjacent shrubs/grasses and water should be included as part of a habitat block.\nI begin by explaining relevant background information and framing my research, before providing details on my methodology and results. After that, I interpret the results, provide recommendations based on my results, and conclude with current shortcomings and directions for future work."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#introduction",
    "href": "posts/habitat-connectivity/index.html#introduction",
    "title": "Habitat Blocks and Connectivity",
    "section": "",
    "text": "When planning for conservation, accurately representing habitat blocks is crucial. The size, shape, location, composition, and connectedness of habitat blocks all provide valuable information regarding the ways that flora and fauna use the space, and the way that we account for these factors impacts the conservation decisions we might make. In this report, I evaluate the options available for one facet of identifying habitat blocks: whether adjacent shrubs/grasses and water should be included as part of a habitat block.\nI begin by explaining relevant background information and framing my research, before providing details on my methodology and results. After that, I interpret the results, provide recommendations based on my results, and conclude with current shortcomings and directions for future work."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#background",
    "href": "posts/habitat-connectivity/index.html#background",
    "title": "Habitat Blocks and Connectivity",
    "section": "Background",
    "text": "Background\nVermont Act 171, enacted in 2016, amended Vermont’s planning statutes to encourage the preservation of forests and biodiversity by local municipalities. Specifically, Act 171 highlights the importance of habitat blocks and connectors and provides guidance as to how municipalities in Vermont should consider them in their planning efforts. In this manner, Act 171 defines how municipalities in Vermont, such as the Town of Middlebury, plan for conservation.\nSome key terms for this discussion include:\n\nForest block: The state defines a forest block as “a contiguous area of forest in any stage of succession and not currently developed for non-forest use” (VT ANR, 2018). Much of Vermont’s biodiversity comes from forest blocks, so protecting discrete regions of forest is important for the protection of native flora and fauna.\nHabitat block: According to Vermont’s BioFinder 3.0 Development Report, habitat blocks are regions of “contiguous forest and other natural habitats that are unfragmented by roads, development, or agriculture”(VT ANR Biofinder/VCD Team, 2019). A forest block would qualify as one or part of one habitat block, but the term habitat block also describes other types of important habitat such as wetlands and grasslands.\nHabitat connector: Also known as wildlife corridors, habitat connectors are the undeveloped land and water that link habitat blocks, allowing plants and animals to move between habitat blocks throughout the landscape (VT ANR, 2018).\n\nHabitat blocks and connectors provide numerous and varied benefits to Vermonters. For example, research shows that healthy forests contribute to cleaner water and air, which have important public health implications for people living in the area (VT ANR, 2018). Additionally, the forestry industry contributes $1.4 billion annually to Vermont’s economy, and forest-based recreation and tourism contributes both financially and culturally to the welfare of Vermont (VT ANR, 2018). Finally, habitat blocks and connectors also contribute to floodwater mitigation and carbon sequestration (VT ANR, 2018).\nOf course, habitat blocks and connectors have value not only for what they provide for Vermonters, but also for what they provide for other species. One important consideration for animals is the extent to which habitat blocks are connected to one another. Many species need to travel between different habitats throughout the year, so ensuring that habitat blocks are sufficiently connected is vital for their welfare. The ultimate goal of habitat planning is to connect all habitat blocks, so as to produce a landscape with a single and vast habitat block."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#research-framework",
    "href": "posts/habitat-connectivity/index.html#research-framework",
    "title": "Habitat Blocks and Connectivity",
    "section": "Research Framework",
    "text": "Research Framework\nIn this analysis, I begin with a script developed by Professor Howarth that identifies habitat blocks in Middlebury. I then modify his script to include (1) adjacent shrubs/grasses and (2) adjacent shrubs/grasses and water. With new results in hand, I assess how these changes affect the habitat blocks and their connectivity. The entire analysis is conducted in Python using Whitebox Tools. The analysis is guided by the following questions:\n\nHow does including adjacent bordering grass, shrubs and water affect what the model considers to be a habitat block in Middlebury?\nHow do these changes affect the connectivity of the habitat blocks?\n\nBecause I attach adjacent grasses/shrubs and water to preexisting blocks, the habitat blocks should expand slightly under my analysis. Furthermore, because adjacent natural cover could stretch from one habitat block to another, I anticipate that the addition of adjacent shrubs/grasses will connect some blocks that were separate under the original methodology, and the subsequent addition of adjacent water features will connect even more blocks. In this manner, using the revised model for future conservation work would make it appear that we are closer to the ideal fully connected habitat model."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#methods",
    "href": "posts/habitat-connectivity/index.html#methods",
    "title": "Habitat Blocks and Connectivity",
    "section": "Methods",
    "text": "Methods\nAs mentioned earlier, my objective is to produce two revised habitat blocks layers in Middlebury, and asses how their habitat blocks and connectivity differs from the original layer. The first revised layer pulls adjacent grasses/shrubs into the habitat blocks and the second revised layer pulls both adjacent grasses/shrubs and water into the habitat blocks.\nPer Professor Howarth’s methodology, the only input into the model is a 1-meter resolution land cover dataset provided by Professor Howarth. In the landcover dataset, each 1 meter by 1 meter pixel of land is labelled with a number that represents its current landcover, distinguishing between tree cover, shrubs/grasses, water, bare soil, agriculture, and several other classes of land-use. This dataset is a great choice for our purposes because it provides detailed, spatially accurate information about current land-use in Middlebury.\nTo generate the habitat blocks, the model first identifies places that have natural landcover (trees, grass/shrubs, and water) and places that have been developed (all other landcover classes). Then, the model identifies “core natural” areas by looking 50 meters within any contiguous area of natural landcover. This eliminates any tiny areas of natural landcover from consideration. After that, the model selects core natural areas for which more than 49% of their land area is tree cover. These steps were employed in all 3 iterations of the model and are illustrated in the diagram below (Figure 1).\n\n\n\nFigure 1: A diagram illustrating how we select core natural areas composed substantially by trees.\n\n\nThe next step is where my models differ from Professor Howarth’s original methodology. The original model brought adjacent forest cover back into the core natural areas and defined those as habitat blocks. Instead, one new methodology pulls adjacent forest cover and adjacent shrubs into the habitat blocks. The other new methodology pulls adjacent trees, shrubs, and water all into the habitat blocks. This is illustrated in the diagram below (Figure 2).\n\n\n\nFigure 2: The differences between the original and revised methodologies.\n\n\nFinally, all models determine the land area of each habitat block and select only those which are over 100 acres in area. My scripts are available at the following links:\n\nProfessor Howarth’s original method\nModified script: shrubs\nModified script: shrubs and water"
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#results",
    "href": "posts/habitat-connectivity/index.html#results",
    "title": "Habitat Blocks and Connectivity",
    "section": "Results",
    "text": "Results\nAfter running my scripts, I found that the number of habitat blocks fell by over half between the original results and the results after filling in adjacent shrubs. However, the number of habitat blocks did not decrease further after filling in adjacent water. To illustrate how the number of habitat blocks changes under the different methodologies, I created the following bar chart (Figure 3).\n\n\n\nFigure 3: Bar chart illustrating how the number of habitat blocks changes when we fill in adjacent shrubs and water.\n\n\nTo further explain the patterns in Figure 3 and illustrate how the different methodologies changed what we consider to be a habitat block, I also created the following close-up maps of habitat blocks. Please note that in the following figures, there may be additional habitat blocks within the map area; I display only the blocks required to illustrate particular patterns.\n\nFigure 4Figure 5Figure 6Figure 7Figure 8\n\n\n\n\n\nFigure 4: An example where filling in adjacent shrubs brought two habitat blocks together. On the left are the two habitat blocks under the original methodology, on the right is the combined block after filling in adjacent shrubs.\n\n\n\n\n\n\n\nFigure 5: An example where filling in adjacent water brought two blocks together. On the left are the two habitat blocks under the original methodology, on the right is the combined block after filling in adjacent shrubs and water.\n\n\n\n\n\n\n\nFigure 6: An example where filling in adjacent shrubs created a new habitat block. This block is totally absent under the original methodology but appears after filling in adjacent shrubs.\n\n\n\n\n\n\n\nFigure 7: An example where filling in adjacent water created a new habitat block. This block is totally absent under the original methodology and absent after filling in adjacent shrubs but appears after filling in adjacent water.\n\n\n\n\n\n\n\nFigure 8: The Ralph Myhre Golf Course, an example of an area of land being classified as a habitat block in all three methodologies when in reality it does not provide the same ecological benefits as a real habitat block should. From left to right we have the block under the original, shrub-inclusive, and water-inclusive methodologies."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#discussion",
    "href": "posts/habitat-connectivity/index.html#discussion",
    "title": "Habitat Blocks and Connectivity",
    "section": "Discussion",
    "text": "Discussion\nIn some ways, these results align with our expectations and in some ways, they differ. First of all, we expected to extend the boundaries of our preexisting blocks because we included adjacent shrubs and water, and this held true. For example, see the habitat block in Figure 8. On the left, we have our initial habitat block. In the middle, we have clearly filled in a number of holes in the middle of the block that are classified as grass/shrubs under the landcover dataset. And on the right, we have filled in a few ponds – for an example, look at the south-east corner of the block. However, Figure 8 is also indicative of one of our model’s errors. The so-called “habitat block” displayed in Figure 8 is actually a golf course! While parts of this habitat block are tree canopy, a large portion of it is regularly mowed for golfing purposes. Our model included this as a habitat block because the landcover dataset classified most of the golf course as grassland/shrubs. This issue could be addressed by changing the landcover labelling of the golf course to a different class and re-running the analysis, but this example is indicative of the fact that our current model is far from perfect.\nWhen it comes to the question of connectivity, Figure 3 clearly illustrates my results. As expected, the number of habitat blocks drops dramatically – from 147 to 70 – after filling in adjacent grasses/shrubs (Figure 3). This is due to scenarios such as the one shown in Figure 4, where bands of shrubs and trees provide connections between preexisting habitat blocks (Figure 4). There are also some cases, such as the one illustrated in Figure 6, where filling in adjacent shrubs makes the land area of a habitat block grow to more than the 100-acre area criterion, creating additional habitat blocks (Figure 6). But because the number of clumps drops by 77 between the original and shrub-inclusive methodology, the number of blocks connected by filling in adjacent shrubs clearly is far greater than the number of new blocks created.\nWe also anticipated that the number of habitat block clumps would decrease between the shrub-inclusive and water-inclusive methodology, because water may connect habitat blocks. However, the number of habitat blocks in Middlebury is actually equal under the two analyses (Figure 3). While this may initially be surprising, careful evaluation of the habitat blocks produced by the methodologies reveals the cause. There were some instances, such as the example illustrated in Figure 5, where filling in adjacent water connected habitat blocks (Figure 5). However, there were an equal number of cases, such as the one illustrated in Figure 7, where filling in adjacent water allowed for a new area of core natural area to meet the required 100-acre threshold to be considered a habitat block. Thus, while the addition of water in the model does increase the connectivity of the blocks that were previously part of the model, it also creates new blocks, raising the statistic reported in my graph (Figure 3).\nThe new block displayed in Figure 7 is almost exclusively Lake Dunmore and the region connecting the two previous blocks in Figure 5 is largely Otter Creek. It is worth asking whether these should really be considered habitat. While they certainly provide habitat for aquatic animals and some benefits for land animals, they do not really provide habitat for land animals. It may make sense to include water when considering all animal habitats, but if restricted to forest habitat blocks like Act 171 discusses then this would not suffice."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#recommendations",
    "href": "posts/habitat-connectivity/index.html#recommendations",
    "title": "Habitat Blocks and Connectivity",
    "section": "Recommendations",
    "text": "Recommendations\nMy results indicate that the inclusion of adjacent grasses/shrubs dramatically improves the connectivity of Middlebury’s habitat blocks. Since our goal is to maximize the connectivity of habitat blocks in Middlebury, perhaps we ought to focus on improving these shrubland connectors by formally protecting them and potentially reforesting them."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#conclusion",
    "href": "posts/habitat-connectivity/index.html#conclusion",
    "title": "Habitat Blocks and Connectivity",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, I found that pulling in adjacent shrubs/grasses dramatically increases the connectivity of habitat blocks, while pulling in adjacent water does less to improve connectivity. Furthermore, both the incorporation of adjacent shrubs/grasses and the incorporation of water creates additional habitat blocks that should at least be evaluated by planners.\nWhile I would love to be able to suggest one of the three models as the “best” model, I cannot say that one is unilaterally better than the others. Rather, one’s intended purpose ought to determine their choice of habitat block layer. Act 171 emphasizes the importance of forested habitat blocks in particular – if one’s focus is strictly on forest habitats, they should opt for the original or shrub-inclusive option. If one seeks to identify habitat blocks of any type, then including water might make sense. Other purposes, such as modeling the habitat of aquatic animals in particular, might require different models altogether.\nAs we have seen, the current work is imperfect. For example, all three methodologies classify the golf course as a habitat block. Future work could modify our landcover dataset to make our results more meaningful. Additionally, while this report presents an approach for identifying habitat blocks and priority lands to improve connectivity, it does nothing to cross-reference these habitats and connectors with currently conserved areas. Future work should identify which of these areas are and are not conserved in order to determine particular locations to target for future conservation easements."
  },
  {
    "objectID": "posts/habitat-connectivity/index.html#references",
    "href": "posts/habitat-connectivity/index.html#references",
    "title": "Habitat Blocks and Connectivity",
    "section": "References",
    "text": "References\n\nVT ANR. (2018). Act 171 Guidance. https://anr.vermont.gov/sites/anr/files/co/planning/documents/guidance/Act171Guidance.pdf\nVT ANR Biofinder/VCD Team. (2019). BioFinder 3.0 Development Report. https://drive.google.com/file/d/1eiLfinDcBmC4skrvpNl3RVbkdLxSHzfj/view"
  },
  {
    "objectID": "posts/imdccal-r-package/index.html",
    "href": "posts/imdccal-r-package/index.html",
    "title": "Water Chemistry R Package",
    "section": "",
    "text": "I contributed to this R package as part of my work at the Rocky Mountain Inventory & Monitoring Network. Specifically, I added functions to help users format CCAL data into the EQuIS Electronic Data Deliverable format.\n\n  Visit Package Website"
  },
  {
    "objectID": "posts/ml-final-project/index.html",
    "href": "posts/ml-final-project/index.html",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github\n\n\n\nIn countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover.\n\n\n\nNASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population.\n\n\n\n\n\nWith this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/ml-final-project/index.html#abstract",
    "href": "posts/ml-final-project/index.html#abstract",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "Our project takes on the challenge of predicting population density in regions lacking data. Leveraging landcover image data and tract geometry, our approach involves computing zonal statistics and employing machine learning models. With this problem in mind, we employ satellite data from Connecticut due to its completeness and its potential to also be applied to other Northeastern States within the US. We create Linear Regression models and Spatial Autoregression models with our zonal statistics and tract data. We gauge their efficacy based on their mean-squared error and \\(R^2\\) value. Through this, we find that Linear Regression with No Penalty works best out of our Linear Regression models and our Endogenous Spatial Autoregression model works better than the Exogenous model. Furthermore, we conclude that Spatial Autoregression is more effective at predicting population density than Linear Regression. In regions without adequate census data, the Exogenous model would improve estimations of population density by taking into account the landcover of a given region and its neighbors. Our code and datasets are available through our Github"
  },
  {
    "objectID": "posts/ml-final-project/index.html#introduction",
    "href": "posts/ml-final-project/index.html#introduction",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "In countries such as the US, there is a large and accurate amount of census data. However there are many ountries in areas where the resources for gathering census data is lesser. This is potentially due to geographic inaccessibility, political conflict, administrative failiure, and as mentioned previously, a lack of resources. Thus, we want a way to predict human populations around the world with the data of the land itself, satellite imagery. With this imaging, the geography is divided into classes which we can then use as variables for our model. Research into this topic has stagnated to a degree, however Tian et al. (2005) produced a hallmark paper which tested the effectivity of modeling population with land cover data. It found that a similar model could have “feasible” and can have “high accuracy”. They utilized Linear Regression, and also manually broke down China into even 1km by 1km cells. Because of availablity of census data, we instead used census tracts, but we continued with the idea of utilizing Linear Regression. With some exploratory graphs of Connecticut, we discovered there might be a Spatial Pattern within our data. In order to take this into account during modeling, we started researching into machine learning algorithms with a spatial component. We came across a paper by Liu, Kounadi, and Zurita-Milla (2022), which concluded that models with a spatial component, such as spatial lag, garner better results than those without. They used spatial lag, and eigvenvectors spatial filtering to predict things beyond our datasets such as soil types. Thus, we sought to create Linear Regression Models and Spatial Autoregressive models, and compare the them to see which is more effective in predicting population density based on land cover."
  },
  {
    "objectID": "posts/ml-final-project/index.html#values-statement",
    "href": "posts/ml-final-project/index.html#values-statement",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "NASA in a webinar session called “Humanitarian Applications Using NASA Earth Observations” presented how satellite remote-sensing data could be useful in monitoring humanitarian conditions at refugee settlements. Human settlements could be detected through remote sensing images and therefore could be used to predict the population in a region. This talk alerted us that we still lack necessary population data in many parts of the world, but also demonstrated how remote sensing could be a powerful tool in tackling this problem and solving lack of population data in different countries. Thus, we decide to investigate the connection between remote sensing land cover data and population density in a context with better data coverage.\nThis type of model would be most beneficial by governments and government organizations. These users would most likely be hospital contractors, policy makers, emergency services providers such as ambulances and firefighers, and sociologists. Population census data is crucial for policy makers as it assists in city management so that the equitable distribution of resources can be better calculated.\nThe implications extend beyond helping users. Real people would be affected by this technology. Those who are workers in fields such as emergency service work, or school teachers who might have been over-worked previously may be relieved by the building of new hospitals and schools to compensate for population changes. However, the negative effects are also extremely real.\nImagining that this model expanded beyond the barriers of Connecticut and is being used in countries with much lower census data such as Brazil, there might be a calculation for a forestry company to continue harvesting wood from the Amazon, but they do not want to affect populations. Our algorithm calculates there are very few people in the area, as there is very dense land cover in the Amazon. This company starts to cut down trees and discovers that they are in an area of Indigenous peoples. A minority group that is already negatively affected continues to be disenfranchised. The issue of undercalculating the population density in an area can also affect the amount of resources a policymaker might provide to a region with a much greater population and lacking resources. This would also continue to negatively impact an already negatively impacted area.\nUltimately, the world would be a more equitable and sustainable place if this type of technology could assist countries lacking population data. The positive aspects of providing data where there is none provides the potential for great resource partioning, and better understanding of a countries population."
  },
  {
    "objectID": "posts/ml-final-project/index.html#materials-and-methods",
    "href": "posts/ml-final-project/index.html#materials-and-methods",
    "title": "Machine Learning Final Project",
    "section": "",
    "text": "With this project being the entire state of Connecticut, we utilized landcover data, population, shape files for graphing, and synthesized data which combined our various data sets into manageable datasets suitable for modeling.\nThe bread and butter of our data stems from a 1-meter resolution landcover imagery covering the entire state of Connecticut. Derived from NAIP, the data has already been processed such that every pixel represents a certain class of landcover.\nAt over 800 MB, the dataset is too large to share via GitHub, and is downloadable by clicking on the first option at this link. This landcover dataset was one of the most complete datsets we could find, which is why we wanted to use it for our modelling.\nOur other data sources are the geometries and population data on the Census tract level for the state of Connecticut. We downloaded tract geometries directly into our Jupyter Notebook final_project.ipynb using the Pygris package, and we downloaded the population data from Social Explorer, storing it at data/population.csv.\n\n\n\nFirst, we clean and prepare our data for the model. We start by combining our Tract Geometry of CT with the Population Data of CT to form a new dataset. We utilize both the CT Landcover Data and the Tracts Data in a calculation of Zonal Statistics. This means we calculate the proportion of pixels within each tract that are of a given landcover class. This then is saved as a combined dataset which we then continue to clean by imputing values, performing more advanced Zonal Statistics, and dropping any NA Columns. From there, we are left with data ready to be used in a model.\nThe flowchart below more elegantly outlines this process\n\n\n\n\n\nflowchart LR\n  A(Population Data) --&gt; B(Tracts Data)\n  C(Tracts Geometry Data) --&gt; B(Tracts Data)\n  B --&gt; D{Zonal Statistics}\n  E(CT Landcover Data) --&gt; D{Zonal Statistics}\n  D{Zonal Statistics} --&gt; F(Combined Data)\n  F(Combined Data) --&gt; |Impute Data| G[Ready for Model]\n  F --&gt; |Additional Landcover Statistics| G[Ready for Model]\n  F --&gt; |Drop Uncommon Landcover| G[Cleaned Data]\n\n\n\n\n\n\nWe then implement three types of Linear Regression:\n\nLinear Regression with No Penalty Term\nLinear Regression with \\(\\ell_1\\) Regularization (Lasso Regression)\nLinear Regression with \\(\\ell_1\\) Regularization (Ridge Regression)\n\nBy utilizing the \\(R^2\\) and Mean Squared Error, we quantified the success of each of our models against one another as well as comparing them to sci-kit learn’s own implementations of each of these Linear Regression Models.\nFollowing Linear Regression, we then wanted to implement two types of Spatial AutoRegression:\n\nEndogenous Spatial Autoregression\nExogenous Spatial Autoregression\n\nAs our data can be plotted on a map of Connecticut, we felt it would be amiss to not explore Spatial Autogression. Through this style of model, we can take into account the spatial aspect of each tract when we are predicting. We chose both Endogenous and Exogenous Models. Endogenous Models take into account the neighboring tract population densities of a given tract. Exogenous Models take into account the zonal statistics of a given tract’s neighbors.\nWe merge our data with shape file and calculate the spatial lag of a each tract’s neighbors. The spatial lag is this case is the average population density of a given tracts of land. We also calculate the average landcover types of a given’s tracts neighbors.\nIn total, we create 8 models which we compare in order to determine the best way to predict population density with landcover data\n\n\n\n\n\nflowchart \nA[Cleaned Data] --&gt; B{No Penalty LR}\nA --&gt; C{Lasso LR}\nB --&gt; K{ours}\nB --&gt; L{sci-kit learn}\nC --&gt; G{ours}\nC --&gt; H{sci-kit learn}\nA --&gt; D{Ridge LR}\nD --&gt; I{ours}\nD --&gt; J{sci-kit learn}\nA --&gt; |Spatial Lag Pop Density| E{Endogenous}\nA --&gt; |Spatial Lag Landcover| F{Exogenous}"
  },
  {
    "objectID": "posts/ml-final-project/index.html#acquire-tract-geometries",
    "href": "posts/ml-final-project/index.html#acquire-tract-geometries",
    "title": "Machine Learning Final Project",
    "section": "Acquire Tract Geometries",
    "text": "Acquire Tract Geometries\nAs a test of concept, lets utilize the pygris library to access the CT tracts information and then let’s do a simple plot to ensure it’s correct.\n\n# Download geometry\nct_tracts = tracts(state = \"CT\", cb = True, cache = True, year = 2016)\n\n# Display geometry\nfig, ax = plt.subplots()\nct_tracts.plot(ax = ax)\nplt.title(\"Tracts Cartographic Boundaries\");\n\nUsing FIPS code '09' for input 'CT'"
  },
  {
    "objectID": "posts/ml-final-project/index.html#calculate-population-density",
    "href": "posts/ml-final-project/index.html#calculate-population-density",
    "title": "Machine Learning Final Project",
    "section": "Calculate Population Density",
    "text": "Calculate Population Density\nBefore we begin our journey into zonal statistics and eventually creating a predictive model, we first want to understand what the population density looks like in Connecticut. We have some general hypotheses that the areas around New Haven and Hartford are going to have higher amounts of population, and we also expect to see some small pockets of communities around Connecticut.\n\n# Import tracts population data\npop = pd.read_csv(\"../data/population.csv\")\n\n# Convert data type so join key matches\nct_tracts[\"Geo_TRACT\"] = ct_tracts[\"TRACTCE\"].astype(int)\n\n# Join attributes to geometry\ntracts = ct_tracts.merge(pop, how = \"inner\", on='Geo_TRACT')\n\n# Project tracts\ntracts = tracts.to_crs(\"EPSG:3857\")\n\n# Calculate area in KM^2\ntracts[\"Area\"] = tracts.area/1000**2\n\n# Calculate population density\ntracts[\"PopDensity\"] = tracts[\"SE_A00001_001\"]/tracts[\"Area\"]\n\n# Create map\ntracts.plot(\"PopDensity\", legend = True);"
  },
  {
    "objectID": "posts/ml-final-project/index.html#first-steps",
    "href": "posts/ml-final-project/index.html#first-steps",
    "title": "Machine Learning Final Project",
    "section": "First steps",
    "text": "First steps\nHere we open our path to our file, and more importantly, we set up our data to be used in zonal statistics. .read turns our data into a Numpy Array. Following this we are going to .transform our data, which means we are going to take the pixel locations of our coordinates (row col) and map them to our spatial coordinates (x, y). These coordinate values are relative to the CRS (Coordinate Reference System) which we defined earlier as “EPSG:2234”\n\n%%script echo skipping\n#the data can be accessed from https://coastalimagery.blob.core.windows.net/ccap-landcover/CCAP_bulk_download/High_Resolution_Land_Cover/Phase_2_Expanded_Categories/Legacy_Land_Cover_pre_2024/CONUS/ct_2016_ccap_hires_landcover_20200915.zip\nraster_path = '../data/ct_2016_ccap_hires_landcover_20200915.tif'\nlandcover = rasterio.open(raster_path)\narr = landcover.read(1)\naffine = landcover.transform\n\nskipping"
  },
  {
    "objectID": "posts/ml-final-project/index.html#performing-zonal-statistics",
    "href": "posts/ml-final-project/index.html#performing-zonal-statistics",
    "title": "Machine Learning Final Project",
    "section": "Performing Zonal statistics",
    "text": "Performing Zonal statistics\nIt’s as simple as importing rasterstats. We have handled the important data manipulation, and now it’s basically plug and play! One function to note is .to_crs which takes in given coordinate reference system and transforms all the points in our dataframe to match that system.\nThe rasterstats library is very good at getting information from rasters, and we can in fact gain more information by using categorical = True. This allows to see the amount of each type of pixel at a given tract.\n\n%%script echo skipping\ndf_new = zonal_stats(zone, arr, affine=affine, categorical = True)\n\nskipping\n\n\nTaking a look at our dataframe, we can confirm that each column is a type of pixel and each row is a tract\n\n%%script echo skipping\ndf_categorical = pd.DataFrame(df_new)\ndf_categorical\n\nskipping"
  },
  {
    "objectID": "posts/ml-final-project/index.html#visualizing-zonal-stats",
    "href": "posts/ml-final-project/index.html#visualizing-zonal-stats",
    "title": "Machine Learning Final Project",
    "section": "Visualizing Zonal Stats",
    "text": "Visualizing Zonal Stats\nNow that we have information on the amount of each pixel at a given tract, we can find the most common pixel per tract by using the function .idxmax() which will through each row and find the column with the largest value.\n\n%%script echo skipping\ndf_categorical['max_type'] = df_categorical.idxmax(axis=1)\ncombined_df = pd.concat([tracts, df_categorical], axis=1)\ncombined_df['max_type'] = combined_df['max_type'].astype(str)\n\nskipping\n\n\n\n%%script echo skipping\ncombined_df.plot(\"max_type\", legend = True);\n\nskipping\n\n\n\nSaving this data\nThese statistics took quite a while to run, and it may be beneficial to save this data as a csv to continue running statistics in the future\n\n%%script echo skipping\n\ncombined_df.to_csv('../data/combined_data.csv', index=False)\n\nskipping"
  },
  {
    "objectID": "posts/ml-final-project/index.html#data-preparation-1",
    "href": "posts/ml-final-project/index.html#data-preparation-1",
    "title": "Machine Learning Final Project",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst, we import our data.\n\n# Import and display data\ndata = pd.read_csv(\"../data/combined_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\nNaN\nNaN\n27939.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\nNaN\nNaN\n13728.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\nNaN\n20584.0\n80161.0\n99956.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\nNaN\nNaN\n9940.0\n68655.0\n486.0\nNaN\nNaN\nNaN\nNaN\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nLooks like there is some missing data in tracts that contain no pixels of a certain class. Let’s impute 0 for all NaN values.\n\n# Impute 0 for missing data\nprint(\"Before imputation, there were\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata[pd.isnull(data.iloc[:,68:-1])] = 0\nprint(\"After imputation, there are\", pd.isnull(data.iloc[:,68:-1]).sum().sum(), \"NaN values.\")\ndata.head()\n\nBefore imputation, there were 5774 NaN values.\nAfter imputation, there are 0 NaN values.\n\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n18\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n136572.0\n423692.0\n142589.0\n1378858.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.0\n0.0\n27939.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.0\n0.0\n13728.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.0\n20584.0\n80161.0\n99956.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.0\n0.0\n9940.0\n68655.0\n486.0\n0.0\n0.0\n0.0\n0.0\n11\n\n\n\n\n5 rows × 87 columns\n\n\n\nNow that we have complete data, we can calculate the proportion of pixels belonging to each class.\n\n# Calculate total number of pixels in each tract\ndata[\"sum\"] = data.iloc[:,68:-1].sum(axis = 1)\n\n# Calculate proportion of pixels belonging to each class\ndata.iloc[:,68:-2] = data.iloc[:,68:-2].div(data['sum'], axis=0)\n\n# View data\ndata.head()\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nTRACTCE\nAFFGEOID\nGEOID\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n...\n19\n20\n21\n22\n7\n6\n0\n23\nmax_type\nsum\n\n\n\n\n0\n9\n1\n11000\n1400000US09001011000\n9001011000\n110.0\nCT\n4473567\n3841130\nPOLYGON ((-8191739.173321358 5013468.769836016...\n...\n0.069327\n0.023331\n0.225616\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n6111530.0\n\n\n1\n9\n1\n20800\n1400000US09001020800\n9001020800\n208.0\nCT\n2315472\n0\nPOLYGON ((-8187432.3302968815 5025136.84023609...\n...\n0.000000\n0.012054\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n11\n2317904.0\n\n\n2\n9\n1\n21400\n1400000US09001021400\n9001021400\n214.0\nCT\n1640443\n0\nPOLYGON ((-8189589.702028457 5021116.993618919...\n...\n0.000000\n0.008350\n0.000000\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1644135.0\n\n\n3\n9\n1\n22200\n1400000US09001022200\n9001022200\n222.0\nCT\n1442382\n117063\nPOLYGON ((-8186995.178656538 5019223.193891366...\n...\n0.013289\n0.051753\n0.064533\n0.000000\n0.0\n0.0\n0.0\n0.0\n2\n1548918.0\n\n\n4\n9\n1\n43100\n1400000US09001043100\n9001043100\n431.0\nCT\n6652660\n58522\nPOLYGON ((-8178763.436270848 5029936.759394648...\n...\n0.000000\n0.001484\n0.010249\n0.000073\n0.0\n0.0\n0.0\n0.0\n11\n6698858.0\n\n\n\n\n5 rows × 88 columns\n\n\n\n\n# Separate predictors and outcome\nX = data.iloc[:,68:-2]\ny = data[\"PopDensity\"]\n\nWe had an issue where our results were not quite matching those of scikit-learn and we discovered that this was due to a way we set up our dataset. Since we have calculated the proportion of pixels in each tract belonging to each landcover class, the landcovers sum to 1 in every row. Since we create an additional column of ones in order to calculate a y-intercept for linear regression with gradient descent, this means that our y-intercept column is equal to the sum of our other columns. In other words, the constant column is linearly dependent on our other predictor columns. To address this issue, we drop some columns that seem unimportant. Specifically, these columns are mostly zero, meaning that they are not very common in Connecticut anyway.\n\n# Drop some landcovers to address issue of linear combination \nX = X[['2', '5', '11', '12', '8', '13', '14', '15', '20', '21']]"
  },
  {
    "objectID": "posts/ml-final-project/index.html#linear-regression-with-no-penalty-term",
    "href": "posts/ml-final-project/index.html#linear-regression-with-no-penalty-term",
    "title": "Machine Learning Final Project",
    "section": "Linear Regression with No Penalty Term",
    "text": "Linear Regression with No Penalty Term\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit a linear regression model with scikit-learn. We do this simply to verify against our own implementation of linear regression.\n\n# Fit model\n# Doing this just for the purpose of seeing what it looks like\n# We can use the results from this package to verify that our implementation is working properly\n\n#Train and test split creation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nLR_s = LinearRegression() \n\nm = LR_s.fit(X_train, y_train)\n\nLinear regression seeks to minimize the mean squared error, so we report the mean square error from scikit-learn’s model here.\n\n# MSE\nmean_squared_error(y_train, LR_s.predict(X_train))\n\n227396.12768129486\n\n\nLet’s check the \\(R^2\\) value of our model. Recall that \\(R^2\\) is also known as the coefficient of determination, and it represents the proportion of variation in one’s outcome variable that is explained by one’s model.\n\n# R^2 value\nm.score(X_train, y_train)\n\n0.7723901708932351\n\n\nWith an \\(R^2\\) value of roughly \\(0.772\\), our ordinary least squares regression model accounts for about \\(77.2\\)% of the variation of the population densities in Connecticut’s tracts.\nLet’s inspect the y-intercept and coefficients to verify that our coefficients seem logical.\n\n# Y-intercept\nprint(\"Intercept:\", m.intercept_)\n\n# Min and max population density\nprint(\"Population Density Min:\", y_train.min())\nprint(\"Population Density Max:\", y_train.max())\n\nIntercept: 983.2395073145441\nPopulation Density Min: 0.0\nPopulation Density Max: 6084.305602883675\n\n\nSince our predictions are proportions of pixels in a tract of a given landcover, it is impossible for all of our predictors to be zero. Basically this means that no tract will be in the situation where all variables are equal to zero, leaving the y-intercept as its population density. However, in theory, in the absence of any landcover pixels, the population density would be \\(983\\) people per square kilometer. With y_train ranging from 0 to 6084, this seems somewhat reasonable.\n\n# Variable coefficients\nm.coef_\n\narray([  3409.40801231,  -2942.65854175,   -917.38563842,  -4525.6598175 ,\n          668.32452458,  -2125.96537456,  -1746.52921947,  -1576.35637606,\n       -13652.09857612,  -1417.12360532])\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nMost of these coefficients are negative, indicating that as the proportion of pixels representing a given landcover type increases, the population density of the tract decreases. The only positive values are the coefficient of 2, which represents developed impervious landcover, and the coefficient of 8, which represents grassland/herbaceous landcover. We definitely anticipated a positive coefficient for 2, as impervious developed surfaces like buildings and roads are a major marker of human presence. The documentation indicates that while this landcover cannot be used for tilling, it can be used for grazing, so perhaps the positive coefficient is indicative of population density associated with farming. Also, Connecticut is generally forested in rural areas, so grassy areas are likely in suburbia or near urban areas. The magnitude of 2 is much larger than 8, however, indicating that developed impervious landcover is the most important factor increasing population density.\nThe negative coefficients correspond to developed open space, mixed forest, shrub, palustrine forested wetland, palustrine scrub/shrub wetland, palustrine emergent wetland, barren land, and open water. With the exception of developed open space, these landcovers are generally not associated with population density. And developed open space does not necessitate people living in that location – people could live in one tract and commute to a tract with developed open space for recreational purposes, for example. Thus it makes sense that increased values of these variables contribute to less population density.\n\n\nTest Model\nNow that we have evaluated the basic interpretation of our model on our training data, let us check the performance of our model on our testing data. First, we calculate our predictions.\n\n# Create predictions (on test data)\npreds = LR_s.predict(X_test)\n\nLet us inspect the mean square error of our model on the testing data.\n\n# MSE\nmean_squared_error(y_test, preds)\n\n373799.85511504946\n\n\nAt \\(373,800\\), the mean squared error of our model on the testing data is much larger than the mean squared error on the training data, which was \\(227,396\\). This makes sense as our model was fit specifically to the tendencies of the training data.\nTo evaluate the explanatory power of our model, let’s also calculate the \\(R^2\\) value on our testing data.\n\n# Test R^2 value\nr2_score(y_test, preds)\n\n0.7086666350845903\n\n\nAs one might anticipate, the \\(R^2\\) value of the testing data is lower than the training data. However, at \\(0.709\\), the \\(R^2\\) of the testing data is only \\(0.064\\) lower than the \\(R^2\\) of the training data. In other words, our model explains \\(6.4\\)% less of the variation of the population density in our testing data. This is not a negligible amount, but we are still relatively satisfied with a model that explains over \\(70\\)% of the variation in population density.\n\n\n\nOur Implementation\nWe implemented ordinary linear regression with gradient descent in linear_regression.py. Let us train the model using our implementation and verify that our results roughly match those of scikit-learn.\n\nTrain Model\nFirst, we need to convert our training and testing data to the torch.tensor format to match the expected input of our model. We also add a column of ones at the end of the X training and testing data for the purposes of training our y-intercept.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_train_torch = torch.cat((torch.tensor(X_train.values), torch.ones((X_train.shape[0], 1))), 1)\ny_train_torch = torch.tensor(y_train.values)\nX_test_torch = torch.cat((torch.tensor(X_test.values), torch.ones((X_test.shape[0], 1))), 1)\ny_test_torch = torch.tensor(y_test.values)\n\nNow that we have our data in the appropriate format, we can train our model.\n\n# fit linear regression model\nLR = LinearRegress()\nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec = []\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt.step(X_train_torch, y_train_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR.loss(X_train_torch, y_train_torch) \n    loss_vec.append(loss)\n\nLet’s inspect the evolution of our loss function (mean squared error) to verify that our model has converged to a solution.\n\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n# MSE\nprint(\"Mean squared error after training:\", LR.mse(X_train_torch, y_train_torch).item())\n\nMean squared error after training: 227396.1319467965\n\n\n\n\n\n\n\n\n\nGreat! After \\(500,000\\) iterations, our mean squared error is \\(227,396.132\\), which is essentially equivalent to the mean squared error of \\(227,396.128\\) found by scikit-learn.\nLet’s inspect the y-intercept and coefficients to verify that they are similar to scikit-learn’s solution.\n\n# Y-intercept\nLR.w[-1]\n\ntensor(983.0291, dtype=torch.float64)\n\n\nThis y-intercept is also similar to the figure of \\(983.2395\\) reported by scikit-learn.\n\n# Variable coefficients\nprint(\"Coefficients:\", LR.w[:-1])\n\n# Differences in signs\nprint(\"Differences in sign:\", (torch.tensor(m.coef_)*LR.w[:-1]&lt; 0).sum().item())\n\n# Maximum difference in coefficient\nprint(\"Maximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR.w[:-1])).max().item())\n\nCoefficients: tensor([  3409.6334,  -2942.3605,   -917.2350,  -4527.3370,    669.5791,\n         -2126.6514,  -1721.1996,  -1578.8776, -13651.8922,  -1416.8399],\n       dtype=torch.float64)\nDifferences in sign: 0\nMaximum coefficient difference: 25.329603726808955\n\n\nOur coefficients are very similar to those from scikit-learn’s solution! All coefficients have the same sign and the maximum difference between a coefficient in our two models is \\(25\\). Considering the magnitude of the coefficients, this difference is relatively small. Thus the interpretation of our model matches the interpretation of scikit-learn’s model, making us confident that we have implemented linear regression correctly.\n\n# Compute R^2 score\nLR.r2(X_train_torch, y_train_torch)\n\ntensor(0.7724, dtype=torch.float64)\n\n\nOur \\(R^2\\) value is the same as scikit-learn’s.\n\n\nTest Model\nNow we inspect our model’s performance on the testing data.\n\n# MSE\nLR.mse(X_test_torch, y_test_torch)\n\ntensor(373801.8165, dtype=torch.float64)\n\n\nAt \\(373,802\\), our implementation’s testing MSE is very similar to scikit-learn’s \\(373,800\\), indicating similar performance. Once again, this is substantially larger than the training MSE, indicating that our model did not generalize perfectly.\n\n# R^2 value\nLR.r2(X_test_torch, y_test_torch)\n\ntensor(0.7087, dtype=torch.float64)\n\n\nScikit-learn’s testing \\(R^2\\) value was also \\(0.7087\\)! Overall, it appears that we have succesfully implemented linear regression in a manner that achieves similar results to scikit-learn."
  },
  {
    "objectID": "posts/ml-final-project/index.html#linear-regression-with-ell_1-regularization",
    "href": "posts/ml-final-project/index.html#linear-regression-with-ell_1-regularization",
    "title": "Machine Learning Final Project",
    "section": "Linear Regression with \\(\\ell_1\\) Regularization",
    "text": "Linear Regression with \\(\\ell_1\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Lasso and inspect the resulting model. As before, we do this simply to verify against our own implementation.\n\n# Fit model\nLR_s_l1 = Lasso(alpha = 1)\n\nm = LR_s_l1.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l1.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 236944.40360579122 \nR^2: 0.7628329217280919 \nY-intercept: -191.72480712350455 \nCoefficients:\n [ 4358.88007237 -1696.29039686   224.06934601    -0.\n     0.            -0.            -0.            -0.\n -6028.66936275  -279.44578393]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is slightly larger and the training \\(R^2\\) is slightly smaller than linear regression with no regularizer, which makes sense as we have applied a penalty to help prevent overfitting. The y-intercept is closer to \\(0\\), and many of the coefficients are equal to exactly \\(0\\), making them more interpretable: some coefficients simply do not matter! In this model, landcover 2 (developed impervious) again has a positive coefficient, and with a large magnitude, it remains the main driver in high population density. There is one other variable, 11 (mixed forest), which has a positive coefficient. Interestingly, it was negative in the other model, leading to confusion in its interpretation. But with a somewhat small magnitude, this variable overall has a minor impact on population density, only changing the population density by 224 people per square kilometer as its value increases from 0 to 1. With the \\(\\ell_1\\) regularizer, the landcovers of shrub, grassland/herbaceous, palustrine forested wetland, palustrine scrub/shrub wetland, and palustrine emergent wetland are now equal to zero. These coefficients must not have been that important to the model, as our regularizer made them have zero impact on population density. Variables with negative coefficients are developed open space, barren land, and open water, probably for the same reasons that they were negative earlier.\n\n\nTest Model\nNext, we discover whether the \\(\\ell_1\\) regularizer actually made the model generalize better to the testing data.\n\n# Create predictions (on test data)\npreds = LR_s_l1.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 390639.95954704983 \nR^2: 0.6955417389066836\n\n\nOur new MSE of \\(390,639\\) is actually larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_1\\) regularizer did not help our model generalize to the testing data. Furthermore, the \\(R^2\\) value was larger in the previous model, meaning that the model with no regularizer explained more variation in the outcome variable.\n\n\n\nOur Implementation\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results match those of scikit-learn. Note that scikit-learn uses an algorithm known as coordinate descent to find their solution, but we learned about gradient descent in this class. Coordinate descent is better suited for lasso regression because it allows some coefficients to equal exactly zero. Gradient descent with the \\(\\ell_1\\) norm makes some coefficients much smaller, but does not cause any of them to equal exactly zero. To mimick their results, in our implementation we set our coefficients equal to zero if they are below a selected threshold. We allow our model \\(5000\\) iterations to begin learning the coefficients before applying this threshold.\n\nTrain Model\n\n# fit linear regression model\nLR_l1 = LinearRegress(penalty = \"l1\", lam = 1) # 1 in scikit-learn\nopt_l1 = GradientDescentOptimizer(LR_l1)\n\n# initialize vector to record loss values\nloss_vec_l1 = []\n\n# fit model\nfor i in range(50000):\n    # update model\n    opt_l1.step(X_train_torch, y_train_torch, alpha = 0.001)\n\n    # set coefs equal to zero after model has had enough learning time\n    if i &gt; 5000:\n        LR_l1.w[torch.abs(LR_l1.w) &lt; 500] = 0\n\n    # calculate and record loss\n    loss = LR_l1.loss(X_train_torch, y_train_torch) \n    loss_vec_l1.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l1, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIt appears that our model converged to a solution with a similar loss function value! Note that the small upwards blip in the loss function occured at iteration \\(5000\\) when we began allowing our model to set some coefficients equal to zero. Let us inspect our results and compare them to scikit-learn’s output.\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l1.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l1.w[-1],\n      \"\\nCoefficients:\\n\", LR_l1.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l1.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l1.w[:-1])).max().item())\n\nMSE: 233736.21710488532 \nR^2: tensor(0.7660, dtype=torch.float64) \nY-intercept: tensor(0., dtype=torch.float64) \nCoefficients:\n tensor([ 4257.6646, -1977.7857,     0.0000,     0.0000,     0.0000,     0.0000,\n            0.0000,     0.0000, -7781.5773,  -551.3119], dtype=torch.float64) \nDifferences in sign: 0 \nMaximum coefficient difference: 1752.9079262978257\n\n\nOur model’s MSE of \\(233,736\\) is slightly smaller than scikit-learn’s MSE of \\(236,944\\) and our model’s \\(R^2\\) of \\(0.7660\\) is slightly larger than scikit-learn’s \\(R^2\\) of \\(0.7628\\), indicating that our linear regression model with the \\(\\ell_1\\) norm performed marginally better than theirs. This difference could have occured due to differences in the optimizer and the number of training iterations. Additionally, these MSE and \\(R^2\\) metrics are both slightly worse than what our implementation achieved with no regularizer, which makes sense as we are attempting to prevent overfitting.\nOne should note that our workaround for setting coefficients equal to zero is not ideal for several reasons. First, we hard-coded a certain threshold for choosing coefficients to set equal to zero, as well as a certain number of iterations at which to begin checking for these low-magnitude coefficients. Most users probably do not want to decide on such a threshold. Second, our method did not exactly replicate the output from scikit-learn. Adjusting our parameters to exactly reproduce the coefficients set to zero proved difficult, and the best we were able to do involved setting the y-intercept and landcover 11 equal to zero, while they were nonzero in scikit-learn’s solution. Landcover 11 represents mixed forest and was the one coefficient with a somewhat counterintuitive value in scikit-learn’s model, so in terms of interpretation, our new model still makes sense. All coefficients have the same sign as scikit-learn’s model with similar magnitudes, making us confident that our model is successfully describing the situation, despite the minor discrepancies.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l1.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l1.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(384765.6761, dtype=torch.float64) \nR^2: tensor(0.7001, dtype=torch.float64)\n\n\nThese values are pretty similar to the ones we have seen already. At \\(384,766\\), our implementation’s MSE is less than scikit-learn’s \\(390,640\\), and at \\(0.7001\\), our implementation’s \\(R^2\\) is slightly more than scikit-learn’s \\(0.6955\\). This means that our model generalized slightly better to the testing data, in addition to performing better on the training data. Again, this can likely be explained by differences in the optimization method and the number of training iterations.\nFurthermore, this MSE is slightly larger than the \\(373,802\\) figure returned by our implementation of linear regression with no penalty term, and this \\(R^2\\) is slighly smaller than the \\(0.7087\\) figure, indicating that linear regression with the \\(\\ell_1\\) penalty did not generalize better to the testing data."
  },
  {
    "objectID": "posts/ml-final-project/index.html#linear-regression-with-ell_2-regularization",
    "href": "posts/ml-final-project/index.html#linear-regression-with-ell_2-regularization",
    "title": "Machine Learning Final Project",
    "section": "Linear Regression with \\(\\ell_2\\) Regularization",
    "text": "Linear Regression with \\(\\ell_2\\) Regularization\n\nSci-kit Learn\n\nTrain Model\nFirst, we fit the model with scikit-learn Ridge and inspect the resulting model. As before, we do this to assess the validity of our own implementation.\n\n# Fit model\nLR_s_l2 = Ridge(alpha = .1)\n\nm = LR_s_l2.fit(X_train, y_train)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_train, LR_s_l2.predict(X_train)),\n      \"\\nR^2:\", m.score(X_train, y_train),\n      \"\\nY-intercept:\", m.intercept_,\n      \"\\nCoefficients:\\n\", m.coef_)\n\nMSE: 235049.69085940512 \nR^2: 0.7647294150800621 \nY-intercept: 69.8570955883946 \nCoefficients:\n [ 4146.87047622 -2058.81157194     6.57006522 -1039.66258053\n   107.03266863  -815.93549227  -127.78253829  -231.19197573\n -6438.07336424  -692.08973348]\n\n\n\n# Columns\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nThe training MSE is larger and the training \\(R^2\\) is smaller than scikit-learn’s linear regression with no regularizer. We anticipated this would be true in comparison to the no regularizer model as the penalty term helps prevent overfitting. It appears that with our chosen parameters, lasso regression performed better than ridge regression in terms of both MSE and \\(R^2\\), but this will change depending on the selected value for parameters.\nThe y-intercept and all coefficients except for landcover 2 are smaller than they were under linear regression without regularization, indicating that the regularization method has been successful in decreasing the magnitude of our coefficients. None of the coefficients are equal to exactly zero, but that is to be expected when working with the \\(\\ell_2\\) penalty.\nThe sign of every coefficient in this model is the same as in the original linear regression model except for landcover 11 (mixed forest), which is now positive and was also positive under lasso regression. However, the magnitude of this coefficient is really small; at 6.57, a location’s population density only changes by 6.57 people per square kilometer as the proportion of pixels represented by mixed forest increases from 0 to 1.\n\n\nTest Model\n\n# Create predictions (on test data)\npreds = LR_s_l2.predict(X_test)\n\n# Report results\nprint(\"MSE:\", mean_squared_error(y_test, preds),\n      \"\\nR^2:\", r2_score(y_test, preds))\n\nMSE: 387635.2059385676 \nR^2: 0.6978835936921313\n\n\nOn the testing data, our MSE of \\(387,635\\) is similar to the result of \\(390,640\\) with the \\(\\ell_1\\) regularizer but larger than the MSE of \\(373,800\\) with no regularizer, indicating that the \\(\\ell_2\\) regularizer also did not help our model generalize to the testing data better than unregularized linear regression. The \\(R^2\\) value was also larger in linear regression, meaning that the model without regularization explained more variation in the outcome variable.\n\n\n\nOur Implementation\n\nTrain Model\nLet’s fit linear regression with the \\(\\ell_1\\) norm with our own implementation and verify that our results are reasonably similar to those of scikit-learn.\n\n# fit linear regression model\nLR_l2 = LinearRegress(penalty = \"l2\", lam = .1/X_train_torch.shape[0]) \nopt_l2 = GradientDescentOptimizer(LR_l2)\n\n# initialize vector to record loss values\nloss_vec_l2 = []\n\n# fit model\nfor i in range(1000000): \n    # update model\n    opt_l2.step(X_train_torch, y_train_torch, alpha = 0.00001)\n\n    # calculate and record loss\n    loss = LR_l2.loss(X_train_torch, y_train_torch) \n    loss_vec_l2.append(loss)\n\n# plot the changes in loss \nplt.plot(loss_vec_l2, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\n\nm.coef_\n\narray([ 4146.87047622, -2058.81157194,     6.57006522, -1039.66258053,\n         107.03266863,  -815.93549227,  -127.78253829,  -231.19197573,\n       -6438.07336424,  -692.08973348])\n\n\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_train_torch, y_train_torch).item(),\n      \"\\nR^2:\", LR_l2.r2(X_train_torch, y_train_torch),\n      \"\\nY-intercept:\", LR_l2.w[-1],\n      \"\\nCoefficients:\\n\", LR_l2.w[:-1],\n      \"\\nDifferences in sign:\", (torch.tensor(m.coef_)*LR_l2.w[:-1]&lt; 0).sum().item(),\n      \"\\nMaximum coefficient difference:\", torch.abs((torch.tensor(m.coef_)-LR_l2.w[:-1])).max().item())\n\nMSE: 246031.0025521462 \nR^2: tensor(0.7537, dtype=torch.float64) \nY-intercept: tensor(-258.6768, dtype=torch.float64) \nCoefficients:\n tensor([ 4417.6162, -1821.4942,   373.5831,  -372.9745,  -251.1601,  -436.8005,\n          -43.2542,  -109.9865, -2181.9550,  -541.2076], dtype=torch.float64) \nDifferences in sign: 1 \nMaximum coefficient difference: 4256.118320893194\n\n\n\nX.columns\n\nIndex(['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'], dtype='object')\n\n\nFirst of all, our implementation does not generate identical output to scikit-learn’s implementation. In order to make our implementation converge to a solution, we needed to make \\(\\lambda\\) far smaller than in their implementation. This may have occurred because we are using the optimization technique of gradient descent, but scikit-learn has implemented a number of more complex techniques and automatically detects which one to use depending on the dataset it receives as input. It is also possible that they implemented their loss function as the sum of squared error rather than the mean squared error. If this is the case, then dividing our \\(\\lambda\\) by the number of observations should theoretically produce identical results. In the code above, we opt for this implementation; however, it should be noted that we have not confirmed whether scikit-learn actually uses the sum of squares in their loss function. Even with this modification, our model has converged to a different solution than theirs, for reasons we have not uncovered.\nAlthough our results are different, they are not drastically different. Our MSE is \\(246,030\\) rather than \\(235,050\\) and our \\(R^2\\) is \\(0.7537\\) rather than \\(0.7647\\), differences that are not ideal but also not terrible. All coefficients have the same sign as their solution except for 8 (grassland/herbaceous). In all prior models, the coefficient of landcover 8 has been positive or zero, but in this model, it is negative! This confuses the interpretation of landcover 8, but with only one discrepancy it does not necessarily ring alarm bells. Perhaps if we had the time to confirm scikit-learn’s loss function and implement the same optimization method we would achive more similar results.\n\n\nTest Model\n\n# Report results\nprint(\"MSE:\", LR_l2.mse(X_test_torch, y_test_torch),\n      \"\\nR^2:\", LR_l2.r2(X_test_torch, y_test_torch))\n\nMSE: tensor(399693.2117, dtype=torch.float64) \nR^2: tensor(0.6885, dtype=torch.float64)\n\n\nAt \\(399,692\\), our implementation’s MSE is more than scikit-learn’s \\(387,635\\) as well as all prior results. And at \\(0.6885\\), our implementation’s \\(R^2\\) is less than scikit-learn’s \\(0.6979\\) and all other results. We could achieve better results by modifying our parameter values, but we were unable to identically reproduce the output of scikit-learn. Overall, our results indicate that for this problem, regularization does not lead to improved performance on the testing data, although it may facilitate interpretation of coefficients."
  },
  {
    "objectID": "posts/ml-final-project/index.html#discussion-of-linear-regression",
    "href": "posts/ml-final-project/index.html#discussion-of-linear-regression",
    "title": "Machine Learning Final Project",
    "section": "Discussion of Linear Regression",
    "text": "Discussion of Linear Regression\nIn linear regression, a major assumption is that all observations are independent of each other. However, when working with spatial data, nearby observations are often similar, such that observations are not independent if they are in close proximity to each other. In order to determine whether our model suffers from such spatial dependence, we will fit a linear regression model on the entire dataset and produce a map of our model’s residuals. We opt for linear regression without regularization due to its higher performance in the work above.\n\n# convert to torch tensors\n# add column of ones for y-intercept\nX_torch = torch.cat((torch.tensor(X.values), torch.ones((X.shape[0], 1))), 1)\ny_torch = torch.tensor(y.values)\n\n# fit linear regression model\nLR_full = LinearRegress()\nopt_full = GradientDescentOptimizer(LR_full)\n\n# fit model\nfor i in range(500000): \n    # update model\n    opt_full.step(X_torch, y_torch, alpha = 0.01)\n\n    # calculate and record loss\n    loss = LR_full.loss(X_torch, y_torch) \n\n\n# calculate residuals\nresid = (y_torch - LR_full.pred(X_torch))\n\n# add residual column to tracts\ntracts[\"resid\"] = resid\n\n# specify that color ramp should be centered at 0\ndivnorm = TwoSlopeNorm(vmin=-3000, vcenter=0., vmax = 3000)\n\n# create map\nresid_map = tracts.plot(\"resid\", legend = True, cmap = \"seismic\", norm = divnorm, figsize = (8,8))\nplt.title(\"Residual Map\");\n\n\n\n\n\n\n\n\nIn an ideal scenario with spatially independent observations, the values of residuals would be distributed randomly throughout the map. However, with clear clusters of red and blue, our model visually appears to be making similar errors in nearby places. In other words, our residuals suffer from spatial autocorrelation. This may occur because the population density in one tract influences the population density in another tract; similarly, the landcover in one tract may influence the population density in a neighboring tract. Fortunately, there exists an entire field of spatial statistics dedicated to addressing issues of spatial autocorrelation. In the following section, we will employ one technique, known as spatial lag regression, in order to account for spatial dependence and hopefully improve our results. Before continuing to our section on spatial autoregression, we first perform cross-validation on linear regression and report the average root mean squared error (RMSE) in order to compare our results to our autoregressive results. We will opt for scikit-learn’s linear regression class since it is faster and achieves identical results to ours.\n\n# define model\nLR = LinearRegression() \n\n# define scoring function\n# this is just required in order to use scikit-learn's cross_val_score function\n# basically they multiply the MSE by -1, so we need to account for that afterwards\nmse_score = make_scorer(mean_squared_error, greater_is_better = False)\n\n# cross validation\ncv_scores_LR = cross_val_score(estimator = LR, X = X, y = y, scoring = mse_score, cv = 4)\n\n# compute average RMSE\nnp.sqrt(-1*cv_scores_LR).mean()\n\n503.0545511056982\n\n\nWith regular linear regression, we have achieved an average cross-validation RMSE of \\(503\\) people per square kilometer. Let’s see if accounting for space can improve our results!"
  },
  {
    "objectID": "posts/ml-final-project/index.html#data-processing-and-exploration",
    "href": "posts/ml-final-project/index.html#data-processing-and-exploration",
    "title": "Machine Learning Final Project",
    "section": "Data Processing and Exploration",
    "text": "Data Processing and Exploration\nIn this spatial autoregression model, we adopt queen criterion to construct spatial continuity weight matrix. The queen criterion defines neighbors as spatial units sharing a common edge or a common vertex. This means that in our model, we will add the features and characteristics of the neighboring tracts as part of the prediction variables.\nTo find the weight matrix, we need to introduce geometry to our dataset. Here, I am merging the csv file to a shapefile and convert the merged data to a GeoDataFrame format. Later, I calculate the queen spatial continuity matrix using the libpysal pacakge. Using the spatial weight continuity matrix, we can then calculate the spatial lag data of population density, which is the mean population density of the neighboring tracts.\n\n# import shapefile\n# need separate shapefile because the one form pygris didn't cooperate with the weights matrix functions\ndata = pd.read_csv(\"../data/combined_data.csv\")\ngdf = gpd.read_file('../data/tl_2016_09_tract.shp')\n\n# create merge columns\ngdf['TRACTCE'] = gdf['TRACTCE'].astype(int)\ndata['TRACTCE'] = data['TRACTCE'].astype(int)\n\n# merge csv with shapfile using TRACTCE\nmerged_gdf = gdf.merge(data, on='TRACTCE', how='left')\n\n# make merged_gdf into geo dataframe\nmerged_gdf = gpd.GeoDataFrame(merged_gdf)\n\n# drop out all rows that have no population density\nmerged_gdf = merged_gdf.dropna(subset=['PopDensity'], axis=0)\n\n# clean tracts that have truncated data on population density\nmerged_gdf = merged_gdf[merged_gdf['PopDensity'] != 0]\nmerged_gdf = merged_gdf[merged_gdf['TRACTCE'] != 194202]\n\n# define the geometry_x column to be the geometry feature \nmerged_gdf.set_geometry(\"geometry_x\", inplace=True)\n\n# calculate Queen's neighbor weights for each tracts\nw = lp.weights.Queen.from_dataframe(merged_gdf)\nw.transform = 'R'\n\n# compute spatial lag of population density\nmerged_gdf['spatial_lag_PopDens'] = lp.weights.lag_spatial(w, merged_gdf['PopDensity'])\n\n# calculate the mean pop density of each tract's neighbors\n#merged_gdf['avg_neighbor_density'] = merged_gdf.groupby('TRACTCE')['spatial_lag'].transform('mean')\nmerged_gdf['PopDensity'] = merged_gdf['PopDensity'].astype(float)\n\n# download merged_gdf to csv file\nmerged_gdf.to_csv('../data/merged_gdf.csv', index=False)\n\n/tmp/ipykernel_18572/2255761594.py:27: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n  w = lp.weights.Queen.from_dataframe(merged_gdf)\n\n\nNext, we want to perform a spatial autocorrelation evaluation using Global Moran’s I index. This evaluation assesses the spatial distribution characteristic of the entire region. We plot the scatter plot between the population denisty and mean population denisty of tract’s neighbors. The Global Moran’s I index, if we do not delve into its mathematical details, is the slope of the best fit line between these two numbers. In our case, we calculated the Moran’s I index to be 0.6. Together with the distribution of the scatter plot, we believe that population density of the neighboring tracts are dependent. We also want to inspect the spatial association at a local scale. The color of each tract is based on its own population density and the population density of its surrounding tracts.\nMoran’s Scatterplot has four categories: High-High, High-Low, Low-High, Low-Low. High/low before the dash means whether the tract has a populuation density that is higher/lower than the mean overall population density. High/low after the dash means whether the tract’s neighbors population denisty is above/below the average population density. After categorization, we map the tracts to inspect the distribution of the tracts’ categories. We find that High-High tracts are usually in urban areas, Low-High tracts are usually suburbs, High-Low tracts are typically towns in the rural area, and Low-Low are rural tracts. Therefore, we believe that by taking into account the characteristics of the target tract’s neighboring tract, we are able to predict population density better than ordinary least square regression.\n\n# read data\nmerged_csv_moran = pd.read_csv(\"../data/merged_gdf.csv\", usecols=['PopDensity', 'spatial_lag_PopDens', \"Geo_NAME\"]).dropna()\n\n# Extract x and y columns from the DataFrame\nx = merged_csv_moran['PopDensity'].values.reshape(-1, 1)  # Reshape to make it a 2D array for scikit-learn\ny = merged_csv_moran['spatial_lag_PopDens'].values\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_csv_moran['category'] = 0  # Initialize category column\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &lt; p) & (merged_csv_moran['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_csv_moran['spatial_lag_PopDens'].mean()\nq = merged_csv_moran['PopDensity'].mean()\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Create a scatter plot of x vs y\nscatter = plt.scatter(x, y, color=merged_csv_moran['category'].map(colors))\n\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n# Get the slope and intercept of the fitted line\nslope = model.coef_[0]\nintercept = model.intercept_\n\n# Plot the fitted line\nplt.plot(x, model.predict(x), color='red', label=f'Linear Regression (y = {slope:.2f}x + {intercept:.2f})')\n\n# Add labels and title\nplt.xlabel('Population Density')\nplt.ylabel('Spatial Lag Density')\nplt.title(\"Moran's I = 0.60\")\n\n# Create legend entries manually\nlegend_patches = [\n    Patch(color=color, label=label) for label, color in colors.items()\n]\n\n# Add the legend with custom entries and regression equation\nplt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n# Draw horizontal and vertical dashed line at y = p\nplt.axhline(y=p, color='gray', linestyle='--')\nplt.axvline(x=q, color='gray', linestyle='--')\n\n# Show plot\nplt.show()\n\n/tmp/ipykernel_18572/4162410205.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_csv_moran.loc[(merged_csv_moran['spatial_lag_PopDens'] &gt;= p) & (merged_csv_moran['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/4162410205.py:51: MatplotlibDeprecationWarning: An artist whose label starts with an underscore was passed to legend(); such artists will no longer be ignored in the future.  To suppress this warning, explicitly filter out such artists, e.g. with `[art for art in artists if not art.get_label().startswith('_')]`.\n  plt.legend(handles=legend_patches + [scatter, plt.Line2D([0], [0], color='red', label=f'(y = {slope:.2f}x + {intercept:.2f})')])\n\n\n\n\n\n\n\n\n\n\n# Calculate the average for 'spatial_lag_PopDens' and 'PopDensity'\np = merged_gdf['spatial_lag_PopDens'].mean()\nq = merged_gdf['PopDensity'].mean()\n\n# Categorize the rows based on conditions\nmerged_gdf['category'] = 0  # Initialize category column\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-High'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-Low'\nmerged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &lt; p) & (merged_gdf['PopDensity'] &lt; q), 'category'] = 'Low-Low'\n\n# Define custom colors for categories\ncolors = {'High-High': '#F47E3E', 'Low-Low': '#0FA3B1', 'Low-High': '#D9E5D6', 'High-Low': '#DCC156'}\n\n# Plot the map using custom colors\nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\nplt.title('Map of Moran Scatterplot Quadrants')\nplt.show()\n\n/tmp/ipykernel_18572/3545356962.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'High-High' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  merged_gdf.loc[(merged_gdf['spatial_lag_PopDens'] &gt;= p) & (merged_gdf['PopDensity'] &gt;= q), 'category'] = 'High-High'\n/tmp/ipykernel_18572/3545356962.py:17: UserWarning: Only specify one of 'column' or 'color'. Using 'color'.\n  merged_gdf.plot(column='category', ax=ax, color=merged_gdf['category'].map(colors), legend=True)\n\n\n\n\n\n\n\n\n\nInstead of using all possible land cover types, we are going to use land cover types that are more common among all tracts in CT for density prediction. The land cover types we selected are the same as the ones in linear regression section.\n\n# All landcover types\nall_landcover = ['2', '5', '8', '11', '12', '13', '14', '15', '17', '18', '19', '20', '21', '22', '7', '6', '0', '23']\nall_landcover_pct = ['2pct', '5pct', '8pct', '11pct', '12pct', '13pct', '14pct', '15pct', '17pct', '18pct', '19pct', '20pct', '21pct', '22pct', '7pct', '6pct', '0pct', '23pct']\n\n# Select landcover types\nlandcover_types = ['2', '5', '11', '12', '8', '13', '14', '15', '20', '21'] #, '22', '7', '8', '13', '14', '15', '20', '21'\nlandcover_pct = ['2pct', '5pct', '11pct', '12pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'] # , '22pct', '7pct', '8pct', '13pct', '14pct', '15pct', '20pct', '21pct'\n\n# Merge them into our data\nmerged_gdf['sum'] = merged_gdf[all_landcover].sum(axis=1)\nmerged_gdf[all_landcover_pct] = merged_gdf[all_landcover].div(merged_gdf['sum'], axis=0).multiply(100).astype(float)\n\n# Download merged_gdf to csv file optionally \n#merged_gdf.to_csv('merged_gdf_saved.csv', index=False)"
  },
  {
    "objectID": "posts/ml-final-project/index.html#spatial-lag-regression",
    "href": "posts/ml-final-project/index.html#spatial-lag-regression",
    "title": "Machine Learning Final Project",
    "section": "Spatial Lag Regression",
    "text": "Spatial Lag Regression\n\nEndogenous vs. Exogenous: What’s the Difference?\nThere are two types of spatially lagged regression models. The first one is spatially lagged endogenous regression model. The endogenous model includes the spatial lagged value of the target variable as one of the explanatory variables for regression. In our case, the population density of a tract’s neighbor is part of the variables we use to predict the population density of the tract.\nThe second type of spatially lagged regression model is spatially lagged exogenous regression model. Instead of taking into account the population density, our target variable, of the neighboring tracts, the exogenous model considers the explanatory variables of the tract’s surroundings. In our case, the spatially lagged exogenous model adds neighbors’ land type information to the model. We will calculate the spatial lagged value of each land cover type for all tracts and include them as part of the predictor variables.\nWe first fit both models to the entirety of CT and map their residuals on each tract. First, we fit the endogenous model.\n\n# Endogenous model: consider spatial lag population denisty\npredictor = landcover_pct + ['spatial_lag_PopDens']\n\n# Get explanatory variables and target variable\nX_merged_gdf = merged_gdf[predictor].values\ny_merged_gdf = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n# Create, fit, and predict with Linear Regression\nmodel = LinearRegression()\nmodel.fit(X_merged_gdf, y_merged_gdf)\ny_pred = model.predict(X_merged_gdf)\n\n# Calculate residuals \nresiduals = y_merged_gdf - y_pred\nmerged_gdf['residuals'] = residuals\n\n# Remove Spatial lag so that our Exogenous model does not take this into account\nmerged_gdf.drop(columns=['spatial_lag_PopDens'], inplace=True)\n\nNext, we fit the exogenous model.\n\n# Exogenous model: consider\nexo_predictor = landcover_pct + ['lag_2pct', 'lag_5pct', 'lag_11pct', 'lag_12pct', 'lag_8pct', 'lag_13pct', 'lag_14pct', 'lag_15pct', 'lag_20pct', 'lag_21pct'] \n\nfor i in range(len(landcover_pct)):\n        merged_gdf['lag_' + landcover_pct[i]] = lp.weights.lag_spatial(w, merged_gdf[landcover_pct[i]])\n\n# Get explanatory variables and target variable\nX_merged_gdf_exo = merged_gdf[exo_predictor].values\ny_merged_gdf_exo = merged_gdf['PopDensity'].values.reshape(-1, 1)\n\n#Create, fit, and predict with Linear Regression\nmodel_exo = LinearRegression()\nmodel_exo.fit(X_merged_gdf_exo, y_merged_gdf_exo)\ny_pred_exo = model_exo.predict(X_merged_gdf_exo)\n\n#Calculate Residuals and make new column\nresiduals_exo = y_merged_gdf_exo - y_pred_exo\nmerged_gdf['residuals_exo'] = residuals_exo\n\nNow, we visualize the map of residuals for both models.\n\n# Define the colors for the custom colormap\ncolors = [(0, 'brown'), (0.5, 'white'), (1, 'green')]  # Position 0 is brown, position 0.5 is white, position 1 is green\n\n# Create the colormap\ncmap = LinearSegmentedColormap.from_list('custom_cmap', colors)\n\n# Determine the range of residuals to be used for normalization\nresiduals_max = max(abs(merged_gdf['residuals_exo'].max()), abs(merged_gdf['residuals'].max()))\nvmax = residuals_max * 0.75  # Adjust the factor as needed\n\n# Create a normalization object\nnorm = Normalize(vmin=-vmax, vmax=vmax)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 1 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 2 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnum_bins = 50\nhist_range = (0, 2000)\n\n# Create subplots with two columns\nfig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot the first histogram\naxs[0].hist(merged_gdf['residuals_exo'], bins=num_bins, range=hist_range, color='green')\naxs[0].set_xlabel('Absolute Residual')\naxs[0].set_ylabel('Number of Rows')\naxs[0].set_title('Distribution of Absolute Residuals (exogenous)')\n\n# Plot the second histogram\naxs[1].hist(merged_gdf['residuals'], bins=num_bins, range=hist_range, color='green')\naxs[1].set_xlabel('Absolute Residual')\naxs[1].set_ylabel('Number of Rows')\naxs[1].set_title('Distribution of Absolute Residuals (endogenous)')\n\n# Adjust layout\nplt.tight_layout()\n\n# Show plots\nplt.show()\n\n\n\n\n\n\n\n\nThe exogenous spatial lag residual map is on the left and the endogenous spatial lag residual map is on the right. Qualitatively assessing the these two maps, we see both models tend to underestimate the population density in urban areas. It is reasonable as land cover data is only two dimensional and does not account for the vertical height of the buildings. We also see slightly differences in prediction of rural areas between Exogenous and Endogenous. Endogenous is more accurate in rural areas as the landcover is not a sole factor of prediction, and it instead takes into account the population density of the tracts around it. Exogenous is slightly more inaccurate in these regions for the lack of this parameter. Both models tend to have a better performance at predicting density in less populated areas (Low-Low tracts).\nContinuing to explore, we created two residual histograms. We noted that they are similar to our map of CT and do not present any new pattern.\nTo explore a bit deeper into our dataset, we will look at a snapshot of our map around Hartford and its neighboring cities.\n\n# Create a normalization object\nnorm = Normalize(vmin=-2000, vmax=2000)\n\n# First graph\nfig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with 1 row and 2 columns\n\n# Graph 2 - Exogenous variables\nmerged_gdf.plot(column='residuals_exo', cmap=cmap, legend=True, ax=axes[0], vmax=vmax, norm=norm)\naxes[0].set_title('Spatial Distribution of Residuals Near Hartford (Exogenous)')\naxes[0].set_xlabel('Longitude')\naxes[0].set_ylabel('Latitude')\n\n# Graph 1 - Spatial lag of PopDensity\nmerged_gdf.plot(column='residuals', cmap=cmap, legend=True, ax=axes[1], vmax=vmax, norm=norm)\naxes[1].set_title('Spatial Distribution of Residuals Near Hartford (Endogenous)')\naxes[1].set_xlabel('Longitude')\naxes[1].set_ylabel('Latitude')\n\naxes[0].set_ylim([41.6, 41.8])\naxes[0].set_xlim([-72.9, -72.5])\naxes[1].set_ylim([41.6, 41.8])\naxes[1].set_xlim([-72.9, -72.5])\n\nplt.show()\n\n\n\n\n\n\n\n\nOne area we were particularly curious about was Hartford as it is a large urban hub in the central of Connecticut. We noticed that we were grossly underestimating densely populated areas, which makes sense as they are relatively large outliers from the rest of the relatively lower population and spread out suburban areas of Connecticut. However, we were better at calculating more densely populated areas with the Endogenous model. We hypothesize this is due to the fact that Endogenous Models inherently take into account the population densities of neighboring tracts. Thus, there is a greater likelihood that the model will “self-correct” by knowing the population of its neighbors."
  },
  {
    "objectID": "posts/ml-final-project/index.html#training-and-testing-cross-validation",
    "href": "posts/ml-final-project/index.html#training-and-testing-cross-validation",
    "title": "Machine Learning Final Project",
    "section": "Training and Testing Cross Validation",
    "text": "Training and Testing Cross Validation\nFor training and testing, we need to separate the data into two. Due to the spatial dependence of tracts, we cannot randomly select tracts from the dataset and assign them to either training or testing data because neighboring tracts will not be in the same dataset. Therefore, to minimize the rupture of spatial relations, we decide to separate training and testing data by neighboring counties to ensure that all tracts in training and testiing data are countinuous. Later, we perform for loops on each set of training and testing data and calculate their mean RMSE for each training and testing set for both endogenous and exogenous model.\n\nmerged_csv = pd.read_csv(\"../data/merged_gdf.csv\")\n\n# Extract the county name from the the Geo_NAME column. \nmerged_gdf['County'] = merged_gdf['Geo_NAME'].str.split(',').str[1].str.strip().str.replace(' ', '')\nmerged_gdf = merged_gdf.dropna(subset=['County'])\n\n\n# Spatially lagged endogenous regressor\nodd_counties = ['NewLondonCounty', 'NewHavenCounty', 'LitchfieldCounty', 'TollandCounty']\neven_counties = ['MiddlesexCounty', 'FairfieldCounty','HartfordCounty', 'WindhamCounty']\n\nrmse = []\n\nfor i in range(4):\n    # Splitting training and testing counties\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    # Queen weight matrix for each train and test\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n    \n    # Regularize the weights\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n    \n    # Calculate the spatial lag pop density\n    train_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(train_1_w, train_1['PopDensity'])\n    test_1['spatial_lag_PopDens'] = lp.weights.lag_spatial(test_1_w, test_1['PopDensity'])\n    \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse.append(test_rmse)\n\n\nnp.mean(rmse)\n\n382.27553702535505\n\n\nThe average root mean square error of the spatially lagged endogenous regression model is 382.28. The endogenous model is more advantageous when we have a relatively higher coverage of census data and we need to predict the population density of small region surrounded by regions with good census.\nNext, we do training and testing cross validation for the exogenous spatial lagged model.\n\n# Spatially lagged exogenous regressors\n\nrmse_exo = []\n\n# Set loops for each set of different counties\nfor i in range(4):\n\n    train_1 = merged_gdf[(merged_gdf['County'] != odd_counties[i]) & (merged_gdf['County'] != even_counties[i])]\n    test_1 = merged_gdf[(merged_gdf['County'] == odd_counties[i]) | (merged_gdf['County'] == even_counties[i])]\n\n    train_1_w = lp.weights.Queen.from_dataframe(train_1)\n    test_1_w = lp.weights.Queen.from_dataframe(test_1)\n\n    train_1_w.transform = 'R'\n    test_1_w.transform = 'R'\n\n    # Calculate spatial lag \n    for j in range(len(landcover_pct)):\n        train_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(train_1_w, train_1[landcover_pct[j]])\n        test_1['lag_' + landcover_pct[j]] = lp.weights.lag_spatial(test_1_w, test_1[landcover_pct[j]])\n    \n    # Extract training and test data \n    y_train = np.array(train_1['PopDensity']).reshape((-1,1))\n    x_train = np.array(train_1[exo_predictor])\n\n    y_test = np.array(test_1['PopDensity'])\n    x_test = np.array(test_1[exo_predictor])\n\n    # Fit linear regression model using scikit-learn \n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    # Predict on test data\n    y_pred_test = model.predict(x_test)\n\n    # Calculate RMSE\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n\n    rmse_exo.append(test_rmse)\n\n\nnp.mean(rmse_exo)\n\n391.66561692553\n\n\nThe average RMSE of the spatially lagged endogenous regression model cross validation is 391.67, which is slightly larger than the RMSE of the endogenous model. The exogenous model is more applicable to scenarios when we have good satellite data but sparse census data.\nComparing our Spatial Autoregression to our Linear regression, it is clear that our Spatial Regression yields better results."
  },
  {
    "objectID": "posts/ml-final-project/index.html#concluding-discussion",
    "href": "posts/ml-final-project/index.html#concluding-discussion",
    "title": "Machine Learning Final Project",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nThrough this project, we were able to implement three different forms of Linear Regression, as well as create Spatial Autoregression models, and we determined the efficacy of each of these models both mathematically and graphically. Our results were relatively similar to Tian et al. (2005) in that they underpredicted the population density in densely populated urban areas more frequently than other plots of land, and over-predicted population density in rural areas. Overall, we accomplished a few key things with project. Through our models, we were able to predict population density with only landcover with relatively strong accuracy. We successfully compared different machine learning models and concluded that Spatial Autoregression was more accurate than Linear Regression. With more time, we would have liked to implement Poisson Regression and performed analysis at the block group level instead of tract level. With more computational power, we would have liked to calculate a larger dataset, representing a larger spatial region. Overall, we are proud of our work!"
  },
  {
    "objectID": "posts/ml-final-project/index.html#group-contributions-statement",
    "href": "posts/ml-final-project/index.html#group-contributions-statement",
    "title": "Machine Learning Final Project",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nLiam helped with data acquisition and preparation, wrote our implementation of linear regression with gradient descent in linear_regression.py, and compared the output of our class with that of scikit-learn. Alex acquired landcover data from Conus, and shapefile data of Connecticut. He then implemented zonal statistics with Manny. Alex explained the differences in Spatial Autoregression methods, trained the models, and utilized cross validation. Manny created visualizations of our models to represent graphically the residuals of each model. He proof-read all of our code, making corrections, rewriting descriptions, and ensuring cohesive and consistent writing styles. He also contributed to code in the Spatial Auto Regression section."
  },
  {
    "objectID": "posts/ml-final-project/index.html#personal-reflection",
    "href": "posts/ml-final-project/index.html#personal-reflection",
    "title": "Machine Learning Final Project",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nAs my contributions to this project were primarily related to linear regression, I learned a lot about the implementation of linear regression with gradient descent. I learned to implement lasso regression with the \\(\\ell_1\\) norm and ridge regression with the \\(\\ell_2\\) norm and gained an understanding of the differences between the two penalties. I also learned that there are a lot of fancier optimization methods implemented by scikit-learn that run faster and in some cases like coordinate descent actually yield different output. In collaboration with my project partners, I also learned about spatial autoregression and its advantages when modeling spatial phenomena. I feel proud of our work, as we successfully implemented linear regression from scratch and conducted a spatial regression extension that we had not originally planned. I think it is interesting how accounting for landcover at a location’s neighbors can yield more accurate predictions than considering the landcover at that location alone. As I hope to pursue a career in spatial data science, gaining this deeper understanding of linear regression and exposure to spatial autoregression will be helpful as I move into the professional world."
  },
  {
    "objectID": "posts/dental-gwr/index.html",
    "href": "posts/dental-gwr/index.html",
    "title": "Replication Study with Spatial Regression",
    "section": "",
    "text": "In this analysis, I replicate Broomhead et al.’s study, “National Patterns in Paediatric Hospital Admissions for Dental Extractions in England”. Their original study applied Geographically Weighted Regression using the spgwr package in R to analyze spatial patterns and predictors of dental extractions in children in the United Kingdom. In this replication, I use different, more easily accessible, explanatory variables to predict the same response variable. I verify that Geographically Weighted Regression is a reasonable model for this data set, make a few improvements to their methodology, and find that my model performs similarly to theirs.\nTo see my complete research compendium for this project, click here"
  },
  {
    "objectID": "posts/dental-gwr/index.html#abstract",
    "href": "posts/dental-gwr/index.html#abstract",
    "title": "Replication Study with Spatial Regression",
    "section": "",
    "text": "In this analysis, I replicate Broomhead et al.’s study, “National Patterns in Paediatric Hospital Admissions for Dental Extractions in England”. Their original study applied Geographically Weighted Regression using the spgwr package in R to analyze spatial patterns and predictors of dental extractions in children in the United Kingdom. In this replication, I use different, more easily accessible, explanatory variables to predict the same response variable. I verify that Geographically Weighted Regression is a reasonable model for this data set, make a few improvements to their methodology, and find that my model performs similarly to theirs.\nTo see my complete research compendium for this project, click here"
  },
  {
    "objectID": "posts/dental-gwr/index.html#study-design",
    "href": "posts/dental-gwr/index.html#study-design",
    "title": "Replication Study with Spatial Regression",
    "section": "Study design",
    "text": "Study design\nThe following analysis is an exploratory study analyzing social predictors and spatial variation of pediatric dental extractions in the United Kingdom. The spatial extent of the study is the country of England, and the spatial scale is the local authority level, both of which I selected in order to match the original study. The spatial scale seems appropriate because it is the finest geographic unit for which dental extractions data are available. The analysis was conducted based on tooth extractions data from 2017-18 and census data from 2011."
  },
  {
    "objectID": "posts/dental-gwr/index.html#materials-and-procedure",
    "href": "posts/dental-gwr/index.html#materials-and-procedure",
    "title": "Replication Study with Spatial Regression",
    "section": "Materials and procedure",
    "text": "Materials and procedure\n\nData and variables\n\nExtractions Data: Data on hospital-based dental extractions in the United Kingdom was provided by Public Health England, a government agency. This data indicates the number of times that children, grouped by local authority of the child’s residence, were admitted to a hospital for tooth extractions. For privacy reasons, all counts on the subnational level are rounded to the nearest 5, and any counts less than 6 are not provided to the public.\nCensus Data: In their original paper, Broomhead et al. use several dentistry-related independent variables in their analysis. Their data was not readily available online, so I used several social variables from the United Kingdom’s 2011 census instead. On the local authority level, I queried for all of the available variables; the most relevant ones for this analysis include the number of children, single parent households, foreign born residents, individuals with severe disabilities, average cars per household, unemployed people, and total residents. This data is freely available from Social Explorer, a small business which provides online mapping and data services.\nLocal Authority Geometries: Cartographic boundaries for England’s local authorities are provided by the United Kingdom’s Office for National Statistics, which houses an open data portal on its website.\n\n\n\nGeographic and temporal characteristics\nDental extractions data is from 2017-18 and census data is from 2011. It is important to note that several groups of local authorities have merged to form larger local authorities between the 2011 census and today. The dental extractions dataset provides data in terms of the most updated local authorities, but the census data uses the old local authorities. Curiously, the data provider for the local authority geometries claims that the geometries are accurate as of May 2021, but close inspection of the downloaded data reveals that the boundaries actually match the 2011 boundaries.\nFor this reason, the spatial resolutions and unique identifiers for some local authorities differed between my datasets. I illustrate how I addressed this concern in the following section. The coordinate reference system of the analysis is OSGB 1936, and the spatial extent of the analysis is the extent of England."
  },
  {
    "objectID": "posts/dental-gwr/index.html#data-transformations",
    "href": "posts/dental-gwr/index.html#data-transformations",
    "title": "Replication Study with Spatial Regression",
    "section": "Data transformations",
    "text": "Data transformations\nIn this section, I provide a summary of the data transformations performed on my data. Note that complete documentation of my procedure is available in this R markdown file in my research compendium.\n\nCleaning the Extractions Data\nFirst, I read the extractions csv and extract data from the year of interest.\nextractions &lt;- read_csv(here(\"data\", \"raw\", \"public\", \"extractions.csv\"))\nextractions &lt;- extractions %&gt;%\n  filter(Year == 1718)\nRecall that figures less than 6 are suppressed due to privacy concerns, and other subnational figures are rounded to the nearest 5. I need to perform some imputations in order to consider every local authority, and it is unclear how Broomhead et al. addressed this concern. Since all other subnational figures are rounded to the nearest 5, it is impossible to calculate an accurate average of the missing data. I choose to simply impute a 0 where data is missing. Note that 37.7% of the data is missing, so I am discounting a large amount of data.\nextractions &lt;- extractions %&gt;%\n  mutate(Count = if_else(Count == '*', '0', Count))\nThe data was provided in a format where one observation represented a specific age groups in each local authority (i.e. there were several observations for each local authority). Local authorities are uniquely identified by ONS_code, so I group by the ONS_code and sum the extractions count in order to generate a dataset where one observation represents one local authority.\nextractions &lt;- extractions %&gt;%\n  group_by(ONS_code) %&gt;%\n  summarise(extractions = sum(Count))\nLater on, I will have to join this data set with my other two data sets. I have already mentioned that several local authorities merged to form larger ones, and I will reveal how I address that when I discuss how I cleaned my other two data sets. Additionally, some local authorities have received new uniquely idenitifying local authority codes in the last few years (Sources: https://l-hodge.github.io/ukgeog/articles/boundary-changes.html and https://www.bbc.com/news/uk-england-somerset-44289087). This data set actually uses the updated codes, but the other two datasets use the old codes, so I manually change the codes in this dataset to match the other ones.\nextractions &lt;- extractions %&gt;%\n  mutate(ONS_code = if_else(ONS_code == \"E06000057\", \"E06000048\", ONS_code),\n         ONS_code = if_else(ONS_code == \"E07000240\", \"E07000100\", ONS_code),\n         ONS_code = if_else(ONS_code == \"E07000241\", \"E07000104\", ONS_code),\n         ONS_code = if_else(ONS_code == \"E07000242\", \"E07000097\", ONS_code),\n         ONS_code = if_else(ONS_code == \"E07000243\", \"E07000101\", ONS_code),\n         ONS_code = if_else(ONS_code == \"E08000037\", \"E08000020\", ONS_code))  \n\n\nCleaning the Census Data\nFirst I import the census data as uk_census and rename relevant variables to have more useful names.\nuk_census &lt;- read_csv(here(\"data\", \"raw\", \"public\", \"uk_census.csv\"))\nuk_census &lt;- uk_census %&gt;%\n  rename(\"all_usual_residents\" = \"SE_T001_001\",\n         \"male\" = \"SE_T005_002\",\n         \"female\" = \"SE_T005_003\",\n         \"under5\" = \"SE_T006_002\",\n         \"5to9\" = \"SE_T006_003\",\n         \"10to14\" = \"SE_T006_004\",\n         \"15to17\" = \"SE_T006_005\",\n         \"single_parent_households\" = \"SE_T014_001\",\n         \"foreign_born\" = \"SE_T018_003\",\n         \"severe_disability\" = \"SE_T020_002\",\n         \"num_households\" = \"SE_T030_001\",\n         \"over_1.5_ppr\" = \"SE_T030_005\",\n         \"avg_cars_per_household\" = \"SE_T035_001\",\n         \"labor_force\" = \"SE_T044_001\",\n         \"unemployed\" = \"SE_T044_002\")\nThis data is pretty clean, with the exception that several local authorities have merged to form 6 new local authorities since the census was conducted in 2011. Given one dataset with more specific geographic information and another dataset with more general geographic information, it is best practice to aggregate to the larger geographic extent. Conveniently, the old local authorities nest neatly into the new ones. To accomplish this aggregation, I have to address each new local authority individually.\nI performed the same workflow one time for each new local authority, so I demonstrate the process just once below. First, I filter for the local authorities that merge to form the new local authority, and I sum their quantitative variables. I want to join the aggregated data back to my uk_census dataset using the row bind function, but I lack all string data including the updated local authority code. For this reason, I create a new table with all of the missing columns, and column bind it to my aggregated local authority data. Finally, I row bind the updated local authority to my uk_census dataset and repeat the process for the other 5 new local authorities.\nE06000058 &lt;- uk_census %&gt;%\n  filter(Geo_LA_CODE == \"E06000028\" | Geo_LA_CODE == \"E07000048\" | Geo_LA_CODE == \"E06000029\") %&gt;%\n  #  Bournemouth + Christchurch + Poole merge to become Bournemouth, Christchurch and Poole.  \n  summarize(across(all_usual_residents:unemployed, sum))\n\ntemp &lt;- tribble(\n  ~Geo_Name, ~Geo_QName, ~Geo_FIPS, ~Geo_LA_CODE,\n     \"doesnt matter\", \"doesnt matter\", \"doesnt matter\", \"E06000058\")\n\nE06000058_done &lt;- cbind(temp, E06000058)\n\nuk_census &lt;- rbind(uk_census, E06000058_done)\nUp to this point, all of the relevant variables were given as counts. I now normalize the variables I’m interested in and select just the variables I care about.\nuk_census &lt;- uk_census %&gt;%\n  mutate(pctChild = (under5 + `5to9` + `10to14` +`15to17`)/all_usual_residents,\n         single_parent_households_ph = single_parent_households/num_households,\n         foreign_rate = foreign_born/all_usual_residents,\n         severe_disability_rate = severe_disability/all_usual_residents,\n         crowded_rate = over_1.5_ppr/num_households,\n         unemployment_rate = unemployed/labor_force)\n\nuk_census &lt;- uk_census %&gt;%\n  select(pctChild, single_parent_households_ph, foreign_rate, severe_disability_rate, crowded_rate, unemployment_rate, Geo_Name, Geo_LA_CODE, all_usual_residents)\n\n\nCleaning the Geometry Data\nFirst, I load my geometry data as an sf object and select only the variables I care about (geometry is automatically kept).\nlocal_authorities &lt;- read_sf(here(\"data\", \"raw\", \"private\", \"local_authorities.gpkg\"))\n\nlocal_authorities &lt;- local_authorities %&gt;%\n  select(geo_code)\nThen I face the same issue I had with the census dataset, where I need to aggregate several local authorities in order to match the dental extractions data. Since this dataset contains just the geometry and a unique identifier for each local authority (no variables), I am concerned only with dissolving the geometries of the old local authorities into the newer, larger ones.\nAgain, I performed the same workflow for 6 new local authorities, but I include the code for just one of them below. First I define a new object composed of only the relevant local authorities, and I use the st_union function to dissolve their geometries into one. I transform the object into an sf object, define the geo_code to be the updated local authority code, and row bind this to the whole local_authorities dataset. Then I repeat the process for the other 5 new local authorities.\ngE06000058 &lt;- local_authorities %&gt;%\n  filter(geo_code == \"E06000028\" | geo_code == \"E07000048\" | geo_code == \"E06000029\")%&gt;%\n  st_union()\n\ngE06000058 &lt;- st_as_sf(gE06000058) %&gt;%\n  mutate(geo_code = \"E06000058\")%&gt;%\n  rename(\"geom\" = \"x\")\n\nlocal_authorities &lt;- rbind(local_authorities, gE06000058)\nThere were two other local authorities with different codes in this data set and I simply replace the local authority codes with the ones that the other two datasets use.\nlocal_authorities &lt;- local_authorities %&gt;%\n  mutate(geo_code = if_else(geo_code == \"E41000052\", \"E06000052\", geo_code),\n         geo_code = if_else(geo_code == \"E41000324\", \"E09000033\", geo_code))\n\n\nJoining the Data Sets, Final Touches\nI perform two joins in order to get the geometry, census data, and extractions data all in one dataset.\njoin1 &lt;- inner_join(local_authorities, extractions, by = c(\"geo_code\" = \"ONS_code\"))\n\ndentistry &lt;- inner_join(join1, uk_census, by = c(\"geo_code\" = \"Geo_LA_CODE\"))\nWith all of the data in the same place, I can now normalize the number of pediatric extractions with respect to population. I define my outcome variable as the number of pediatric extractions per 100,000 residents. This is an improvement from Broomhead et al.’s methodology, as they used the raw count of extractions as their outcome variable. Their method is poor practice because it exaggerates relationships in urban centers, where the number of dental extractions is higher due simply to the presence of more people, not their explanatory variables.\ndentistry &lt;- dentistry %&gt;%\n  mutate(extraction_rate = extractions/all_usual_residents*100000)"
  },
  {
    "objectID": "posts/dental-gwr/index.html#analysis",
    "href": "posts/dental-gwr/index.html#analysis",
    "title": "Replication Study with Spatial Regression",
    "section": "Analysis",
    "text": "Analysis\n\nLinear Regression and Assessing Model Fit\nMy end goal is to fit a Geographically Weighted Regression (GWR) model and compare my results to Broomhead et al.’s, but first I will fit an Ordinary Least Squares (OLS) model of my dataset. I do this for two reasons.\n\nI want to see how the linear regression and geographically weighted regression models differ in performance. If the GWR model outperforms the OLS model, that will support Broomhead et al.’s decision to use a GWR model in their study.\nA dataset must satisfy several conditions for it to be a suitable candidate for regression. I want to verify that the data is suitable for Geographically Weighted Regression, and I know that GWR is a particular tyhpe of linear regression. It is fairly straightforward to check the conditions for linear regression, so I verify that my dataset satisfies those conditions in this section. One should know, however, that it’s possible for these conditions to hold up in the context of the entire dataset and not be satisfied by subsets of the data generated in GWR (and vice versa). This represents an improvement from the original paper, as Broomhead et al. did not describe any work to confirm the validity of their model.\n\nThe first issue I check for is multicollinearity: where several explanatory variables are closely related. To assess the relationships between my variables, I created a corrplot that displays the Pearson correlation coefficients between pairs of quantitative variables.\n# select the relevant variables\ndentistry_nogeom &lt;- dentistry_nogeom %&gt;%\n  select(extraction_rate, pctChild, single_parent_households_ph, foreign_rate, severe_disability_rate, crowded_rate, unemployment_rate)\n\n# make correlation matrix\ncorr_mtx &lt;- cor(dentistry_nogeom, use = \"pairwise.complete.obs\")\n\n# create corrplot\ncorrplot&lt;- ggcorrplot(corr_mtx, # input the correlation matrix\n           hc.order = TRUE, #helps keep the negatives and positives together to make graph easier to interpret\n           type = \"upper\", #tells computer to plot the values above the diagonal (we could also do \"lower\")\n           lab = TRUE,\n           method = \"circle\")\n\nIt appears that most of my variables exhibit weak relationships with each other, with a few notable exceptions like crowded_rate & foreign_rate and unemployment_rate & single_parent_households_ph. I arbitrarily decide that if two of my explanatory variables exhibit a correlation coefficient of over 0.6 or less than -0.6, I will include just one of them in my regression model. This prevents issues of multicollinearity by eliminating closely related variables. Specifically, I remove crowded_rate and single_parent_households_ph. I’ll perform a more rigorous test for multicollinearity after fitting a linear model.\nI fit the linear regression model.\nm &lt;- lm(formula = extraction_rate ~ pctChild + severe_disability_rate + foreign_rate + unemployment_rate, data = dentistry_nogeom)\nSidenote: The R-Squared value for this Ordinary Least Squares regression model is 0.1916. This means that the fitted model only explains 19.16% of the variance of pediatric tooth extractions in the UK! I hope that applying a Geographically Weighted Regression model will improve the model’s performance.\nTo test the independent variables in this model more robustly for multicollinearity, I use a statistical test known as the Variance Inflation Factor (VIF). For context, when the VIF = 1, no multicollinearity is present. Generally, there is cause for concern when the VIF &gt;= 10. For further reading on Variance Inflation Factors, please see: https://online.stat.psu.edu/stat462/node/180/.\nvif(m)\nThe above function outputs the following VIF values.\npctChild: 1.600338\nsevere_disability_rate: 4.371897\nforeign_rate: 2.557536\nunemployment_rate: 4.258962\nSince all VIF’s are all less than 5, multicollinearity is not problematic in my model. Note that we are addressing correlations on a global level, and it is possible that different correlations exist on a local level. A more rigorous methodology to determine multicollinearity for Geographically Weighted Regression models is advised.\nThe second condition that I check for is linearity. Both OLS and GWR models assume that each explanatory variable exhibits a linear relationship with the response variable, so I create scatterplots that will reveal whether each explanatory variable in my model is linearly related with my response variable.\ndentistry_relevant &lt;- dentistry_nogeom %&gt;%\n  select(foreign_rate, pctChild, severe_disability_rate, unemployment_rate, extraction_rate)\n\ndentistry_nogeom_pivoted &lt;- pivot_longer(data = dentistry_relevant,\n               cols = -extraction_rate,\n               names_to = \"Type\",\n               values_to = \"Number\")\n\nlinearity_check &lt;- dentistry_nogeom_pivoted %&gt;%\n  ggplot(aes(x = Number, y = extraction_rate)) +\n  geom_point(size = 1) +\n  ylab(\"Extractions per 100,000 People\")+\n  ggtitle(\"Tooth Extractions vs Explanatory Variables\") +\n  facet_wrap(~ Type, scales = \"free\") +\n  xlab(\"Rates\")\n\nAll of the variables seem to exhibit some line-like linear relationship. It’s not perfect, but for the purposes of this assignment, we will proceed.\nThe third condition that I check for is homoscedasticity: constant variability along the least squares line. I check this condition by creating a residual plot.\nm_aug &lt;- augment(m)\n\nresidual_plot &lt;- ggplot(m_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", lty = \"dashed\") +\n  xlab(\"Fitted Values\")+\n  ylab(\"Residuals\")+\n  ggtitle(\"Residual Plot\")\n\nSince residuals exhibit relatively constant variability along the zero line, this condition is satisfied.\nThe final condition for linear regression is a normal distribution of residuals. I check this condition by creating a histogram below.\nresidual_histogram &lt;- ggplot(m_aug, aes(x = .resid)) +\n  geom_histogram()+\n  xlab(\"Residuals\")+\n  ylab(\"Count\")+\n  ggtitle(\"Residual Histogram\")\nresidual_histogram\n\nThe distribution is slightly skewed right, but it’s not terrible. Ideally, the residuals would exhibit a more normal distribution, but for the purposes of this assignment I move on to fitting a Geographically Weighted Regression model.\n\n\nWhat is Geographically Weighted Regression?\nLinear regression models relationships between variables, predicting outcomes of a response variable using multiple explanatory variables. It creates one model for the entire study area, under the assumption that the relationships between variables are the same everywhere in the study area.\nGeographically Weighted Regression is a different way of modeling, which accounts for spatial autocorrelation: that nearby observations are more closely related than faraway observations. This technique essentially performs linear regression for every observation, weighting nearby observations higher than faraway ones. Observations that are far enough away aren’t even considered in an observation’s regression model. By only considering nearby observations when calculating the model, Geographically Weighted Regression allows the relationships to vary across space. Geographically Weighted Regression might be a more suitable model for our data, because different social factors could interact with dental extractions differently in different locations and because cultural practices that play into these relationships may vary regionally.\nTo run a GWR model, one must decide on a spatial kernel: the size of the neighborhood around each point to consider when calculating the model. As the following image illustrates, for a given point X, the weight assigned to data points in the local regression decreases the futher you are from point X until it is 0 past the specified spatial kernel.\n\nIn some cases, a fixed spatial kernel is inappropriate, and it makes more sense to use an adaptive kernel. Rather than keeping the maximum distance constant, adaptive kernels keep the number of observations included in the neighborhood constant, as illustrated by the next figure.\n\nIn this case, I opt for an adaptive kernel, because I want to ensure that the model includes sufficient data when generating regression models in both rural and urban areas. If I were to use a fixed kernel, then urban areas would include more data in their local regressions and rural areas would include less.\n\n\nFitting the Geographically Weighted Regression Model\nThe GWR model in the spgwr package requires point geometry coordinates as inputs, so I find the coordinates for each local authority’s centroid.\ncentroids &lt;- dentistry %&gt;%\n  st_centroid() %&gt;%\n  st_coordinates()\nThe syntax for the GWR model is simpler for sp objects, so I convert dentistry to an sp object.\ndentistry.sp &lt;- as(dentistry, \"Spatial\")\nBefore calculating my GWR model, I need to decide on the size of my adaptive kernel. The following code chunk uses the gwr.sel function in order to identify the optimal size. If you run this, you’ll notice that the function reports a series of adaptive q values and CV score values. Adaptive q refers to the proportion of observations that are included in a neighborhood, while CV score is a measure of how good of a fit the model, where a lower score represents a better fit. Essentially, this function finds the CV score corresponding to a number of different Adaptive q values and selects the Adaptive q that results in the smallest CV score.\nGWRbandwidth_adapt &lt;- gwr.sel(extraction_rate ~ pctChild + severe_disability_rate + foreign_rate + unemployment_rate, data = dentistry.sp, adapt = T)\nMy optimal adaptive kernel size, GWRbandwidth_adapt, incorporates the nearest 11 points in local regression calculations. Using this kernel size as one input, I fit a GWR model as follows.\nGWR_adapt &lt;- gwr(extraction_rate ~ pctChild + severe_disability_rate + foreign_rate + unemployment_rate, data = dentistry.sp, adapt = GWRbandwidth_adapt, hatmatrix = T, se.fit = T)\nThe model automatically outputs as a list, which contains several nested lists and is difficult to work with. In order to get our GWR results into a form with I’m familiar with, I convert it to a Data Frame and attach it to the original dentistry.sf data.\nresults &lt;-as.data.frame(GWR_adapt$SDF)\ngwr.map_adapt &lt;- cbind(dentistry, as.matrix(results))\n\n\nDetermining Statistical Significance\nIn their paper, Broomhead et al. present side-by-side maps for each predictor. The maps on the left display the coefficients for the predictor in each local authority, and the maps on the right display the same information, but only for coefficients which are statistically significant. Unfortunately, they do not describe how they determined statistical significance, so I had to figure out my own methodology. I decided to follow the approach presented in section 9.6.7 of this “Spatial Modelling for Data Scientists” tutorial. Using the coefficients and corresponding standard errors calculated by the GWR model, I calculated the test statistic: t = (estimated coefficient)/(standard error).\ngwr.map_adapt &lt;- gwr.map_adapt %&gt;%\n  mutate(t_pct_child = pctChild.1/pctChild_se,\n         t_foreign_rate = foreign_rate.1/foreign_rate_se,\n         t_severe_disability_rate = severe_disability_rate.1/severe_disability_rate_se,\n         t_unemployment_rate = unemployment_rate.1/unemployment_rate_se,)\nAccording to the tutorial, one can roughly consider a value to be statistically significant when the absolute value of the t-value is greater than 1.96. This is how I consider coefficients to be statistically significant when I map them in the results section.\nThis is definitely a shortcut for assessing statistical signififance and is far from the most accurate methodology. A more robust approach would be to calculate p-values adjusted for multiple-hypothesis testing. Lu et al. describe how to calculated p-values in this article, and there appears to be a fairly easy way to implement this in R if one uses the GWmodel package for GWR instead of the spgwr package."
  },
  {
    "objectID": "posts/dental-gwr/index.html#results",
    "href": "posts/dental-gwr/index.html#results",
    "title": "Replication Study with Spatial Regression",
    "section": "Results",
    "text": "Results\nTo replicate Broomhead et al.’s maps, I created choropleth maps using the tmap package, classifying observations using the jenks method centered at 0. The code used to generate these maps is included in my replication repository.\n\nPercent ChildrenPercent DisabilityForeign RateUnemployment Rate\n\n\n\n\n\nFigure 1: Coefficients and their statistical significance for percent children. n = 127 local authorities with statistically significant results.\n\n\n\n\n\n\n\nFigure 2: Coefficients and their statistical significance for percent with disability. n = 96 local authorities with statistically significant results.\n\n\n\n\n\n\n\nFigure 3: Coefficients and their statistical significance for percent foreign-born. n = 131 local authorities with statistically significant results.\n\n\n\n\n\n\n\nFigure 4: Coefficients and their statistical significance for the unemployment rate. n = 30 local authorities with statistically significant results.\n\n\n\n\n\nI also created the following maps to illustrate the accuracy of the model’s predictions.\n\n\n\nFigure 5: Actual extraction rates, predicted extraction rates, and associated R-Squared values."
  },
  {
    "objectID": "posts/dental-gwr/index.html#discussion",
    "href": "posts/dental-gwr/index.html#discussion",
    "title": "Replication Study with Spatial Regression",
    "section": "Discussion",
    "text": "Discussion\nFigures 1 - 4 illustrate that relationships between each explanatory variable and the response variable vary greatly across the country. The left map in each figure reveals that for each predictor, there are regions where the coefficient is positive and regions where the coefficient is negative. And the right map in each figure reveals that for every predictor except the percentage of the population composed of children, there are statistically significant results for both positive and negative coefficients. This leads me to believe that the social factors leading to pediatric dental extractions vary across space, but it is difficult to deduce why these relationships are different without knowing the region better. This result is similar to the results from Broomhead et al.’s paper, as their maps revealed both positive and negative coefficients for most of their indicators. The values for the coefficients in my model are also astonishingly large. It seems unlikely that a 1% increase in any of my indicators would result in the outcome variable of dental extractions changing by over a thousand when there are no local authorities with even 500 total extractions per 100k people.\nDespite selecting only social predictors in my analysis, I found that the number of statistically significant results was similar for each coefficient in my study to those from Broomhead et al.’s study. In my model, the number of significant coefficients was 127 for percent children, 96 for percent with disability, 131 for percent foreign, and 30 for the unemployment rate. Similarly, Broomhead et al. found 44, 27, 250, and 29 significant coefficients for their predictors, which were mean dental caries in 5 year olds, a socioeconomic index called the Index of Multiple Deprivation, units of dental activity, and child access to dentists, respectively. Notably, one of their predictors, the number of contracted Units of Dental Activity, far outperformed all of the other predictors in both the original and replication study. According to their paper, “UDAs are used to measure the amount of work conducted during dental treatment in NHS primary care general dental services.” Since UDAs represent the amount of dental work in a region, it makes sense that we see high UDA levels where we see high rates of one particular dental procedure. Broomhead et al. suggest several reasons why there might be few statistically significant results for the other predictors. There could actually be no relationship between the independent and dependent variables, the local models rely on very few data points so they could be highly influenced by even one outlier, and slight issues with the normality of data (as we saw previously) could all be in play.\nFor the results that were statistically significant, let’s examine exactly what they tell us. My first predictor, the percentage of population composed by children, shows that significant results had strictly positive coeffients. This makes sense to me, because there should be more pediatric dental extractions where there are more children. My second and third predictors, the percent of disabled and foreign-born individuals, also have primarily positive significant coefficients. I’m not sure why this is true: perhaps these individuals are more likely to take children with tooth issues to a dentist or perhaps they are more likely to have tooth issues themselves? My final predictor, the unemployment rate, actually exhibits more negative than positive significant coefficients. Rather than a sign of good-health, this may have occured because unemployed people are less able to afford expensive tooth procedures.\nI also extend the analysis to generate predicted extractino rates around the UK. Figure 5 shows side-by-side maps illustrating the actual extraction rates and my predicted extraction rates. It appears that my model predicts far less local variation in extraction rates than exist in reality, as the prediction map contains far more orange and less yellow and red.\nFigure 5 also shows the R-Squared values for the GWR model across the country. Recall that we observed an R-Squared value of 0.1916 in the linear regression model. According to this figure, the minimum value for a local authority is 0.2150, and in parts of the country, especially the southeastern and northwestern parts, R-Squared values are far higher, sometimes around 0.7. I calculated the average R-Squared value for the GWR model and found it to be 0.5072. Thus, while linear regression can explain 19% of the variation in pediatric dental extractions, GWR can explain 51% of the variation – that’s 32% more! While this GWR model is far from perfect, it is far better than the OLS alternative, and in certain regions of the country is highly accurate.\nOne major limitation of both the original study and my replication is the manner in which the outcome variable was provided. If the number of dental extractions had been provided at a more granual level, we may have been able to observe stronger spatial patterns. Additionally, because some of the data was excluded due to privacy concerns, about 37% of the data in the original table had to be imputed with 0s. This represents a major limitation of the analysis because it makes the model underestimate the coefficients for any local authority that had very few pediatric dental extractions. A limitation of my replication in particular is that predictor variables were exclusively social variables. I had hoped to include a more established denistry-related variable such as fluoridation levels in drinking water, but struggled to find good data for the UK."
  },
  {
    "objectID": "posts/dental-gwr/index.html#conclusions",
    "href": "posts/dental-gwr/index.html#conclusions",
    "title": "Replication Study with Spatial Regression",
    "section": "Conclusions",
    "text": "Conclusions\nBy completing this replication of Broomhead et al.’s study, I verified their results and made several improvements & extensions to their methods. As they had determined using their own explanatory variables, the relationships between explanatory variables and pediatric dental extractions vary greatly across space. Some of those relationships are statistically significant, but many of them are not, indicating that the selected indicators do a subpar job of modeling outcomes.\nIn the original paper, the response variable was not normalized and old cartographic boundaries were used for the analysis. I improved the study by normalizing the response variable, dental extractions, with respect to population, and cleaning the data such that all variables match current local authority boundaries in the UK. I also extended the study by producing a reproducible research compendium and evaluating whether a regression model is appropriate given my explanatory variables and comparing OLS regression and GWR performance using R-Squared values. Overall, I found that the dataset satisfies the conditions for regression and that GWR far outperforms OLS regression, but the predictors used in this analysis perform relatively poorly on their own. Perhaps there are other indicators, such as water fluoridation, which demonstrate more statistically significant results; further research ought to assess this possibility."
  },
  {
    "objectID": "posts/dental-gwr/index.html#references",
    "href": "posts/dental-gwr/index.html#references",
    "title": "Replication Study with Spatial Regression",
    "section": "References",
    "text": "References\n\nLu, Binbin, Paul Harris, Martin Charlton, and Chris Brunsdon. 2014. “The Gwmodel R Package: Further Topics for Exploring Spatial Heterogeneity Using Geographically Weighted Models.” Geo-Spatial Information Science 17 (2): 85–101.\nBroomhead, T., Rodd, H. D., Baker, S. R., Jones, K., Davies, G., White, S., Wilcox, D., Allen, Z., & Marshman, Z. (2021). National patterns in paediatric hospital admissions for dental extractions in England. Community Dentistry and Oral Epidemiology, 49(4), 322–329. https://doi.org/10.1111/cdoe.12603\nVariance Inflation Factor background information: https://online.stat.psu.edu/stat462/node/180/\nHandout from my data science class on the conditions for linear regression\nGWR Tutorials:\n\nhttps://gdsl-ul.github.io/san/09-gwr.html\nhttps://cran.r-project.org/web/packages/spgwr/spgwr.pdf\nhttps://rpubs.com/quarcs-lab/tutorial-gwr1\nhttps://rstudio-pubs-static.s3.amazonaws.com/44975_0342ec49f925426fa16ebcdc28210118.html"
  },
  {
    "objectID": "posts/clearings-report/index.html",
    "href": "posts/clearings-report/index.html",
    "title": "Scenic Viewsheds",
    "section": "",
    "text": "When one thinks of conservation efforts, they probably imagine preservation of natural spaces and indigenous flora and fauna. A lesser known, yet nonetheless important, aspect of conservation planning is the scenic aspect. Breathtaking views, be they of mountain peaks or of rolling fields, are a critical aspect of our interaction with the natural landscape, and it would be a disservice to future generations to let them disappear. In this analysis, I investigate the effects of using different types of Digital Elevation Models in viewshed analyses, hoping this information will be valuable to conservation planners seeking to preserve scenic views."
  },
  {
    "objectID": "posts/clearings-report/index.html#introduction",
    "href": "posts/clearings-report/index.html#introduction",
    "title": "Scenic Viewsheds",
    "section": "",
    "text": "When one thinks of conservation efforts, they probably imagine preservation of natural spaces and indigenous flora and fauna. A lesser known, yet nonetheless important, aspect of conservation planning is the scenic aspect. Breathtaking views, be they of mountain peaks or of rolling fields, are a critical aspect of our interaction with the natural landscape, and it would be a disservice to future generations to let them disappear. In this analysis, I investigate the effects of using different types of Digital Elevation Models in viewshed analyses, hoping this information will be valuable to conservation planners seeking to preserve scenic views."
  },
  {
    "objectID": "posts/clearings-report/index.html#background",
    "href": "posts/clearings-report/index.html#background",
    "title": "Scenic Viewsheds",
    "section": "Background",
    "text": "Background\nYou don’t have to go far to see the importance of scenic views in conservation efforts. Middlebury’s own Town Plan includes a section on “scenic resources management” which enumerates a number of vistas that are critical to the character of the town (Planning Commission, 2017). The nearby town of Monkton commissioned a 27-page-long report inventorying their scenic views for planning purposes (Addison County RPC, 2016). And in a previous winter term class at Middlebury College, students identified scenic features in the Town of Middlebury as a part of a conservation plan they created for the town (Anixter et al., 2013a).\nAlthough scenic views are a subjective matter, planners – including the authors of the Monkton report and the student conservation plan – still employ GIS methods to aid in the identification of prime viewing locations. In particular, to determine what can be seen from a given location, a planner might conduct a viewshed analysis (Addison County RPC, 2016).\nA viewshed analysis determines whether a raster of points in a Digital Elevation Model (DEM) are in the line of sight from a given viewpoint(s) (Addison County RPC, 2016). There are more sophisticated viewshed tools, but the basic version outputs a binary raster with a 1 where a location is visible and a 0 where a location is not visible (Cuckovic, 2016). While viewsheds provide planners with a seemingly more objective way of determining prime viewing locations, there are plenty of errors inherent to this type of analysis.\nIn this report, we will investigate the sensitivity of viewshed analyses on Middlebury College lands to the choice of DEM. This is a highly relevant parameter to investigate, because whether a given point is visible from a given viewpoint depends entirely on the elevation data in the DEM. Furthermore, planners are not consistent in the DEMs they choose for their analyses. For example, the authors of the Monkton report used a Digital Terrain Model (DTM) in their analysis, completely ignoring forest cover (Addison County RPC, 2016). On the other hand, the Middlebury students attempted to account for forest cover by adding 15 meters to a DTM where vegetation was present, regardless of whether that vegetation was actually trees or whether the trees were actually that tall (Anixter et al., 2013b). Other authors suggest beginning with a Digital Terrain Model and modifying it with vegetation elevation data from a Digital Surface Model or other source (Guth, 2009). In this report, we compare viewsheds generated for four different viewpoints using a bare earth DTM, a leaf-off DEM derived from a DTM and a DSM, and a leaf-on DEM derived from the same DTM and DSM."
  },
  {
    "objectID": "posts/clearings-report/index.html#methods-results",
    "href": "posts/clearings-report/index.html#methods-results",
    "title": "Scenic Viewsheds",
    "section": "Methods & Results",
    "text": "Methods & Results\nFirst, I walked around campus, selected four points of varying surroundings (open field, built environment, deciduous trees, and coniferous trees), and took pictures at those locations to ground-truth my results. I also used go/mobility to record each point’s latitude and longitude.\nBack at my computer, I imported a DTM and a DSM from the Vermont Center for Geographic Information into Google Earth Engine and used them to generate the three DEMs: bare earth (DTM), leaf-off, and leaf-on. I exported these DEMs to my Google drive and exported their corresponding hillshades as an asset, and I made my code available here.\nI then created an excel file for each point containing the point’s latitude and longitude, added these as delimited text layers into QGIS, and then used the “Create Viewpoint” tool in the QGIS “Visibility Analysis” plugin to make each viewpoint suitable for the viewshed analysis. Since people generally stand on the ground – not in trees – I used the bare earth digital terrain model to create the viewpoints. For my radius of analysis, I chose 15,000 meters in order to cover the entire DEMs I downloaded for Middlebury. For my observer height, I used 1.6 meters, and for my target height, I used 0 meters. After that, I used the “Viewshed” tool in the “Visibility Analysis” plugin to create viewsheds for each point and each terrain model. After generating all 12 viewsheds, I uploaded them to Google Earth Engine as GeoTIFF assets and enabled sharing.\nBack in Google Earth Engine, I imported my viewsheds and created maps of agreement and disagreement between the three DEMs for each of my four points. This visualization script is available here. Please see the resulting images and figures in the following few pages.\n\nPoint 1: Built EnvironmentPoint 2: Open/FieldPoint 3: DeciduousPoint 4: Coniferous\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Agreement and disagreement map for chapel viewpoint over a leaf-on hillshade.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Agreement and disagreement map for the open field viewpoint over a leaf-on hillshade.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Agreement and disagreement map for deciduous viewpoint over a leaf-on hillshade.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Agreement and disagreement map for coniferous viewpoint over a leaf-on hillshade."
  },
  {
    "objectID": "posts/clearings-report/index.html#discussion",
    "href": "posts/clearings-report/index.html#discussion",
    "title": "Scenic Viewsheds",
    "section": "Discussion",
    "text": "Discussion\nFirst, let us turn our attention to the agreement and disagreement maps. In these maps, a pixel is green if all viewsheds agree that the land represented by that pixel is visible and gray/black if all viewsheds agree that the land represented by that pixel is not visible. Notice how much of these maps are neither gray nor green. This indicates that there is substantial disagreement between the viewsheds produced by different DEMs. Let us investigate the ways in which each DEM affects our results.\nWhen the bare earth model is the only DEM in which a pixel can be seen, my maps color that pixel brown; when the bare earth model is the only DEM in which a pixel cannot be seen, my maps color that pixel purple. Notice how much brown there is in these maps, particularly for points 1 and 4 (Fig 1, 4). When there are no trees and buildings in the way, we can often see further. In reality, we cannot see through buildings and trees, so including them in our analysis improves the accuracy of our results. On the other hand, notice how little purple shows up in our maps. This indicates that we tend not to miss stuff in our analysis when using a DTM. If we want to minimize the amount of visible land that we fail to include in our results, the bare earth model would be a good – generous – approach. When we do see purple, it tends to occur on rooftops, since our bare earth model ignores buildings (Fig 3, 6, 9).\nNow let’s turn our attention to the leaf-off DEM. When the leaf-off model is the only DEM in which a pixel can be seen, my maps color that pixel red; when the bare earth model is the only DEM in which a pixel cannot be seen, my maps color that pixel dark blue. There is relatively little red in my maps – this tends to occur on buildings that are visible when their height is raised to the accurate level in the leaf-off model but are blocked from our vision when deciduous trees get in the way in the leaf-on model (Fig 1, 2, 3). It may be more accurate to use the leaf-off model when modeling winter conditions and the leaf-on model when modeling summer conditions, but this also becomes complicated because one still cannot see through tree trunks and branches in the winter.\nFinally, we turn our attention to the leaf-on DEM. When the leaf-on model is the only DEM in which a pixel can be seen, my maps color that pixel cyan; when the leaf-on model is the only DEM in which a pixel cannot be seen, my maps color that pixel yellow. Some cyan patches occur where there are deciduous trees – this makes sense because the trees raise the elevation of these pixels, making them more visible (Fig 1, 2, 3, 4). There are also some instances of swaths of open fields and buildings that are only visible in the leaf-on model (Fig 1, 3). It seems to me that additional trees would only serve to obscure the views of fields and buildings behind them, so I don’t understand why this occurs. Finally, as you can see by scrolling between analogous figures with the leaf-off and leaf-on hillshades, yellow shows up where points are hidden behind deciduous trees but visible if those trees are absent, as one would expect (Fig 1, 2, 3, 4)."
  },
  {
    "objectID": "posts/clearings-report/index.html#conclusion",
    "href": "posts/clearings-report/index.html#conclusion",
    "title": "Scenic Viewsheds",
    "section": "Conclusion",
    "text": "Conclusion\nThese results reveal both strengths and weaknesses of all three DEMs. If one cares more about ensuring that all points that are truly visible are marked as visible in their analysis, they might opt for the DTM – or maybe just add buildings to the DTM – and accept that their model will overestimate results, as the authors acknowledged in the Monkton report (Addison County RPC, 2016). If one cares more about ensuring that points marked as visible by their analysis truly are visible in reality, they might opt for the leaf-off or leaf-on composite DEM, depending on whether it is winter or summer, as the students did in their winter term conservation planning class or in the more sophisticated manner illustrated in my Earth Engine script (Anixter et al., 2013a). While the latter approach will reduce the number of points falsely labelled as visible, it will also increase the number of points falsely labelled as not visible, because in reality one can see through gaps in the trunks and branches of trees. Overall, there are strengths and weaknesses to each DEM; perhaps an analyst’s choice of DEM should depend on which type of error they wish to minimize.\nIn this report, I conducted a cursory analysis of the strengths and weaknesses of different DEMs in viewshed analysis, but I did not have the opportunity to conduct a viewshed analysis for all Middlebury College lands. Future work ought to actually identify places on Middlebury College lands with scenic value for the purposes of conservation planning. Additionally, while my figures revealed a lot about the ways in which different DEMs overestimate or underestimate viewsheds compared to other DEMs, there were some observations that I failed to explain, as well as other options for DEMs (such as bare earth plus buildings), both of which future work could investigate.\nFurthermore, any substantive work surrounding scenic views ought to incorporate personal opinion, perhaps via survey or public forum, in order to incorporate the subjectivity of aesthetics into the identification of scenic views. It is worth considering whether it makes sense at all to use a machine to perform such a subjective task as locating places of beauty."
  },
  {
    "objectID": "posts/clearings-report/index.html#bibliography",
    "href": "posts/clearings-report/index.html#bibliography",
    "title": "Scenic Viewsheds",
    "section": "Bibliography",
    "text": "Bibliography\nAddison County Regional Planning Commission. (2016). Monkton Scenic Viewshed Study 2015‐2016. https://drive.google.com/file/d/1r6ooUZvOWZ3z-sEZjNT2B5r29Orfhnut/view\nAnixter, H., Bering, J., Bernegger, Q., & Borah, A. (2013a). A Conservation Plan for Middlebury, Vermont. https://drive.google.com/file/d/1-LkGxMfSvb1wxqfrQ2jB41S2vjYIS0go/view\nAnixter, H., Bering, J., Bernegger, Q., & Borah, A. (2013b). Technical Appendix. https://drive.google.com/file/d/1-LkGxMfSvb1wxqfrQ2jB41S2vjYIS0go/view\nCuckovic, Z. (2016). Advanced Viewshed Analysis: A Quantum GIS Plug-in for the Analysis of Visual Landscapes. The Journal of Open Source Software, 1(4), 32. https://doi.org/10.21105/joss.00032\nGuth, P. (2009). Incorporating Vegetation in Viewshed and Line-of-Sight Algorithms. ASPRS/MAPPS 2009 Specialty Conference. https://www.asprs.org/wp-content/uploads/2010/12/Guth.pdf\nPlanning Commission. (2017). Middlebury 2017 Town Plan. Town of Middlebury. https://cms5.revize.com/revize/middlebury/document_center/Planning%20Zoning/Middlebury-2017-Town-Plan.pdf"
  },
  {
    "objectID": "posts/malawi-deforestation/index.html",
    "href": "posts/malawi-deforestation/index.html",
    "title": "Deforestation in Protected Areas",
    "section": "",
    "text": "Just because an area is “protected” does not necessarily mean it is free from tree loss. There are many ways to practice environmental conservation, and some methods are far more protective than others. In this analysis, I ask whether less protected parks in Malawi are subject to greater deforestation."
  },
  {
    "objectID": "posts/malawi-deforestation/index.html#introduction",
    "href": "posts/malawi-deforestation/index.html#introduction",
    "title": "Deforestation in Protected Areas",
    "section": "",
    "text": "Just because an area is “protected” does not necessarily mean it is free from tree loss. There are many ways to practice environmental conservation, and some methods are far more protective than others. In this analysis, I ask whether less protected parks in Malawi are subject to greater deforestation."
  },
  {
    "objectID": "posts/malawi-deforestation/index.html#analysis",
    "href": "posts/malawi-deforestation/index.html#analysis",
    "title": "Deforestation in Protected Areas",
    "section": "Analysis",
    "text": "Analysis\nUsing Google Earth Engine and Hansen’s Global Forest Change dataset, I find that in Malawi, less protected parks witness substantially higher levels of deforestation than strictly protected parks. In fact, the percentage of tree cover loss was 35 times greater in less protected parks, at 1.75% versus 0.05%. You can see these results clearly in figure 1 below. Areas outlined in neon green are strictly protected parks while areas outlined in white are less strictly protected parks. Red dots, which represent forest loss, are far more common inside of the white boundaries than the green boundaries.\n\n\n\nFigure 1: Deforestation in Malawi.\n\n\nIn figure 1, tree cover is symbolized in dark green and tree loss in red, strictly protected areas are outlined in neon green, and less protected areas are outlined in white. Generated with Google Earth Engine. Please see my code here."
  },
  {
    "objectID": "posts/malawi-deforestation/index.html#a-note-on-our-class-definitions",
    "href": "posts/malawi-deforestation/index.html#a-note-on-our-class-definitions",
    "title": "Deforestation in Protected Areas",
    "section": "A Note on our Class Definitions",
    "text": "A Note on our Class Definitions\nThis analysis hinges on a few major assumptions. One relates to the way we classify strictly and less protected parks. In this project, we used the International Union for Conservation of Nature’s (IUCN) classifications of protected areas, assuming that the following categories are strictly protected:\n\n1a – Strict Nature Preserve (0 in Malawi)\n1b – Wilderness Area (0 in Malawi)\nII – National Park (5 in Malawi)\nIII – Natural Monument or Feature (0 in Malawi)\nIV – Habitat/Species Management Area. (4 in Malawi)\n\nWe assumed that these categories are less protected:\n\nV – Protected Landscape/Seascape (0 in Malawi)\nVI – Protected area with sustainable use of natural resources (0 in Malawi)\nNot Reported (121 in Malawi)\nNot Applicable (3 in Malawi)\n\nIt seems reasonable to assume that Malawi’s National Parks and Habitat/Species Management Areas would be well protected, but we made a major assumption in our designation of the less protected parks. Of the 133 protected areas in Malawi, 121 of them do not have a reported IUCN category. We genuinely do not know the categorization of those parks, so we cannot deduce how well they are protected. Additionally, the IUCN categories are supposedly not applicable for 3 protected areas. With no additional information about these parks, it is difficult to assess the validity of our assumption that they are less protected. Because the IUCN categories of all of the ‘less protected’ parks were either not reported or not applicable, our designation of these parks as less protected is a substantial assumption that certainly impacted the results."
  },
  {
    "objectID": "posts/malawi-deforestation/index.html#discussion",
    "href": "posts/malawi-deforestation/index.html#discussion",
    "title": "Deforestation in Protected Areas",
    "section": "Discussion",
    "text": "Discussion\nThe ways we define ‘forest’, ‘tree cover’, and ‘forest loss’ also affect our analysis. In this assignment, we considered a pixel to represent a ‘forest’ if it was composed at least 30% by tree cover. This may be a common threshold for this type of analysis, but it is also a subjective threshold. Personally, I understand the word ‘forest’ to mean a heavily wooded area, far more than 30% tree cover. The way we measure the percentage of tree cover and determine whether an area experienced forest loss is also subjective. For the purposes of this analysis, I used Hansen’s tree cover and forest loss bands. A more thorough investigation ought to interrogate the way Hansen et al calculated these bands and make any appropriate adjustments.\nFurthermore, the methodology used in this analysis is incapable of telling when the loss of trees actually constitutes deforestation. For example, my analysis detected tree cover loss near Chikangawa, which falls within one of the less protected parks. Upon inspecting the area with Google Earth Pro, I found that there are regularly trees in this area. And when I researched the location, I discovered the presence of Kawandama Hills Plantation, which plants and harvests trees every year. Considering this region to be the subject of deforestation is misleading, because although trees are regularly lost in Chikangawa, they are also regularly replanted. This situation is not unique to Chikangawa and unquestionably affected the results of this analysis. Some locations, such as Nyika National Park, experienced loss of naturally occurring trees, while other locations, like Chongoni Forest, contain plantations. This analysis is a great first step towards identifying deforestation in Malawi but ought to be validated and supported with additional research."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html",
    "href": "posts/hurricane-twitter/index.html",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "",
    "text": "As social media use has risen, a massive quantity of Volunteered Geographic Information has become available to the general public. In the age of big data, researchers and corporations around the world use VGI from Twitter data to conduct research in fields from marketing to geography. Wang et al. illustrate in Spatial, temporal, and content analysis of Twitter for wildfire hazards that Twitter data provides valuable information about natural hazards and their impacts. Specifically, they find that during a natural disaster, Twitter activity reflects the location and time of the disaster, as people tend to Tweet situational updates and news and local governments play a prominent role in retweet networks. Identifying spatial, network, and content trends in Twitter activity during natural disasters is useful because it can help us formulate our responses to such disasters. However, Crawford and Finn articulate a number of uncertainties and ethical dilemmas that accompany the use of Twitter data during natural disasters, which we ought to be wary of when conducting or reading about this kind of work.\nIn 2019, Professor Holler conducted an analysis of Twitter activity relating to Hurricane Dorian and the Sharpiegate scandal, seeking to identify differences between the Twitter storm and the actual storm. He found that despite President Trump’s statements that the storm would hit Alabama, the Twitter activity reflected the actual trajectory of the storm.\nIn this investigation, I conduct a replication of his work, applying a similar methodology to Twitter data collected during Hurricane Ida. The purpose of conducting any replication is to confirm the validity of one’s methodology and the generalizability of their findings. In this case in particular, I’m interested in confirming whether Twitter activity about a hurricane during that storm is consistently more prevalent where that storm makes landfall. Replicating this kind of spatial research continues to be relevant as the climate crisis intensifies and hurricanes become more and more powerful. Hurricane Ida devastated Louisiana and parts of the North East, so I anticipate finding higher levels of Twitter activity in those regions. I also extend the analysis in new directions by conducting a temporal analysis of Twitter hotspots, a temporal analysis of the sentiments of Tweets, and a network analysis of retweet activity.\nFor full documentation of this work, please visit these links:\n\nMy replication research repository\nProfessor Holler’s original research repository"
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#abstract",
    "href": "posts/hurricane-twitter/index.html#abstract",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "",
    "text": "As social media use has risen, a massive quantity of Volunteered Geographic Information has become available to the general public. In the age of big data, researchers and corporations around the world use VGI from Twitter data to conduct research in fields from marketing to geography. Wang et al. illustrate in Spatial, temporal, and content analysis of Twitter for wildfire hazards that Twitter data provides valuable information about natural hazards and their impacts. Specifically, they find that during a natural disaster, Twitter activity reflects the location and time of the disaster, as people tend to Tweet situational updates and news and local governments play a prominent role in retweet networks. Identifying spatial, network, and content trends in Twitter activity during natural disasters is useful because it can help us formulate our responses to such disasters. However, Crawford and Finn articulate a number of uncertainties and ethical dilemmas that accompany the use of Twitter data during natural disasters, which we ought to be wary of when conducting or reading about this kind of work.\nIn 2019, Professor Holler conducted an analysis of Twitter activity relating to Hurricane Dorian and the Sharpiegate scandal, seeking to identify differences between the Twitter storm and the actual storm. He found that despite President Trump’s statements that the storm would hit Alabama, the Twitter activity reflected the actual trajectory of the storm.\nIn this investigation, I conduct a replication of his work, applying a similar methodology to Twitter data collected during Hurricane Ida. The purpose of conducting any replication is to confirm the validity of one’s methodology and the generalizability of their findings. In this case in particular, I’m interested in confirming whether Twitter activity about a hurricane during that storm is consistently more prevalent where that storm makes landfall. Replicating this kind of spatial research continues to be relevant as the climate crisis intensifies and hurricanes become more and more powerful. Hurricane Ida devastated Louisiana and parts of the North East, so I anticipate finding higher levels of Twitter activity in those regions. I also extend the analysis in new directions by conducting a temporal analysis of Twitter hotspots, a temporal analysis of the sentiments of Tweets, and a network analysis of retweet activity.\nFor full documentation of this work, please visit these links:\n\nMy replication research repository\nProfessor Holler’s original research repository"
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#study-information",
    "href": "posts/hurricane-twitter/index.html#study-information",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "Study Information",
    "text": "Study Information\nIn his analysis of Twitter activity during Hurricane Dorian, Professor Holler loosely replicated the methods of Wang et al., applying similar methods to a different time, place, and disaster. Wang et al. used a Twitter API to (1) query generally for tweets containing the words “fire” and “wildfire”, and (2) query specifically for Tweets about wildfires in San Marcos and Bernardo, California. They then used the tm and igraph packages in R 3.1.2 to conduct textual and network analyses on their data. They also investigated the spatial distribution of these Tweets using a Kernal Density Estimation (KDE), but it remains unclear what computational resources they used for this last part of their analysis. They found significant clustering of Twitter activity where the wildfires were worst, discovered that the news media and local authorities were highly prevalent in retweet networks, and learned that the contents of Tweets referenced the geographic location and damage of the disaster.\nIn his replication, Professor Holler used a Twitter API to query for Tweets with keywords “hurricane”, “Dorian”, or “sharpiegate”. He modified Wang et al.’s methodology by using different method to analyze the spatial clustering of Tweets. Instead of using KDE, he created a Normalized Tweet Difference Index (NTDI) and tested for spatial culstering with the local Getis-Ord statistic. The NTDI is calculated as (Dorian Twitter activity - typical Twitter activity)/(Dorian Twitter activity + typical Twitter activity, and his inspiration to use this index comes from the frequent use of other similar indices such as the Normalized Vegetation Difference Index (NVDI). NTDI illustrates where Twitter activity is higher than the baseline and the local Getis-Ord statistic indicates where Twitter activity is significantly higher than other parts of the study area. Professor Holler chose not to conduct a network analysis of retweet activity, focusing exclusively on Tweet content and prevalence. The study found that despite the hysteria surrounding President Trump’s claim that the storm would pass through Alabama, the Twitter data still clustered only in the affected areas of the Atlantic coast. In his analysis, Professor Holler used R, including the rtweet, rehydratoR, igraph, sf, and spdep packages, and he provided his code for students to use in their replication studies."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#materials-and-methods",
    "href": "posts/hurricane-twitter/index.html#materials-and-methods",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "Materials and Methods",
    "text": "Materials and Methods\nSimilar to Professor Holler, I conducted my replication study using R. Required packages include tidyverse, here, svDialogs, rtweet, rehydratoR, tidytext, tm, igraph, ggraph, tidycensus, sf, and spdep.\nThe Twitter data used in this analysis were collected using Twitter API searches conducted by other people. Professor Holler conducted several searches for Tweets relating to Hurricane Ida as the disaster unfolded. I used three of his searchers in my analysis: each search used keywords “hurricane” and “ida” and they were conducted on September 2, September 5, and September 10, 2021. My classmate, Castin Stone, queried for baseline Tweets (instead of keywords he searched for “-filter:verified OR filter:verified”) in November 2021, and I used his data as my control group. I also obtained the geometry and population data of counties in the study region from the census using the tidycensus package.\nYou can rehydrate any of the Twitter search results using the status ID’s linked below (the first three links are in a private repository):\n\nSeptember 2 Hurricane Ida status ID’s\nSeptember 5 Hurricane Ida status ID’s\nSeptember 10 Hurricane Ida status ID’s\nBaseline status ID’s, November 2021\n\nMy analysis closely followed Professor Holler’s methodology. First, I combined the three Hurricane Ida search results into one table using dplyr’s union function. Then, I converted the geographic information in both the control and Hurricane Ida Twitter data to latitude and longitude coordinates, and subsetted the data to include only Tweets with sufficient geographic specificity. I used the Hurricane Ida data to create a line graph illustrating the temporal distribution of Twitter activity, and upon cleaning the text data, I created a bar graph revealing the frequency with which unique words appeared in Tweets and a word cloud displaying the most common word pairs. Upon spatially joining Tweets to counties, I mapped tweet hotspots using the local Getis-Ord statistic, I mapped the heightened Twitter activity using the NTDI, and I mapped the density of Tweets as the number of tweets per 10,000 residents.\nI extended the analysis by mapping Tweet hotspots using the local Getis-Ord statistic over different time periods during the disaster, conducting a sentiment analysis of Tweets over those periods, and fixing bugs in the network analysis of retweet activity. I explain my additions to the code in the following section."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#my-extensions-to-the-replication",
    "href": "posts/hurricane-twitter/index.html#my-extensions-to-the-replication",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "My Extensions to the Replication",
    "text": "My Extensions to the Replication\nTo conduct my temporal analyses, I first subsetted my data into three periods of time. The storm hit Louisiana in the last days of August and the northeast in the first days of September. Thus, by subsetting my data into August 29-31, September 1-3, and September 4-10, I was able to evaluate the differences between when the storm hit Louisiana, when it hit the northeast, and the first few days after it dissipated.\nI used this code to subset my data:\ntime1 &lt;- tevent_rawfull %&gt;%\n  filter(str_detect(created_at, \"-08-29|-08-30|-08-31\"))\n\ntime2 &lt;- tevent_rawfull %&gt;%\n  filter(str_detect(created_at, \"-09-01|-09-02|-09-03\"))\n\ntime3 &lt;- tevent_rawfull %&gt;%\n  filter(str_detect(created_at, \"-09-(0[4-9]|10)\"))\nTo map the Twitter hotspots using the local Getis-Ord statistic, I followed the same methodology employed by Professor Holler. To conduct the sentiment analysis, I used some techniques that I learned in my Data Science course. First, I queried for a sentiment lexicon entitled bing.\nsentiments &lt;- get_sentiments(\"bing\") #bing is a certain list of sentiments\nsentiments\nAfter cleaning the new text datasets using the same methodology as Professor Holler, I inner-joined my text datasets to the sentiment list.\n# Time1\ntime1_wordsent &lt;- time1_words %&gt;%\n  inner_join(sentiments)\n\n# Time2\ntime2_wordsent &lt;- time2_words %&gt;%\n  inner_join(sentiments)\n\n# Time3\ntime3_wordsent &lt;- time3_words %&gt;%\n  inner_join(sentiments)\nFinally, I graphed the most common words conveying positive and negative sentiments that appear in each time period as follows:\nsentiment_analysis1 = time1_wordsent %&gt;%\n  group_by(word, sentiment) %&gt;%\n  summarize(num = n()) %&gt;%\n  filter(num &gt; 1000) %&gt;%\n  ggplot(aes(y = reorder(word, num), x = num, fill = sentiment)) +\n    geom_bar(stat = \"identity\")+\n    facet_wrap(~sentiment, scales = \"free\")+\n    labs(title = \"Hurricane Ida Comments, August 29 - 31\", x = \"Frequency of Usage on Twitter\", y = \"Words Conveying Sentiment\")\n\nsentiment_analysis2 = time2_wordsent %&gt;%\n  group_by(word, sentiment) %&gt;%\n  summarize(num = n()) %&gt;%\n  filter(num &gt; 1000) %&gt;%\n  ggplot(aes(y = reorder(word, num), x = num, fill = sentiment)) +\n    geom_bar(stat = \"identity\")+\n    facet_wrap(~sentiment, scales = \"free\")+\n    labs(title = \"Hurricane Ida Comments, September 1 - 3\", x = \"Frequency of Usage on Twitter\", y = \"Words Conveying Sentiment\")\n\nsentiment_analysis3 = time3_wordsent %&gt;%\n  group_by(word, sentiment) %&gt;%\n  summarize(num = n()) %&gt;%\n  filter(num &gt; 500) %&gt;%\n  ggplot(aes(y = reorder(word, num), x = num, fill = sentiment)) +\n    geom_bar(stat = \"identity\")+\n    facet_wrap(~sentiment, scales = \"free\")+\n    labs(title = \"Hurricane Ida Comments, September 4 - 10\", x = \"Frequency of Usage on Twitter\", y = \"Words Conveying Sentiment\")\nCreating a network graph is a two step process. First, one must define the network. In this case, I defined nodes to be each user and edges to be any retweet or quote of that user.\ntevent_network &lt;- tevent_rawfull %&gt;%\n  network_graph(\"retweet,quote\") %&gt;%\n  simplify(remove.multiple = FALSE)\ntevent_network &lt;- delete.vertices(\n  tevent_network,\n  degree(tevent_network, mode=\"in\") &lt; 1\n  ) %&gt;%\n  simplify()\ntevent_network &lt;- delete.vertices(\n  tevent_network,\n  degree(tevent_network) &lt; 10)\nThe second step is to graph the network in a manner that is easy to understand. I adjusted the size of each node and the label of each node using the frequency with which each user was retweeted, drawing on Wang et al.’s network graph for inspiration.\npng(\"network.png\", 1500, 1500)\nnetwork &lt;- plot.igraph(\n    tevent_network,\n    vertex.size = degree(tevent_network)*.4,\n    vertex.label = ifelse(\n      degree(tevent_network) &gt; 5,\n      V(tevent_network)$name, \"\"),\n    vertex.label.cex = degree(tevent_network)*.1,\n    edge.arrow.mode = \"-&gt;\",\n    edge.arrow.size = 0.1\n  )\nThat summarizes the new intellectual work that I contributed to this replication. If you would like to see the complete code used for this study, you can click here, and if you would like to see my entire replication repository, please click here."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#results",
    "href": "posts/hurricane-twitter/index.html#results",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "Results",
    "text": "Results\n\nReplicated Figures\n\nTemporal AnalysisContent AnalysisWord NetworkNTDITweet Hotspots\n\n\n\n\n\nFigure 1: Tweets relating to Hurricane Ida over the course of the study timeframe.\n\n\n\n\n\n\n\nFigure 2: The most common words in tweets relating to Hurricane Ida.\n\n\n\n\n\n\n\nFigure 3: Network of word pairs in tweets relating to Hurricane Ida.\n\n\n\n\n\n\n\nFigure 4: Increased Twitter activity illustrated by a Normalized Tweet Difference Index.\n\n\n\n\n\n\n\nFigure 5: Hotspots of Twitter activity found with Local Getis-Ord statistic, August 29 - September 10.\n\n\n\n\n\n\n\nExtension Figures\n\nHotspots\n\nAugust 29 - 31September 1 - 3September 4 - 10\n\n\n\n\n\nFigure 6a: Temporal analysis of hotspots of Twitter activity found with Local Getis-Ord statistic, August 29 - 31.\n\n\n\n\n\n\n\nFigure 6b: Temporal analysis of hotspots of Twitter activity found with Local Getis-Ord statistic, September 1 - 3.\n\n\n\n\n\n\n\nFigure 6c: Temporal analysis of hotspots of Twitter activity found with Local Getis-Ord statistic, September 4 - 10.\n\n\n\n\n\n\n\nSentiment Analyses\n\nAugust 29 - 31September 1 - 3September 4 - 10\n\n\n\n\n\nFigure 7a: Sentiment analyses of Twitter activity at different times during the disaster, August 29 - 31.\n\n\n\n\n\n\n\nFigure 7b: Sentiment analyses of Twitter activity at different times during the disaster, September 1 - 3.\n\n\n\n\n\n\n\nFigure 7c: Sentiment analyses of Twitter activity at different times during the disaster, September 4 - 10.\n\n\n\n\n\n\n\nRetweet Network\n\n\n\nFigure 8: Retweet and quote network."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#discussion",
    "href": "posts/hurricane-twitter/index.html#discussion",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "Discussion",
    "text": "Discussion\nFirst, I’d like to describe the ways in which my findings compare to Holler’s and Wang et al.’s results. Figure 1 reveals spikes in Twitter activity when Hurricane Ida hit land. The first major spike occurred when the hurricane hit Louisiana, and the second major spike occurred when the hurricane hit the northeast. This result is consistent with Holler’s analysis of Hurricane Dorian, in which Twitter activity spiked when the hurricane hit land and then dwindled in the following days. Figure 3 also shares striking similarities with the corresponding graph from Holler’s analysis of Hurricane Dorian. The most common word pairings in both analyses were between words that conveyed location information, descriptions about the storm, safety information, and messages about climate change. One notable difference is that the figure in our replication study includes no word pairings relating to fake news, but this makes sense given the different context of this study.\nA similar distinction can be drawn between Figure 2 and the corresponding figure in the Dorian analysis. Figure 2 shows that words about the strength, location, and damage of the storm were most common in queried Tweets. The corresponding figure for Hurricane Dorian contained similar words, but it also included the words “Trump” and “realdonaldtrump” due to the Sharpiegate scandal. This figure also corroborates the findings of Wang et al.; in their analysis of wildfires, they found that the most commonly occuring words related to the effects and location of the wildfires. For both hurricanes, the NTDI maps and hotspot maps (Figure 4 and Figure 5 for the Hurricane Ida analysis) reveal a higher prevalence of Tweets in places where the storm actually struck land. Similarly, Wang et al. found a higher prevalence of Tweets about wildfires in locations near the wildfires.\nHoller’s analysis of Hurricane Dorian did not include a network analysis of retweet activity, so I cannot compare my results to his. However, Wang et al. included a network analysis in their paper. They found that news outlets and authorities on the disaster were the most frequently retweeted users. By searching Twitter for the Twitter handles corresponding to the largest nodes in Figure 8, I discovered that my findings corroborate their result. The largest nodes in my network graph are @NWSNewOrleans, which is the National Weather Service in New Orleans; @NHC_Atlantic, which is the National Hurricane Center; @nelsonqatlanta, who is a journalist and former reporter for CNN; and @galeanTV, who is also a journalist.\nFigure 6 is an extension from previous work, but it illustrates a pattern that make sense given what we have seen so far. Figure 6 shows that when Hurricane Ida first struck Louisiana, the only hotspot of Twitter activity was along the coast where the hurricane had struck. When the storm hit the northeast, a new hotspot emerged in the northeast and the hotspot in the south remained approximately where it was, becoming slightly more geographically specific. In the few days following the storm, hotspots remained in both the northeast and the south, but became slightly more geographically limited. One possible explanation for the decreasing geographic extent of Twitter hotspots is that the fringes of hotspot Twitter activity during the storm may not have witnessed as severe damage as the centers of the Twitter hotspots. In the aftermath of the storm, those whose belongings were destroyed in the storm may have continued Tweeting, while those who were left untouched did not.\nThe sentiment analysis in Figure 7 does not reveal much distinction between the sentiments expressed in each time period. In all cases, the frequency of negative words far outpaced the frequency of positive words. Certain words categorized as positive, such as “strong”, “strongest”, and “powerful” probably were descriptions of the storm and in context would not even have conveyed a positive sentiment. Different words were more prevalent during different time periods, but the prevalent words are all so similar that it is difficult to draw any conclusions from the results.\nIt is important to acknowledge the uncertainties inherent in our research methodology. As Crawford and Finn discuss in The limits of crisis data: Analytical and ethical challenges of using social and mobile data to understand disasters, the effects of natural disasters last for years, but studies using Twitter data reflect the impacts of just a few days. Additionally, the users of Twitter tend to be younger and urban residents, so the analysis may exaggerate the results for these demographics and underrepresent older and rural communities. Finally, Twitter’s algorithms and the presence of Twitter bots may have impacted the frequency with which users were retweeted, propogating error into my network analysis. The vast majority of users are real people, and despite demographic tendencies, the results reflect the anticipated geographic distribution of Twitter activity. However, it is important for any consumer of this research to understand that the figures and maps presented in this report are certainly subject to error."
  },
  {
    "objectID": "posts/hurricane-twitter/index.html#conclusions",
    "href": "posts/hurricane-twitter/index.html#conclusions",
    "title": "Hurricane Ida Spatial Twitter Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nOverall, this replication study corroborates the findings of Holler and Wang et al. In particular, the results reinforce the following conclusions of Holler and Wang et al.’s research, contributing to the growing body of research that the following statements are valid:\n\nTwitter activity is found to spike during a natural disaster and fade away in the following days.\nThe most common words and word associations in natural disaster-related Tweets concern the strength, location, and damage of the event.\nThe location of increased Twitter activity tends to be where the worst impacts of the disaster are felt.\nThe dominant voices in retweet networks during natural disasters are news outlets and authorities on the disaster.\n\nFurthermore, this analysis finds that hotspots of Twitter activity tend to begin when the disaster hits a region and continue for at least several days following the event. There may be a tendency for the geographic extent of hotspots to become slightly smaller in the days following the event, and future research ought to investigate the existence and causes of this phenomenon. This analysis also found that the sentiments expressed on social media during and just after a natural disaster tend to be similar. Further research might seek to corroborate this finding, potentially employing more nuanced textual analysis techniques. This replication study successfully corraborated the results of previous spatial analyses of Twitter data during natural disasters and extended the study in new directions. The replicated conclusions are fairly well-established, but the validity of my extensions ought to be validated by further research.\nReferences:\n\nCrawford, K., & Finn, M. (2015). The limits of crisis data: Analytical and ethical challenges of using social and mobile data to understand disasters. GeoJournal, 80(4), 491–502. https://doi.org/10.1007/s10708-014-9597-z\nProfessor Holler’s Research Compendium for his analysis of Hurricane Dorian\nWang, Z., Ye, X., & Tsou, M.-H. (2016). Spatial, temporal, and content analysis of Twitter for wildfire hazards. Natural Hazards, 83(1), 523–540. https://doi.org/10.1007/s11069-016-2329-6"
  },
  {
    "objectID": "posts/newtons-method/index.html",
    "href": "posts/newtons-method/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, we expand upon our previous implementation of logistic regression to add another optimization option. In our updated implementation of logistic regression, available at logistic.py, we can now optimize our logistic regression model with both gradient descent (with momentum) and Newton’s method. After implementing this extension, we demonstrate with an example that logistic regression with both optimizers converges to the same solution. Using the same example, we reveal that logistic regression with Newton’s method commonly converges to a solution in fewer training iterations than gradient descent. We also illustrate that selecting an appropriate value for \\(\\alpha\\) is critical to Newton’s method, as the optimizer will not converge to a solution if \\(\\alpha\\) is too large. Finally, we count the number of operations required in each iteration of model training for both gradient descent and Newton’s method, revealing that Newton’s method is more computationally expensive than gradient descent, especially when working with many features."
  },
  {
    "objectID": "posts/newtons-method/index.html#abstract",
    "href": "posts/newtons-method/index.html#abstract",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, we expand upon our previous implementation of logistic regression to add another optimization option. In our updated implementation of logistic regression, available at logistic.py, we can now optimize our logistic regression model with both gradient descent (with momentum) and Newton’s method. After implementing this extension, we demonstrate with an example that logistic regression with both optimizers converges to the same solution. Using the same example, we reveal that logistic regression with Newton’s method commonly converges to a solution in fewer training iterations than gradient descent. We also illustrate that selecting an appropriate value for \\(\\alpha\\) is critical to Newton’s method, as the optimizer will not converge to a solution if \\(\\alpha\\) is too large. Finally, we count the number of operations required in each iteration of model training for both gradient descent and Newton’s method, revealing that Newton’s method is more computationally expensive than gradient descent, especially when working with many features."
  },
  {
    "objectID": "posts/newtons-method/index.html#generating-training-data",
    "href": "posts/newtons-method/index.html#generating-training-data",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# Import packages\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# define function to generate data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# define function to plot data\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -0.5, vmax = 1.5, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX, y = classification_data(n_points = 500, noise = 0.6)\n\n# plot data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\nax.set_title(\"Training Data\");"
  },
  {
    "objectID": "posts/newtons-method/index.html#convergence-to-an-appropriate-decision-boundary",
    "href": "posts/newtons-method/index.html#convergence-to-an-appropriate-decision-boundary",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Convergence to an Appropriate Decision Boundary",
    "text": "Convergence to an Appropriate Decision Boundary\nTo begin evaluating the efficacy of our model, we fit logistic regression on our training data with regular gradient descent and with Newton’s method, illustrating that both options converge to the same solution.\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and gradient descent optimizer\nLR_gradient = LogisticRegression() \nopt_gradient = GradientDescentOptimizer(LR_gradient)\n\n# initialize vector to record loss values\nloss_vec_gradient = [10, 5]\n\n# fit model\nwhile loss_vec_gradient[-2] - loss_vec_gradient[-1] &gt; 0.00001:\n\n    # update model\n    opt_gradient.step(X, y, alpha = 0.07, beta = 0)\n\n    # calculate and record loss\n    loss = LR_gradient.loss(X, y) \n    loss_vec_gradient.append(loss)\n\n# remove initialization values\nloss_vec_gradient = loss_vec_gradient[2:]\n\n# reset seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and newton's optimizer\nLR_newton = LogisticRegression() \nopt_newton = NewtonOptimizer(LR_newton)\n\n# initialize vector to record loss values\nloss_vec_newton = [10, 5]\n\n# fit model\nwhile loss_vec_newton[-2] - loss_vec_newton[-1] &gt; 0.00001:\n\n    # update model\n    opt_newton.step(X, y, alpha = 10)\n\n    # calculate and record loss\n    loss = LR_newton.loss(X, y) \n    loss_vec_newton.append(loss)\n\n# remove initialization values\nloss_vec_newton = loss_vec_newton[2:]\n\nNow that we have fit our models, let’s inspect the decision boundaries in the context of our training data.\n\n# define function to draw line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\ndraw_line(LR_gradient.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"black\", label = \"Gradient Descent\")\ndraw_line(LR_newton.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"red\", label = \"Newton's Optimizer\")\nax.legend(loc = \"lower left\")\nax.set_title(\"Training Data and Decision Boundaries\");\n\n\n\n\n\n\n\n\nThe decision boundaries for the gradient descent optimizer and Newton’s optimizer are virtually identical! This result makes me confident that we have correctly implemented logistic regression with Newton’s optimizer."
  },
  {
    "objectID": "posts/newtons-method/index.html#the-speed-of-newtons-optimizer",
    "href": "posts/newtons-method/index.html#the-speed-of-newtons-optimizer",
    "title": "Newton’s Method for Logistic Regression",
    "section": "The Speed of Newton’s Optimizer",
    "text": "The Speed of Newton’s Optimizer\nIn the previous section we saw that logistic regression with gradient descent and Newton’s method converge to the same solution. Naturally, one might wonder whether both options approach the solution at the same rate. As it turns out, when an appropriate value of \\(\\alpha\\) is selected, logistic regression with Newton’s optimizer converges to a solution in far fewer iterations than logistic regression with classic gradient descent. To illustrate this statement, we plot the change in loss over the iterations of both models’ training.\n\n# plot the changes in loss \nplt.plot(loss_vec_gradient, color = \"slategrey\", lw = 3, label = \"Gradient Descent\")\nplt.plot(loss_vec_newton, color = \"#A37933\", lw = 3, label = \"Newton's Optimizer\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.legend(loc = \"upper right\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nAs we would expect, the two optimizers begin with identical loss values. However, from the very beginning, the loss values of logistic regression with Newton’s optimizer decrease more rapidly than logistic regression with gradient descent. In fact, Newton’s approach achieves an incremental improvement to the loss function of less than our threshold of \\(0.00001\\) in just over \\(200\\) iterations, whereas the gradient descent approach requires over \\(1,100\\) iterations. For this data and this choice of \\(\\alpha\\), the Newton’s optimizer approach requires dramatically fewer iterations than regular gradient descent."
  },
  {
    "objectID": "posts/newtons-method/index.html#when-alpha-is-too-large",
    "href": "posts/newtons-method/index.html#when-alpha-is-too-large",
    "title": "Newton’s Method for Logistic Regression",
    "section": "When \\(\\alpha\\) is Too Large",
    "text": "When \\(\\alpha\\) is Too Large\nThe learning rate \\(\\alpha\\) impacts how far logistic regression with Newton’s method moves the decision boundary in any given iteration. When \\(\\alpha\\) is very small, each change to the decision boundary becomes similarly small, such that it takes many iterations to converge to an adequate solution. In the example above, \\(\\alpha\\) was large enough that logistic regression with Newton’s optimizer converged to a solution more rapidly than ordinary gradient descent. But what happens when \\(\\alpha\\) is substantially larger? To answer this question, we experiment with fitting a model where \\(\\alpha = 10,000\\) in the code chunk below.\n\n# reset seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and newton's optimizer\nLR_newton_10000 = LogisticRegression() \nopt_newton_10000 = NewtonOptimizer(LR_newton_10000)\n\n# initialize vector to record loss values\nloss_vec_newton_10000 = [10, 5]\n\n# initialize counter\ncounter = 0\n\n# fit model\nwhile loss_vec_newton_10000[-2] - loss_vec_newton_10000[-1] &gt; 0.00001:\n    # update model\n    opt_newton_10000.step(X, y, alpha = 10000)\n\n    # calculate and record loss\n    loss = LR_newton_10000.loss(X, y) \n    loss_vec_newton_10000.append(loss)\n    \n    # update counter\n    counter += 1\n\n# remove initialization values\nloss_vec_newton_10000 = loss_vec_newton_10000[2:]\n\n# print results\nprint(\"Total iterations:\", counter, \"\\nResulting loss vector\", loss_vec_newton_10000)\n\nTotal iterations: 1 \nResulting loss vector [tensor(nan)]\n\n\nApparently, by increasing \\(\\alpha\\) to \\(10,000\\), we prevented the logistic regression model with Newton’s optimizer from converging to a solution. In fact, with \\(\\alpha\\) set so high, the model produces a loss value of NaN, terminating our training loop in a single iteration. This likely occured because the large value of \\(\\alpha\\) caused the decision boundary to move so far that the loss function’s value exceeded the maximum value that torch can store."
  },
  {
    "objectID": "posts/newtons-method/index.html#operation-counting",
    "href": "posts/newtons-method/index.html#operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Operation Counting",
    "text": "Operation Counting\nLet \\(C_{gd}\\) represent the total computational cost of fitting a logistic regression model with gradient descent and \\(C_{nm}\\) represent the total computational cost of fitting a logistic regression model with Newton’s method.\nSuppose that gradient descent converges to an adequate solution in \\(t_{gd}\\) iterations and Newton’s method converges to an adequate solution in \\(t_{nm}\\) iterations. Furthermore, per Professor Chodrow’s instructions, we assume that the following operations are associated with the following computational costs.\n\nComputing the loss: \\(c\\) computational units\nComputing the gradient: \\(2c\\) units\nComputing the Hessian: \\(pc\\) units\nInverting a \\(p \\times p\\) matrix: \\(k_1p^\\gamma\\) units\nPerforming the matrix-vector multiplication required by Newton’s method: \\(k_2p^2\\)\n\nIn each iteration of gradient descent, we calculate the loss and the gradient once, leading to the following total computational cost.\n\\(C_{gd} = t_{gd} (c + 2c) = t_{gd}(3c)\\)\nIn each iteration of Newton’s method, we calculate the loss, Hessian, inverse Hessian, matrix multiplication, and gradient once, leading to the following total computational cost.\n\\(C_{nm} = t_{nm}(c + pc + k_1p^\\gamma + k_2p^2 + 2c) = t_{nm}(3c + pc + k_1p^\\gamma + k_2p^2)\\)\nIf Newton’s method requires fewer computational units to complete than gradient descent, we have the following inequalities.\n\\[C_{nm} &lt; C_{gd}\\] \\[t_{nm}(3c + pc + k_1p^\\gamma + k_2p^2) &lt; t_{gd}(3c)\\] \\[\\frac{(3c + pc + k_1p^\\gamma + k_2p^2)}{3c} &lt; \\frac{t_{gd}}{t_{nm}}\\]\nThus, Newton’s method will only require less computational effort than gradient descent if the total number of iterations required to complete gradient descent \\(t_{gd}\\) is at least \\(\\frac{(3c + pc + k_1p^\\gamma + k_2p^2)}{3c}\\) times as much as the number of iterations required to complete Newton’s method.\nSince \\(p\\) is raised to the power of \\(2\\) and to the power of \\(\\gamma\\) in this formula, the ratio between \\(t_{gd}\\) and \\(t_{nm}\\) required for Newton’s method to be more efficient than gradient descent increases dramatically as \\(p\\) increases. Practically speaking, when working with datasets of many variables, this is unlikely to happen. For this reason, if working on a dataset with many variables, gradient descent will almost certainly be more efficient than Newton’s method."
  },
  {
    "objectID": "posts/newtons-method/index.html#conclusion",
    "href": "posts/newtons-method/index.html#conclusion",
    "title": "Newton’s Method for Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nCompleting this assignment gave me the opportunity to understand the strengths and weaknesses of two different optimization methods for logistic regression. By implementing and performing experiments with both gradient descent and Newton’s method, I learned that Newton’s method may converge to an adequate solution in fewer training iterations than gradient descent. By counting the number of operations required for both optimizers, I discovered that fewer iterations does not necessarily mean lower computational complexity. In fact, when working with a large number of features, gradient descent is almost certainly less computationally expensive than Newton’s method. In summary, if given the choice between gradient descent and Newton’s method, choose gradient descent!"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#abstract",
    "href": "posts/logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In this blog post, we implement logistic regression via empirical risk minimization and perform a few experiments to highlight the model’s strengths and weaknesses. As a part of our experimentation, we implement gradient descent with momentum and investigate the evolution of our loss function in comparison to ordinary gradient descent. We also test our model on data with more dimensions than observations in order to illustrate the perils of overfitting. To see my implementation of logistic regression, please visit logistic.py."
  },
  {
    "objectID": "posts/logistic-regression/index.html#generating-training-data",
    "href": "posts/logistic-regression/index.html#generating-training-data",
    "title": "Implementing Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# Import packages\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# Define function to generate data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n# define function to plot data\ndef plot_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -0.5, vmax = 1.5, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX, y = classification_data(n_points = 500, noise = 0.6)\n\n# plot data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\nax.set_title(\"Training Data\");"
  },
  {
    "objectID": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "href": "posts/logistic-regression/index.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nTo begin evaluating the efficacy of our model, we fit logistic regression on our training data with regular gradient descent. We actually only implemented gradient descent with momentum in logistic.py, but if we set \\(\\beta = 0\\), as we do below, this is equivalent to regular gradient descent.\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_vanilla = []\n\n# fit model\nwhile len(loss_vec_vanilla) &lt; 2 or loss_vec_vanilla[-2] - loss_vec_vanilla[-1] &gt; 0.00001:\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_vanilla.append(loss)\n\nNow that we have fit our model, let’s inspect our decision boundary in the context of our training data.\n\n# define function to draw line\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# plot decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_data(X, y, ax)\ndraw_line(LR.w, x_min = -1.25, x_max = 2.25, ax = ax, color = \"black\")\nax.set_title(\"Training Data and Decision Boundary\");\n\n\n\n\n\n\n\n\nVisually, this line appears to be an intelligent choice. There are some misclassified points on either side of the line, but our data is not linearly separable, so it is impossible to correctly classify all points using a linear decision boundary. This visual check leads me to believe that our classifier is performing well. As another check, we plot the evolution of our loss function below to verify that it decreases monotonically.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this graph, every iteration has a lower loss value than the iteration that precedes it. In other words, our loss function decreases monotonically, as guaranteed by gradient descent.\n\nThe Benefits of Momentum\nOur model appears to be behaving as expected, leading us to the natural question: can we do better if we use gradient descent with momentum? More specifically, can the model converge to an appropriate weight vector in fewer iterations under gradient descent with momentum? To address this question, we fit a logistic regression model using the same values for \\(\\alpha\\) and our random seed, but with the modification that \\(\\beta = 0.9\\) rather than \\(0\\).\n\n# set seed\ntorch.manual_seed(1234)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_momentum = []\n\n# fit model\nwhile len(loss_vec_momentum) &lt; 2 or loss_vec_momentum[-2] - loss_vec_momentum[-1] &gt; 0.00001:\n\n    # update model\n    opt.step(X, y, alpha = 0.07, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X, y) \n    loss_vec_momentum.append(loss)\n\nTo identify any improvements due to gradient descent with momentum, we plot the evolution of the loss function under both scenarios on the same graph.\n\n# plot the changes in loss \nplt.plot(loss_vec_vanilla, color = \"slategrey\", label = \"Gradient Descent\")\nplt.plot(loss_vec_momentum, color = \"#A37933\", label = \"Gradient Descent with Momentum\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\nplt.legend(loc = \"lower left\");\n\n\n\n\n\n\n\n\nIn this scenario, the loss function of gradient descent with momentum decreased slightly more rapidly than regular gradient descent. To be fully transparent, producing a scenario with this improvement involved some fishing for appropriate data and \\(\\alpha\\). In this case, gradient descent with momentum performed slightly better than regular gradient descent, but this is by no means a guarantee.\n\n\nThe Perils of Overfitting\nIn this section, we construct a scenario where logistic regression overfits to the training data. To accomplish this task, we fit a model on data with substantially more dimensions than observations. Specifically, we generate training data and test data which both contain 30 dimensions and 20 observations, using a function for generating classification data generously provided by Professor Chodrow. Then we fit our logistic regression model on our training data, resulting in the evolution of our loss function as illustrated below.\n\n# set seed\ntorch.manual_seed(1234)\n\n# generate data\nX_train, y_train = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\nX_test, y_test = classification_data(n_points = 20, noise = 0.5, p_dims = 30)\n\n# initialize logistic regression model and optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize vector to record loss values\nloss_vec_overfit = []\n\n# fit model\nfor _ in range(100):\n\n    # update model\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\n    # calculate and record loss\n    loss = LR.loss(X_train, y_train) \n    loss_vec_overfit.append(loss)\n    if loss == 0:\n        break\n\n# plot the changes in loss \nplt.plot(loss_vec_overfit, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\");\n\n\n\n\n\n\n\n\nIn this figure, we find that our loss function decreased in the same monotonic manner as before, converging to a decision boundary with a loss value of less than 0.1. Below, we calculate the training accuracy of our model.\n\n# Compute training accuracy\ny_hat = LR.predict(X_train)\ntrain_accuracy = (1.0*(y_hat == y_train)).mean().item()\ntrain_accuracy\n\n1.0\n\n\nAt 100% training accuracy, our model is correctly predicting the outcome variable of every observation in our training data. But what about testing accuracy?\n\n# Compute testing accuracy\ny_hat = LR.predict(X_test)\ntest_accuracy = (1.0*(y_hat == y_test)).mean().item()\ntest_accuracy\n\n0.6499999761581421\n\n\nAt roughly 65%, our testing accuracy is not nearly as high as our training accuracy. Our model appears to have been overfit to our training data, failing to generalize to test data that was generated in a similar manner. Why might this have happened?\nI suspect that 20 observations of training points is simply insufficient for training a model in 30 dimensional space. There are so many possibilities for where a point can be located in 30 dimensional space, and 20 observations barely scratches the surface of these possibilities. Our model would need substantially more training data in order to have been exposed to enough of these possibilities. In the absence of sufficient training data, our model reflects the noise of our training data rather than the underlying pattern."
  },
  {
    "objectID": "posts/logistic-regression/index.html#conclusion",
    "href": "posts/logistic-regression/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, we implemented logistic regression via empirical risk minimization within the object-oriented framework provided by Professor Chodrow. We investigated the differences between regular gradient descent and gradient descent with momentum, discovering that the latter option converges to a solution more rapidly in some cases. We also illustrated that logistic regression tends to overfit when the training data has more dimensions than observations. Implementing logistic regression and experimenting with the model’s parameters furthered my understanding of logistic regression in particular and gradient descent more broadly."
  },
  {
    "objectID": "posts/stat-learning-project/index.html",
    "href": "posts/stat-learning-project/index.html",
    "title": "Fuzzy C-Means Clustering",
    "section": "",
    "text": "Visit Project Web Page"
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html",
    "href": "posts/sparse-kernel-machines/index.html",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "",
    "text": "In this blog post, we extend our previous implementation of standard logistic regression to a kernelized setting, allowing our model to detect nonlinear decision boundaries. Specifically, we implement a sparse kernel machine using the \\(\\ell_1\\) regulizer to ensure that the vast majority of \\(\\mathbf{a}\\)’s entries are indistinguishable from zero. After implementing sparse kernelized logistic regression, we perform a few experiments to investigate the properties of the algorithm. We test different values of \\(\\lambda\\) to investigate how this parameter impacts the number of points indistinguishable from zero, we test different values of \\(\\gamma\\) to reveal how this parameter impacts the wiggliness of our decision boundary, and we test the model on nonlinear data from scikit-learn’s make_moons() function to illustrate that the model is robust to nonlinear patterns. Finally, we fit a model with a large \\(\\gamma\\)-value and compute ROC curves to reveal how this parameter may lead to overfitting. To see my implementation of sparse kernelized logistic regression, please visit sparse_kernel_logistic.py."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#abstract",
    "href": "posts/sparse-kernel-machines/index.html#abstract",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "",
    "text": "In this blog post, we extend our previous implementation of standard logistic regression to a kernelized setting, allowing our model to detect nonlinear decision boundaries. Specifically, we implement a sparse kernel machine using the \\(\\ell_1\\) regulizer to ensure that the vast majority of \\(\\mathbf{a}\\)’s entries are indistinguishable from zero. After implementing sparse kernelized logistic regression, we perform a few experiments to investigate the properties of the algorithm. We test different values of \\(\\lambda\\) to investigate how this parameter impacts the number of points indistinguishable from zero, we test different values of \\(\\gamma\\) to reveal how this parameter impacts the wiggliness of our decision boundary, and we test the model on nonlinear data from scikit-learn’s make_moons() function to illustrate that the model is robust to nonlinear patterns. Finally, we fit a model with a large \\(\\gamma\\)-value and compute ROC curves to reveal how this parameter may lead to overfitting. To see my implementation of sparse kernelized logistic regression, please visit sparse_kernel_logistic.py."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#generating-training-data",
    "href": "posts/sparse-kernel-machines/index.html#generating-training-data",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Generating Training Data",
    "text": "Generating Training Data\nFirst, we import the packages we will need for this assignment.\n\n# import packages\n%load_ext autoreload\n%autoreload 2\nfrom sparse_kernel_logistic import KernelLogisticRegression, GradientDescentOptimizer\nimport torch \nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_moons\nimport numpy as np\nplt.style.use('seaborn-v0_8-whitegrid')\n\nNext, we generate the data that we will use to train our model. Thank you to Professor Chodrow for providing the functions for generating and visualizing training data.\n\n# define function for creating classification data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n# define function for plotting classification data\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# set seed\ntorch.manual_seed(1)\n\n# create classification data\nX, y = classification_data(n_points = 100, noise = 0.4)\n\n# plot classification data\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Training Data\");"
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#fitting-our-first-model",
    "href": "posts/sparse-kernel-machines/index.html#fitting-our-first-model",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Fitting Our First Model",
    "text": "Fitting Our First Model\nWith our training data in hand and our implementation of kernelized logistic regression in sparse_kernel_logistic.py, we are ready to fit our first model! In our first model, we are paying particular attention to whether our output matches our expected output, hoping to confirm that our implementation is adequate and bug-free.\nIn this model and for the remainder of this blog post, we will use the Gaussian radial basis function (RBF) kernel.\n\n# define kernel\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .1, gamma = 0.1)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\nIn sparse kernelized logistic regression, the \\(\\ell_1\\) norm is used to make the model sparse – that is, to make most entries of \\(\\mathbf{a}\\) equal to zero. Let us confirm that must of our entries for \\(\\mathbf{a}\\) are indeed close to zero.\n\n# compute proportion of coefficients distinguishable from zero\n(1.0*(torch.abs(KR.a) &gt; 0.001)).mean()\n\ntensor(0.0500)\n\n\nSuccess! Finally, let us confirm that our model is scoring points appropriately. We will not actually implement a decision threshold in this blog post, but the important point here is whether the scores of one class tend to be different from the scores of the other class.\n\n# plot results using Professor Chodrow's provided code\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nIn our output, there is a dark brown ring around most of the brown points in the lower left side of the plot. Expanding outwards, each subsequent concentric ring is less and less likely to contain brown points. These rings correspond to scores of that region of the graph, so you can imagine the model picking an appropriate score cutoff that serves as a pretty accurate decision threshold."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#basic-experiments",
    "href": "posts/sparse-kernel-machines/index.html#basic-experiments",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Basic Experiments",
    "text": "Basic Experiments\nIn this section, we tinker with our parameter values in order to see how they impact the output of our model.\n\nExperiment 1: Adjusting \\(\\lambda\\)\nIn our first experiment, we leave all of the parameters the same as our initial model except for \\(\\lambda\\), which we increase from \\(0.1\\) to \\(0.11\\). Recall that \\(\\lambda\\) is the coefficient of the \\(\\ell_1\\) regulizer. As the \\(\\ell_1\\) regulizer is responsible for making \\(\\mathbf{a}\\) sparse, increasing the weight of this regularization term will have the effect of making \\(\\mathbf{a}\\) even more sparse.\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .11, gamma = 0.1)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\nNow that we have trained our model, let us determine how many entries of \\(\\mathbf{a}\\) are distinguishable from zero. As before, we define distinguishable from zero to mean that a coefficient’s magnitude is greater than \\(0.001\\).\n\n# calculate total points distinguishable from zero\n(1.0*(torch.abs(KR.a) &gt; 0.001)).sum()\n\ntensor(1.)\n\n\nIn this case, by increasing \\(\\lambda\\) from \\(0.1\\) to just \\(0.11\\), our model went from having \\(5\\) points with weights distinguishable from zero to having only \\(1\\).\n\n\nExperiment 2: Adjusting \\(\\gamma\\)\nIn this experiment, we use the same parameters from our initial model except for \\(\\gamma\\), which controls the bandwidth of our RBF kernel. According to scikit-learn, this means that \\(\\gamma\\) controls how far the influence of a training point reaches. Smaller \\(\\gamma\\)-values give points influence over more space, while larger \\(\\gamma\\)-values give points influence over less space. In our first example, the shape of our score contours was very smooth. When points have influence over a small amount of space, the model becomes more sensitive to individual points, so I suspect that a larger \\(\\gamma\\) value will result in wigglier score contours. To see whether this is true, I increase \\(\\gamma\\) from \\(0.1\\) to \\(3\\) below.\n\n# set seed\ntorch.manual_seed(1)\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(X, rbf_kernel, lam = .1, gamma = 3)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X, y, alpha = 0.0001)\n\n\n# plot results using Professor Chodrow's provided code\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nAs I anticipated, the score contours are less smooth and less circular. In the lower left, rather than one set of concentric rings, we now have two sets. On the right hand side, we now witness some sharper curves than those present in the concentric rings from our first model. In this manner, our results confirm our expectations that larger values of \\(\\gamma\\) result in wigglier decision boundaries.\n\n\nExperiment 3: Detecting Nonlinear Patterns\nFirst, let us use scikit-learn’s make_moons() function to generate data with a nonlinear pattern.\n\n# set seed\ntorch.manual_seed(1)\n\n# Create nonlinear training data\nmoons_X, moons_y = make_moons(n_samples=100, noise = 0.1)\n\n# Convert to torch tensors\nmoons_X = torch.tensor(moons_X, dtype = torch.float32)\nmoons_y = torch.tensor(moons_y, dtype = torch.float32)\n\n# plot data\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(moons_X, moons_y, ax)\nplt.title(\"Training Data\");\n\n\n\n\n\n\n\n\nWhile the pattern of this data is clear to the human eye, it is also clear that the curve separating the two classes of data is not a straight line. Let us see whether our kernelized logistic regression model can detect this difference.\n\n# create kernel logistic regression model\nKR = KernelLogisticRegression(moons_X, rbf_kernel, lam = .075, gamma = 4)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(200000):\n    # update model\n    opt.step(moons_X, moons_y, alpha = 0.0001)\n\n\n# show results\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(moons_X[:,0].min() - 0.2, moons_X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(moons_X[:,1].min() - 0.2, moons_X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(moons_X, moons_y, ax)\nplt.scatter(moons_X[ix, 0],moons_X[ix, 1], facecolors = \"none\", edgecolors = \"black\");\n\n\n\n\n\n\n\n\nIt took me some time to discover parameter values that resulted in a relatively well-fit model, but with some tuning, it appears that kernelized logistic regression can indeed find nonlinear patterns in data such as the one above. The model has not performed perfectly, but on the top left there is a curve that generally follows the shape of the upper moon, and on the bottom right there is a curve that generally follows the shape of the lower moon. The most difficult region for the model to predict the correct label appears to be where one moon almost intersects the center of the other moon. While it is not perfect, I am overall impressed by how well the model has performed. The fact that kernelized logistic regression can find nonlinear patterns represents a substantial improvement over traditional logistic regression."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#overfitting",
    "href": "posts/sparse-kernel-machines/index.html#overfitting",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nIn this section, we intentionally overfit a model with a poor choice of \\(\\gamma\\) in order to demonstrate the potential issues of overfitting. First, we generate training and testing data using the same function.\n\n# set seed\ntorch.manual_seed(1)\n\n# generate training data\nX_train, y_train = classification_data(n_points = 100, noise = 0.6)\n\n# generate testing data\nX_test, y_test = classification_data(n_points = 100, noise = 0.6)\n\n# show training and testing data side by side\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Testing Data\");\n\n\n\n\n\n\n\n\nBecause we used Professor Chodrow’s classification_data() function for generating both datasets, we know that the training and testing data have the same underlying pattern. By inspecting the graphs above, it is obvious that one class tends to be located in the lower left of the graph, while the other class tends to be located in the upper right. By introducing noise into the generation of both datasets, we ensured that the two datasets are not identical. Now that we have our data, we train our model and inspect its score contours.\n\n# train model\nKR = KernelLogisticRegression(X_train, rbf_kernel, lam = .065, gamma = 50)\nopt = GradientDescentOptimizer(KR)\n\n# fit model\nfor i in range(100000):\n    # update model\n    opt.step(X_train, y_train, alpha = 0.0001)\n    \n# show results\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X_train[:,0].min() - 0.2, X_train[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X_train[:,1].min() - 0.2, X_train[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1,1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG_r\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X_train, y_train, ax)\nax.set_title(\"Training Data and Score Contours\");\n\n\n\n\n\n\n\n\nVisually, the score contours appear to reflect the individual data points relatively well. The background is blue, with a region of brown surrounding groups of and occasionally individual brown points. However, the underlying pattern used to generate this data is a roughly linear boundary between the upper right and lower left. These somewhat wiggly contours may reflect the noise of our training data more than the overall pattern in the data.\nLet us evaluate the performance of our model on our training data and testing data by computing ROC curves. Thank you to Professor Chodrow for providing some code for creating ROC curves in his lecture notes.\n\n# roc curve training data\n\n# store min and max scores\n    # train\ns_train = KR.score(X_train)\nscore_min_train = s_train.min()\nscore_max_train = s_train.max()\n    # test\ns_test = KR.score(X_test)\nscore_min_test = s_test.min()\nscore_max_test = s_test.max()\n\n# create vectors for TPR and FPR\nnum_thresholds = 101\n    #train\nFPR_train = np.zeros(num_thresholds)\nTPR_train = np.zeros(num_thresholds)\nT_train = np.linspace(score_min_train, score_max_train, num_thresholds)\n    # test\nFPR_test = np.zeros(num_thresholds)\nTPR_test = np.zeros(num_thresholds)\nT_test = np.linspace(score_min_test, score_max_test, num_thresholds)\n\n# calculate TPR and TNR\n    # train\nfor i in range(num_thresholds):\n    t = T_train[i]\n    preds    = s_train &lt;= t\n    FPR_train[i]   = ((preds == 1) & (y_train == 0)).sum() / (y_train == 0).sum()\n    TPR_train[i]   = ((preds == 1) & (y_train == 1)).sum() / (y_train == 1).sum()\n    # test\nfor i in range(num_thresholds):\n    t = T_test[i]\n    preds    = s_test &lt;= t\n    FPR_test[i]   = ((preds == 1) & (y_test == 0)).sum() / (y_test == 0).sum()\n    TPR_test[i]   = ((preds == 1) & (y_test == 1)).sum() / (y_test == 1).sum()\n\n# create figure\nfig, ax = plt.subplots(1, 1)\n\nax.plot(FPR_train, TPR_train, color = \"black\", label = \"Training ROC\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.plot(FPR_test, TPR_test, color = \"red\", label = \"Testing ROC\")\nax.set_aspect('equal')\nplt.legend(loc = \"lower right\")\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nRecall that a perfect ROC curve reaches the top left corner of this graph with a false positive rate of 0 and a true positive rate of 1. Because data is rarely perfect, models rarely achieve this goal. Rather, the effectiveness of a model is represented by how close a model comes to achieving that. In our case, the training ROC curve is substantially above and to the left of the testing ROC curve, indicating that our model does not generalize well to the testing data. In fact, the distance between the training and testing ROC curves is roughly the same as the distance between the testing curve and the diagonal line that represents a model that randomly guesses at the classes. In this manner, it is clear that our model performs dramatically better on our training data than on our testing data. Because \\(\\gamma\\) is so large, the score contours reflect the noise in our data more than the underlying pattern of our data, leading our model to ineffectively generalize to new data."
  },
  {
    "objectID": "posts/sparse-kernel-machines/index.html#conclusion",
    "href": "posts/sparse-kernel-machines/index.html#conclusion",
    "title": "Sparse Kernelized Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we extended our work with logistic regression to a new situation: nonlinearity. In our prior work, we implemented standard logistic regression and performed experiments with several optimizers, including standard gradient descent, gradient descent with momentum, and Newton’s method. In this assignment, rather than searching for more efficient optimization algorithms, we implemented a kernelized model that can find a wide array of nonlinear patterns. We performed several experiments in order to deepen our understanding of the model, discovering that larger values of \\(\\lambda\\) result in fewer training points with non-zero coefficients, larger values of \\(\\gamma\\) result in wigglier decision boundaries, and these large \\(\\gamma\\) values can often lead to overfitting. We also illustrated that our model can find nonlinear decision boundaries by fitting our model on data created with scikit-learn’s make_moons() function. Overall, this assignment allowed me to learn the theory behind and implementation of an intruiging variation of an otherwise familiar machine learning algorithm."
  },
  {
    "objectID": "posts/math-thesis/index.html",
    "href": "posts/math-thesis/index.html",
    "title": "Mathematics Thesis",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "posts/local-vs-global-classification/index.html",
    "href": "posts/local-vs-global-classification/index.html",
    "title": "Local Versus Global Classification",
    "section": "",
    "text": "In my last post, I used Hansen’s Global Forest Change dataset to identify deforestation in Malawi. While global datasets may be convenient and comprehensive, they often overlook local circumstances. In this post, I identify tree cover in a study site in Cameroon by training my own random forest classifier. I then compare my results with Hansen’s, illustrating the value of training classifiers locally."
  },
  {
    "objectID": "posts/local-vs-global-classification/index.html#introduction",
    "href": "posts/local-vs-global-classification/index.html#introduction",
    "title": "Local Versus Global Classification",
    "section": "",
    "text": "In my last post, I used Hansen’s Global Forest Change dataset to identify deforestation in Malawi. While global datasets may be convenient and comprehensive, they often overlook local circumstances. In this post, I identify tree cover in a study site in Cameroon by training my own random forest classifier. I then compare my results with Hansen’s, illustrating the value of training classifiers locally."
  },
  {
    "objectID": "posts/local-vs-global-classification/index.html#analysis",
    "href": "posts/local-vs-global-classification/index.html#analysis",
    "title": "Local Versus Global Classification",
    "section": "Analysis",
    "text": "Analysis\nI first created a class schema of 6 land cover classes: forest cover, bare ground, water, cropland, shrubs, and other. I selected these classes because they were each prevalent in the landscape and including them helped distinguish between trees and other features. I then selected about 45 training points in each class and fit a random forest classifier.\nTo determine the quality of my random forest classifier, I created two maps of my study area. The first map symbolizes the pixels which my random forest algorithm identifies as trees in white and outlines my study area in black. The second map displays Google’s base satellite imagery for the purposes of comparison with the ground truth.\n\n\n\nFigure 1. Locations of trees in white (left) compared with true color image (right) in my study site in Cameroon. See the code used to create this map here.\n\n\nWithin my study site, my model performs relatively well, but far from perfectly. Feel free to Zoom in on specific clusters of trees using the Google Earth Engine link above. You will notice that some pixels are misclassified as trees and some pixels are failed to be classified as trees. These misclassifications are relatively common, especially along the rivers, along the edges of forested area, and in other sparsely forested areas. That said, where forests are dense my model performs very well, correctly classifying the bulk of each forested area. This pattern can be clearly seen in figure 1. Please note that my study area is but a fraction of the scene I worked with. Other regions in my scene perform far worse, with large swaths of water and fields misclassified as forests. Feel free to explore the rest of my scene at the link to my code above.\nAfter classifying landcover in my study region, I imported Hansen’s Global Forest Change dataset to compare my tree cover classifier with Hansen’s. A map illustrating the agreement and disagreement between our two methodologies, where I define forests in Hansen’s dataset as pixels with greater than 30% tree cover, is illustrated in Figure 2.\n\n\n\nFigure 2. Agreement and disagreement between my tree cover classifier and Hansen’s. See the code used to create this map here.\n\n\nSurprisingly, Hansen’s dataset does not identify any trees in the study region when I apply a standard definition of forest as at least 30% tree cover. Thus, figure 2 displays the pixels my classifier identifies as trees in blue and all other pixels in grey. This map makes it blatantly obvious that, at least at the 30% threshold, my classifier performs far better than Hansen’s. This makes sense to me. I trained my classifier specifically using the conditions present in my study area, while Hansen’s dataset attempts to apply one classifier to the entire world. Naturally, a local classifier should perform well where it was trained, as the global classifier does not account for the individuality of different regions."
  },
  {
    "objectID": "posts/local-vs-global-classification/index.html#quantifying-performance-of-picking-a-better-threshold-for-the-hansen-dataset",
    "href": "posts/local-vs-global-classification/index.html#quantifying-performance-of-picking-a-better-threshold-for-the-hansen-dataset",
    "title": "Local Versus Global Classification",
    "section": "Quantifying Performance of & Picking a Better Threshold for the Hansen Dataset",
    "text": "Quantifying Performance of & Picking a Better Threshold for the Hansen Dataset\nTo quantify the performance of Hansen’s data in my study site, I used the following accuracy metrics using my classifier as the ground truth.\n\nTrue Positive Rate (TPR) = (# true positives)/(# true positives + # false negatives)\nTrue Negative Rate (TNR) = (# true negatives)/(# true negatives + # false positives)\nBalanced accuracy rate (BAR) = (TPR + TNR)/2\n\nI calculated these metrics for eight different thresholds of percent tree cover, and my findings are summarized in the following table.\n\nTable 1. Accuracy assessment between Hansen’s classification and my classification, where my model is considered to be the truth. I calculated the entries in this table using figures reported in charts in my code on Google Earth Engine. Please see here for those charts and here for the functions called when making those charts.\n\n\nThreshold\nTrue Positive Rate\nTrue Negative Rate\nBalanced Accuracy Rate\n\n\n\n\n0%\n1\n0\n0.5\n\n\n1%\n0.929\n0.720\n0.824\n\n\n2%\n0.654\n0.977\n0.815\n\n\n5%\n0.640\n0.978\n0.809\n\n\n10%\n0.115\n~1\n0.558\n\n\n20%\n~0\n~1\n~0.5\n\n\n30%\n0\n1\n0.5\n\n\n50%\n0\n1\n0.5\n\n\n\nAs the table shows, the highest TPR and BAR occur at a threshold of 1%. When the threshold is increased to just 2%, the TNR becomes very close to 1 but the TPR drops by almost 0.3, resulting in a slightly lower BAR. This pattern continues as the threshold is increased to higher percentages. I would experiment with thresholds between 1% and 2% in order to find a higher BAR, but unfortunately, this is not possible because Hansen’s treeCover band is reported as an integer. The best-performing percentage of tree cover in Hansen’s dataset is the 1% threshold."
  },
  {
    "objectID": "posts/local-vs-global-classification/index.html#conclusion",
    "href": "posts/local-vs-global-classification/index.html#conclusion",
    "title": "Local Versus Global Classification",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, these findings indicate that Hansen’s classifier performs so poorly in this part of Cameroon that the lowest possible indication of tree cover, 1%, performs the best. This just goes to show that massive world-scale models are not well-suited for localized studies. When working in a specific region, we tend to witness the highest performance when our models are trained in the region of our study."
  },
  {
    "objectID": "posts/green-space/index.html",
    "href": "posts/green-space/index.html",
    "title": "Green Space and Heat Anomalies",
    "section": "",
    "text": "Visit Project Web App"
  },
  {
    "objectID": "posts/flood-vulnerability/index.html",
    "href": "posts/flood-vulnerability/index.html",
    "title": "Flood Risk and Food Security",
    "section": "",
    "text": "This analysis assessed the extent to which residences in Dar Es Salaam, Tanzania, would lose access to food sources in the case of a severe flooding event. Specifically, I conducted a distance analysis on PostGIS to determine how much the distance from every residence in Dar Es Salaam to the nearest grocery vendor would increase between an everyday scenario and a severe flood scenario. The results revealed that flooding has minimal impact on food accessibility, except for in a few small regions. Of the 1,292,622 residences in Dar Es Salaam, only 1,317 of them would be over 500 meters further from a non-flooded grocery vendor in the event of severe flooding."
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#abstract",
    "href": "posts/flood-vulnerability/index.html#abstract",
    "title": "Flood Risk and Food Security",
    "section": "",
    "text": "This analysis assessed the extent to which residences in Dar Es Salaam, Tanzania, would lose access to food sources in the case of a severe flooding event. Specifically, I conducted a distance analysis on PostGIS to determine how much the distance from every residence in Dar Es Salaam to the nearest grocery vendor would increase between an everyday scenario and a severe flood scenario. The results revealed that flooding has minimal impact on food accessibility, except for in a few small regions. Of the 1,292,622 residences in Dar Es Salaam, only 1,317 of them would be over 500 meters further from a non-flooded grocery vendor in the event of severe flooding."
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#important-links",
    "href": "posts/flood-vulnerability/index.html#important-links",
    "title": "Flood Risk and Food Security",
    "section": "Important Links",
    "text": "Important Links\n\nResearch Compendium\nInteractive Map of my Results in Dar Es Salaam"
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#study-design",
    "href": "posts/flood-vulnerability/index.html#study-design",
    "title": "Flood Risk and Food Security",
    "section": "Study design",
    "text": "Study design\nThe following analysis is an exploratory study addressing the impacts of extreme flooding on food accessibility in Dar Es Salaam, Tanzania. The spatial extent of the study is the city of Dar Es Salaam, Tanzania. I conducted this analysis on a household-level spatial scale, and I aggregated this information to the wards level to summarize my results. The analysis was conducted based on flood projections and Open Street Map data as of October, 2021. Thus, the temporal extent of the study is limited to 2021, or more realistically, when the features were digitized in OSM up until 2021."
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#materials-and-procedure",
    "href": "posts/flood-vulnerability/index.html#materials-and-procedure",
    "title": "Flood Risk and Food Security",
    "section": "Materials and procedure",
    "text": "Materials and procedure\n\nData and variables\nOpenStreetMap is a worldwide open source mapping project, which served as the source of the planet_osm_point and planet_osm_polygon layers used in this analysis. I found the locations of all of the residences and grocery vendors in Dar Es Salaam using this data source.\nResilience Academy is an organization which brings several universities in Tanzania together to educate youth about mapping, develop community maps, and assess climate risks. The Resilience academy served as the source of my wards and flood_scenario layers, which provided information about the location and geometry of governmental subdivisions and potential floods in Dar Es Salaam.\n\n\nGeographic characteristics\nThis analysis is based on the locations of wards, grocery vendors and residences in Dar Es Salaam, as well as the projections of where flooding may occur. The locations of grocery vendors and residences are open data taken from OpenStreetMap. As such, the locations have been added over the past couple of years, so they may not all be up to date. Additionally, they were added to the OpenStreetMap database by a variety of users, and may not be entirely accurate. The locations of different wards are defined by the government and unlikely to have much error. It is difficult to discern the accuracy of the flood_scenarios, as they are exactly as accurate and precise as the data providers, Resilience Academy, could make them. Resilience Academy generated the layer using reports of historic floods, so its accuracy depends on the quality of elevation models and the accurate memory of those reporting past flood events. I used a coordinate reference system of ESPG: 32737, and the spatial extent of the analysis is the land area of the city of Dar Es Salaam.\n\n\nData transformations\nIn order to perform an analysis on the accessibility of grocery vendors to residences in flooded and not-flooded scenarios, I first had to manipulate the given data to make such comparison possible. The first step was to identify the points and polygons in the OpenStreetMap database that represented food vendors, which I accomplished with the following code:\nCREATE TABLE foodpoints2 AS\nSELECT name, amenity, shop, osm_id, way\nFROM planet_osm_point\nWHERE amenity = 'marketplace' OR shop IN ('convenience', 'supermarket', 'kiosk', 'bakery', 'butcher', 'greengrocer', 'pastry');\n\nCREATE TABLE foodpolygons AS\nSELECT name, amenity, shop, osm_id, way\nFROM planet_osm_polygon\nWHERE amenity = 'marketplace' OR shop IN ('convenience', 'supermarket', 'kiosk', 'bakery', 'butcher', 'greengrocer', 'pastry');\nThese queries generated two separate tables of the grocery vendors in Dar Es Salaam: one where these sites were recorded as points and one where they were recorded as polygons. In order to convert the polygon features to point features and combine all of the information into one data table, I used the following query:\nCREATE TABLE foodsources AS\nSELECT name, osm_id, st_transform(way, 32737)::geometry(point, 32737) AS geom FROM foodpoints2\nUNION\nSELECT name, osm_id, st_transform(st_centroid(way), 32737)::geometry(point,32737) AS geom FROM foodpolygons;\nI conducted three analogous queries to extract the residences from the OSM data:\nCREATE TABLE residencepoints2 AS\nSELECT building, osm_id, way\nFROM planet_osm_point\nWHERE building IN ('residential', 'yes');\n\nCREATE TABLE residencepolygons AS\nSELECT building, osm_id, way\nFROM planet_osm_polygon\nWHERE building IN ('residential', 'yes');\n\nCREATE TABLE residences AS\nSELECT osm_id, st_transform(way, 32737)::geometry(point, 32737) AS geom FROM residencepoints2\nUNION\nSELECT osm_id, st_transform(st_centroid(way), 32737)::geometry(point,32737) AS geom FROM residencepolygons;\nLater on in my analysis, it will be helpful to know which ward each residence is located within. I joined the ward information to my table of residences as follows.\nALTER TABLE residences\nADD COLUMN ward text;\n\nUPDATE residences\nSET ward = wards.ward_name\nFROM wards\nWHERE st_contains(wards.utmgeom, residences.geom);\nI also needed to perform a few operations in order to prepare my flood scenario data for comparison with grocery vendors. Specifically, I needed to fix the geometry of the flood layer and dissolve the polygons into one feature.\nUPDATE flood\nSET geom = st_makevalid(geom);\n\nCREATE TABLE flooddissolve AS\nSELECT st_union(geom)::geometry(multipolygon,32737) as geom\nFROM flood;\n\n\nAnalysis\nWith the appropriate data transformations complete, I was ready to conduct the spatial analysis relevant to my question. I began by calculating the distance from every residence to the nearest grocery store in a not flooded scenario.\nCREATE TABLE foodaccess AS\nSELECT residences.*, st_distance(a.foodsourcesgeom, residences.geom) AS dist\nFROM residences CROSS JOIN lateral (\n    SELECT foodsources.geom AS foodsourcesgeom\n    FROM foodsources\n    ORDER BY foodsources.geom &lt;-&gt; residences.geom\n    LIMIT 1) a;\nIn order to do the same for a flooded scenario, I first needed to find the food sources that were located within the flood zones and create a table involving the food source that were NOT flooded.\nCREATE TABLE foodflood AS\nSELECT foodsources .*, st_multi(st_intersection(foodsources.geom, flooddissolve.geom))::geometry(multipoint, 32737) as geom2\nFROM foodsources INNER JOIN flooddissolve\nON st_intersects(foodsources.geom, flooddissolve.geom);\n\nALTER TABLE foodflood\nDROP COLUMN geom;\n\nCREATE TABLE alt_foodnotflood AS\nSELECT *\nFROM foodsources\nWHERE osm_id NOT IN (SELECT osm_id FROM foodflood);\nWith that information in hand, I was prepared to calculate the distance from every residence to the nearest grocery vendor in the event that some grocers were flooded by a severe storm.\nCREATE TABLE foodaccess_flood AS\nSELECT residences.*, st_distance(a.alt_foodnotfloodgeom, residences.geom) AS dist\nFROM residences CROSS JOIN lateral (\n    SELECT alt_foodnotflood.geom AS alt_foodnotfloodgeom\n    FROM alt_foodnotflood\n    ORDER BY alt_foodnotflood.geom &lt;-&gt; residences.geom\n    LIMIT 1) a;\nIn order to summarize the effects of flooding on food access in each ward, I grouped the food access tables for both scenarios by ward and calculated the average distance to the nearest grocery store from the individual distances. I then created a new table which illustrates the change in average distance to the nearest grocery vendor for each ward.\nCREATE TABLE foodaccess_wards AS\nSELECT ward, avg(dist)\nFROM foodaccess\nGROUP BY ward;\n\nCREATE TABLE foodaccess_flood_wards AS\nSELECT ward, avg(dist)\nFROM foodaccess_flood\nGROUP BY ward;\n\nCREATE TABLE change_in_access_wards AS\nSELECT foodaccess_flood_wards.ward, foodaccess_flood_wards.avg AS flood_avg_dist, foodaccess_wards.avg AS normal_avg_dist, foodaccess_flood_wards.avg - foodaccess_wards.avg AS change_avg_dist\nFROM foodaccess_flood_wards LEFT JOIN foodaccess_wards\nON foodaccess_flood_wards.ward = foodaccess_wards.ward;\nIn order to map this information, I needed the geometry of the wards! For this reason, I joined the change in average distance from change_in_access_wards back to the wards table.\nALTER TABLE wards\nADD COLUMN change_avg_dist REAL;\n\nUPDATE wards\nSET change_avg_dist = change_in_access_wards.change_avg_dist\nFROM change_in_access_wards\nWHERE wards.ward_name = change_in_access_wards.ward;\nUpon visualizing this information, I realized that my results would be better illustrated if I included the locations of the most impacted residences. For this reason, I calculated the change in distance for every residence in Dar Es Salaam and extracted just the residences where the distance increased by over 500 meters.\nCREATE TABLE change_in_access AS\nSELECT foodaccess_flood.osm_id, foodaccess_flood.geom, foodaccess_flood.dist AS flood_dist, foodaccess.dist AS normal_dist, foodaccess_flood.dist - foodaccess.dist AS change_dist\nFROM foodaccess_flood LEFT JOIN foodaccess\nON foodaccess_flood.osm_id = foodaccess.osm_id;\n\nCREATE TABLE meters_500 AS\nSELECT *\nFROM change_in_access\nWHERE change_dist &gt; 500;"
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#results",
    "href": "posts/flood-vulnerability/index.html#results",
    "title": "Flood Risk and Food Security",
    "section": "Results",
    "text": "Results\nMy analysis found the average distance from residences in each ward to the nearest grocery vendor in both flooded and not flooded scenarios, and then calculated the change in average distance between the two scenarios. The average change in distance from residences to grocery vendors in the event of a severe storm is summarized in the table below. I include only the top wards for brevity – please refer to the report in my research compendium for all of the details.\n\n\n\nWard Name\nAverage Change in Distance (meters)\n\n\n\n\nMikocheni\n131.58932\n\n\nMsasani\n64.51961\n\n\nTandale\n37.49041\n\n\nKunduchi\n36.18561\n\n\nKawe\n31.273449\n\n\nMwananyamala\n27.822485\n\n\nMchikichini\n23.75622\n\n\nKigogo\n15.701152\n\n\nKijitonyama\n12.482358\n\n\nVingunguti\n12.239834\n\n\n\nAs the table reveals, the average distance in each ward increases by over 25 meters in only 6 wards. Furthermore, the meters_500 table wound up with a feature count of just 1,317. This means that of the 1,292,704 residences in Dar Es Salaam, only 1,317 of them experience an increase in the distance to the nearest grocery store of over 500 meters in the case of an extreme flood.\nThe following choropleth map reveals the locations of these 1,317 residences and illustrates the cumulative impact of flooding on grocery store proximity in each ward. It is interesting to note that the residences experiencing the worst food access impacts of flooding appear to be concentrated in three main areas, all of which lie near the shore at the northern end of the city.\n\n\n\nDar Es Salaam Flood Risk and Food Access Map\n\n\nPlease also see this interactive map of my results and hover your mouse over different residences and wards to see exactly how flooding would impact different households and regions within Dar Es Salaam."
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#discussion",
    "href": "posts/flood-vulnerability/index.html#discussion",
    "title": "Flood Risk and Food Security",
    "section": "Discussion",
    "text": "Discussion\nWhile one might expect severe flooding to impede residents’ access to food, this analysis reveals that the impact is actually quite limited in Dar Es Salaam. In fact, just 1,317 residences which would see the distance to the nearest grocery vendor increase by over 500 meters and the greatest change for any one residence would be 1434 meters. These 1,317 residences represent a tiny fraction, just 0.1%, of the residences in Dar Es Salaam. Furthermore, 500 meters is a little under a third of a mile and 1434 meters is not even a mile, so even the 0.1% most impacted residences would experience relatively minor impacts on food accessibility.\nThese impacts are diminished even further when residential data is aggregated to the ward level. My analysis found that 62 of the 95 wards would witness an increase in average distance of less than 1 meter, revealing that residents’ access to food in large swaths of the city is left largely untouched. With just 6 wards witnessing an increase in the the average distance of over 25 meters and the maximum change being 131.6 meters, it is clear that no single ward would experience a particularly damaging loss of food accessibility. This is great news for the city of Dar Es Salaam! They appear to have developed a resilient food distribution network that can provide for citizens even during cases of extreme flooding, and this network appears to be regionally equitable, since most wards experienced similar levels of impact.\nOne of the reasons why residences can access food reliably during times of flooding may simply be the prolific number of grocery vendors in the city. With little cornerstores in most neighborhoods, there are almost 14 thousand grocery vendors in Dar Es Salaam. For this reason, even when a significant portion of them, 1152 in my analysis, are flooded, residences still have other grocers relatively nearby.\nOne limitation of my analysis may be the aggregation to the ward level. As the points on my map illustrate, there are a few smaller subregions where the impacts are more strongly felt. Professor Holler identified one of these regions as the fishing district. Unfortunately, the size of the wards are such that the effects in the regions most impacted by flooding are dampered by the other residences in their respective wards. Perhaps a subward or neighborhood level analysis would be more telling of the impacts of flooding in individual communities.\nI also noticed a strange error in my final results. There are 40 residences in Dar Es Salaam for which the distance to the nearest grocery vendor actually decreased in the flood scenario. This should be impossible, given that the flood eliminates, rather than creates, grocers. I selected these 40 residences and visualized them on my map as shown below.\n\n\n\nSearching for Spatial Autocorrelation in my Erroneous Results\n\n\nCuriously, these points appear somewhat randomly distributed around Dar Es Salaam, demonstrating no evidence of spatial autocorrelation. This leads me to believe that the mistake is on the computational rather than theoretical side of my analysis, since nearby households should experience similar changes in distance to grocery vendors. Perhaps the way I calculated distance is prone to errors – I am not sure. This error ought to be further investigated in order to assess the validity of my results and the accuracy of the method I used to calculate distances."
  },
  {
    "objectID": "posts/flood-vulnerability/index.html#conclusions",
    "href": "posts/flood-vulnerability/index.html#conclusions",
    "title": "Flood Risk and Food Security",
    "section": "Conclusions",
    "text": "Conclusions\nFortunately, Dar Es Salaam’s food network appears to be quite resilient in the face of extreme flooding. More broadly, cities with widely distributed food networks may fare better during floods. Where I live in the USA, large swaths of the population tend to shop at a select few supermarkets. If a couple of supermarkets were to flood, entire communities would lose access to nearby grocery stores. In this manner, the food distribution network of Dar Es Salaam is far more resilient to flood risk than the food networks here at home. I suspect that the grocery patterns I am accustomed to are typical of developed nations and the grocery patterns of Dar Es Salaam are typical of developing nations. It would be beneficial for researchers to conduct similar analyses in other major cities of developing nations and contrast those with analyses in major cities of developed nations in order to further assess this relationship. Perhaps developed countries should be looking to the widely distributed grocery systems of developing countries as examples for how we can minimize our own climate vulnerabilities."
  },
  {
    "objectID": "posts/cart-final-project/index.html",
    "href": "posts/cart-final-project/index.html",
    "title": "Cartography Final Project",
    "section": "",
    "text": "This is a large file (65 MB) that will load slowly in your browser. For better viewing, download the PDF below.\n\n\n\n\n  \n     Download PDF"
  },
  {
    "objectID": "posts/road-segmentation/index.html",
    "href": "posts/road-segmentation/index.html",
    "title": "Road Segmentation",
    "section": "",
    "text": "Semantic segmentation is a process that takes an input image and returns an output image of the same size where each pixel belongs to a particular class of objects, not distinguishing between objects within a class. For this study, the resulting image is binary, true meaning that a pixel is part of the class we are trying to segment and false meaning a pixel is not part of the class.\nThe purpose of this project was to compare different methods to use satellite data to segment roads. Three machine learning methodologies were used and each had increasing complexity and computational demands. These methods were K-Means Clustering, Random Forests, and Deep Learning using the UNet architecture. K-Means clustering is an unsupervised machine learning algorithm that partitions the data in k clusters, where k is determined by the user based on how many categories you want to cluster. It iteratively updates the centroids of each cluster using mean coordinates until convergence is achieved. Random forest classification is an ensemble learning method involving many decision trees. Each decision tree is trained on a random subset of observations and features, and predictions of the random forest are based on a plurality vote of the decision trees. The UNet architecture for deep neural networks was used as the final method. UNet consists of two convolutional neural networks put together: an encoder and a decoder. The encoder analyzes and extracts features and important information from the input image and the decoder takes this information and generates the binary segmentation image.\nTo produce our model with k-means clustering, we consulted journal articles by Jinxin, Qixin, and Liguang (2006) and Maurya, Gupta, and Shukla (2011). To implement the random forest algorithm, we mainly used scikit-learn’s documentation, but to understand the algorithm, we read this article on decision trees and a follow up article on random forests. For context on deep learning with remote sensing imagery, we consulted Lv et al. (2023) and Fan et al. (2023).\nEach method was implemented and trained, if necessary, and then segmentation metrics were tested to see how well each method was performing. The metrics used were precision, recall, and Dice Coefficient. In addition to comparing the performance of each method, we discuss the computational power, training data, and time needed in order to implement them. The dataset used was Massachusetts Road Dataset, which consists of 1110 training, 14 validation, and 49 testing satellite image and label pairs. Each image is 1500 by 1500 pixels at 1 meter resolution, resulting in each imaging capturing 2.25 square kilometers.\nThis report is a summary of our work, and does not include all code. In particular, training the U-Net model required substantial computational effort and we did not re-run the entire model in this notebook. For full documentation of our work, please visit our GitHub repository."
  },
  {
    "objectID": "posts/road-segmentation/index.html#introduction",
    "href": "posts/road-segmentation/index.html#introduction",
    "title": "Road Segmentation",
    "section": "",
    "text": "Semantic segmentation is a process that takes an input image and returns an output image of the same size where each pixel belongs to a particular class of objects, not distinguishing between objects within a class. For this study, the resulting image is binary, true meaning that a pixel is part of the class we are trying to segment and false meaning a pixel is not part of the class.\nThe purpose of this project was to compare different methods to use satellite data to segment roads. Three machine learning methodologies were used and each had increasing complexity and computational demands. These methods were K-Means Clustering, Random Forests, and Deep Learning using the UNet architecture. K-Means clustering is an unsupervised machine learning algorithm that partitions the data in k clusters, where k is determined by the user based on how many categories you want to cluster. It iteratively updates the centroids of each cluster using mean coordinates until convergence is achieved. Random forest classification is an ensemble learning method involving many decision trees. Each decision tree is trained on a random subset of observations and features, and predictions of the random forest are based on a plurality vote of the decision trees. The UNet architecture for deep neural networks was used as the final method. UNet consists of two convolutional neural networks put together: an encoder and a decoder. The encoder analyzes and extracts features and important information from the input image and the decoder takes this information and generates the binary segmentation image.\nTo produce our model with k-means clustering, we consulted journal articles by Jinxin, Qixin, and Liguang (2006) and Maurya, Gupta, and Shukla (2011). To implement the random forest algorithm, we mainly used scikit-learn’s documentation, but to understand the algorithm, we read this article on decision trees and a follow up article on random forests. For context on deep learning with remote sensing imagery, we consulted Lv et al. (2023) and Fan et al. (2023).\nEach method was implemented and trained, if necessary, and then segmentation metrics were tested to see how well each method was performing. The metrics used were precision, recall, and Dice Coefficient. In addition to comparing the performance of each method, we discuss the computational power, training data, and time needed in order to implement them. The dataset used was Massachusetts Road Dataset, which consists of 1110 training, 14 validation, and 49 testing satellite image and label pairs. Each image is 1500 by 1500 pixels at 1 meter resolution, resulting in each imaging capturing 2.25 square kilometers.\nThis report is a summary of our work, and does not include all code. In particular, training the U-Net model required substantial computational effort and we did not re-run the entire model in this notebook. For full documentation of our work, please visit our GitHub repository."
  },
  {
    "objectID": "posts/road-segmentation/index.html#methods",
    "href": "posts/road-segmentation/index.html#methods",
    "title": "Road Segmentation",
    "section": "Methods",
    "text": "Methods\n\nPackages and Functions\nBefore thoroughly describing our work, we import necessary packages and define a few functions.\n\n\nCode\n# Import packages\nimport skimage.io as skio\nimport skimage.util as sku\nimport skimage.color as skol\nfrom skimage import filters, feature, transform\nimport skimage.morphology as skm\nimport skimage.draw as draw\nfrom skimage.color import rgb2gray\nfrom scipy.signal import convolve2d\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d\nimport numpy as np\nimport cv2\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.ndimage import gaussian_filter, gaussian_laplace\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets\nimport tensorflow.keras.backend as K\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.random import sample_without_replacement\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.ndimage import gaussian_laplace\nfrom scipy.ndimage import maximum_filter\nfrom scipy.ndimage import minimum_filter\nfrom scipy.ndimage import median_filter\n\n\n\n\nCode\n# Function to compute DICE\nsmooth=1\ndef dice_coef(y_true, y_pred):\n    intersection = np.sum(y_true * y_pred)\n    return (2. * intersection + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n        \n# Function to print several accuracy metrics\ndef accuracy_metrics(y_true, y_pred):\n    # Create confusion matrix\n    C = confusion_matrix(y_true, y_pred, labels=(True, False))\n\n    # Overall accuracy rate\n    acc = (C[0,0] + C[1,1])/C.sum()\n\n    # Recall\n    recall = (C[0,0])/(C[0,0] + C[1,0])\n    \n    # Precision\n    prec = (C[0,0])/(C[0,0] + C[0,1])\n\n    # DICE\n    dice = dice_coef(y_true, y_pred)\n\n\n    # Print results\n    print(\"Confusion matrix:\\n\", C)\n    print(\"Overall accuracy:\", np.round(acc, 3), \"\\nPrecision:\", np.round(recall, 3),\n            \"\\nRecall\", np.round(prec, 3), \"\\nDICE:\", np.round(dice, 3))\n\n# Function to create input layer\ndef classify_gray(image):\n\n    # Compute the standard deviation of the r, g, and b channels\n    std_dev = np.std(image, axis = 2)\n\n    # Define a threshold for classifying gray pixels\n    diff_threshold = 6 # Adjust as needed\n\n    # Classify pixels as gray or not gray based on the standard deviation\n    gray_mask = std_dev &lt; diff_threshold\n        \n    return gray_mask\n    \n# Function to compute layers for additional model features\ndef compute_features(img, include_categorical = True):    \n    # Range of values (gray pixels will have low range)\n    r = img.max(axis = 2) - img.min(axis = 2)\n\n    if include_categorical:\n        \n        # Canny edge detection\n        canny_edges_r = feature.canny(img[:,:,0], sigma=4)\n        canny_edges_g = feature.canny(img[:,:,1], sigma=4)\n        canny_edges_b = feature.canny(img[:,:,2], sigma=4)\n\n         # Calculation Canny gradient\n        image_gray = rgb2gray(img)\n        canny_edges = feature.canny(image_gray, sigma=3)\n    \n        # Create disk\n        disk = skm.disk(1)\n    \n        # Area closing for hough lines\n        closed_edges = skm.dilation(canny_edges, footprint = disk)\n        closed_edges = closed_edges * 255\n    \n        lines = transform.probabilistic_hough_line(closed_edges, threshold=5, line_length=25, line_gap=3)\n        hough_lines = np.zeros(image_gray.shape, dtype=np.uint8)\n    \n        # Draw the detected lines on the canvas\n        for line in lines:\n            p0, p1 = line\n            # Draw line segment\n            rr, cc = draw.line(p0[1], p0[0], p1[1], p1[0])\n            hough_lines[rr, cc] = 255  # Set the pixel values to white (255) along the line\n    \n        #create gray mask\n        gray_mask = classify_gray(img)\n        gray_mask = gray_mask.reshape((img.shape[0], img.shape[1]))\n    \n\n        img = np.dstack([img, canny_edges_r, canny_edges_g, canny_edges_b, gray_mask, hough_lines])\n        \n    \n    # Gaussian blur sigma = 1\n    gaus_r_1 = gaussian_filter(img[:,:,0], sigma = 1)\n    gaus_g_1 = gaussian_filter(img[:,:,1], sigma = 1)\n    gaus_b_1 = gaussian_filter(img[:,:,2], sigma = 1)\n    \n    # Gaussian blur sigma = 3\n    gaus_r_3 = gaussian_filter(img[:,:,0], sigma = 3)\n    gaus_g_3 = gaussian_filter(img[:,:,1], sigma = 3)\n    gaus_b_3 = gaussian_filter(img[:,:,2], sigma = 3)\n\n    # Gaussian blur sigma = 5\n    gaus_r_5 = gaussian_filter(img[:,:,0], sigma = 5)\n    gaus_g_5 = gaussian_filter(img[:,:,1], sigma = 5)\n    gaus_b_5 = gaussian_filter(img[:,:,2], sigma = 5)\n    \n    # LoG blur sigma = .5\n    log_r_5 = gaussian_laplace(img[:,:,0], sigma = .5)\n    log_g_5 = gaussian_laplace(img[:,:,1], sigma = .5)\n    log_b_5 = gaussian_laplace(img[:,:,2], sigma = .5)\n    \n    # LoG blur sigma = .6\n    log_r_6 = gaussian_laplace(img[:,:,0], sigma = .6)\n    log_g_6 = gaussian_laplace(img[:,:,1], sigma = .6)\n    log_b_6 = gaussian_laplace(img[:,:,2], sigma = .6)\n    \n    # LoG blur sigma = .8\n    log_r_8 = gaussian_laplace(img[:,:,0], sigma = .8)\n    log_g_8 = gaussian_laplace(img[:,:,1], sigma = .8)\n    log_b_8 = gaussian_laplace(img[:,:,2], sigma = .8)\n    \n    # Add layers to model\n    return np.dstack([img, r,\n                     gaus_r_1, gaus_g_1, gaus_b_1, gaus_r_3, gaus_g_3, gaus_b_3,\n                     gaus_r_5, gaus_g_5, gaus_b_5, log_r_5, log_g_5, log_b_5,\n                     log_r_6, log_g_6, log_b_6, log_r_8, log_g_8, log_b_8])\n\ndef identify_road_cluster(clustered_image, image_label):\n\n    cluster_labels = np.unique(clustered_image)\n\n    best_recall = 0\n    best_cluster = -1\n\n    for i in cluster_labels:\n        cluster = (clustered_image==i)\n        C = confusion_matrix(image_label.ravel(), cluster.ravel(), labels=(True, False))\n\n        # # Overall accuracy rate\n        # acc = (C[0,0] + C[1,1])/C.sum()\n        # # Recall\n        recall = (C[0,0])/(C[0,0] + C[1,0])\n        # Precision\n        # prec = (C[0,0])/(C[0,0] + C[0,1])\n\n        if recall &gt; best_recall:\n            best_recall = recall\n            best_cluster = i\n\n    return best_cluster\n\n\n\n\nImport Images\nFirst, we import the images that we will use for most of our work. For the k-means and random forest sections, we use one image for training and another for testing, so we display those images here.\n\n\nCode\n# Read the data\nrgb = skio.imread(\"../data/MA_roads/tiff/train/10828735_15.tiff\")\nans = skio.imread(\"../data/MA_roads/tiff/train_labels/10828735_15.tif\") &gt; 0\n\nrgb_test = skio.imread(\"../data/MA_roads/tiff/train/21929005_15.tiff\")\nans_test = skio.imread(\"../data/MA_roads/tiff/train_labels/21929005_15.tif\") &gt; 0\n\n# Display training data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb, ax = ax[0])\nax[0].set_title(\"Training Data\")\nskio.imshow(ans, ax = ax[1])\nax[1].set_title(\"Training Solution\");\n\n# Display testing data and correct output\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Data\")\nskio.imshow(ans_test, ax = ax[1])\nax[1].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Image Filters\nIn our k-means and random forest sections, we will compute many features to help identify specific aspects of roads. In the previous section, we defined a function for creating this features. Let us take a closer look at a smaller area and then apply our filters to that area.\n\n\nCode\n# Create training subset of data\nsmall_rgb = rgb[0:400, 1200:, :]\nsmall_ans = ans[0:400, 1200:]\n\n# Create testing subset of data\nsmall_rgb_test = rgb[1200:, 0:400, :]\nsmall_ans_test = ans[1200:, 0:400]\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(small_rgb, ax = ax[0])\nax[0].set_title(\"Training Image\")\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Training Image\");\n\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\nskio.imshow(small_rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let us create and inspect our features.\n\n\nCode\n# Create features\nsmall_rgb_layers = compute_features(small_rgb)\n\n# Inspect features\nfig, ax = plt.subplots(2, 3, figsize = (8, 8))\nskio.imshow(small_rgb_layers[:,:,8], ax = ax[0,0])\nax[0,0].set_title(\"Range of RGB\")\nskio.imshow(small_rgb_layers[:,:,3], cmap = \"gray\", ax = ax[0,1])\nax[0,1].set_title(\"Canny Edges Red\")\nskio.imshow(small_rgb_layers[:,:,6], cmap = \"gray\", ax = ax[0,2])\nax[0,2].set_title(\"Gray Mask\")\nskio.imshow(small_rgb_layers[:,:,7], cmap = \"gray\", ax = ax[1,0])\nax[1,0].set_title(\"Hough Transform\")\nskio.imshow(small_rgb_layers[:,:,12:15], ax = ax[1,1])\nax[1,1].set_title(\"Gaussian Blur RGB\")\nskio.imshow(small_rgb_layers[:,:,18:21], ax = ax[1,2])\nax[1,2].set_title(\"Log of Gaussian RGB\");\n\n\n\n\n\n\n\n\n\nOn the top left is the range of red, green and blue for each pixel. We chose this feature because roads are gray, and in the RGB color space, gray pixels have similar values of red, green and blue.\nOn the top middle is canny edges for the red channel. We created this feature hoping to detect the edges of roads, but as you can see, it detects edges in many objects other than roads. We include canny edges in the red, green and blue channels.\nOn the top right is the graymask created using standard deviation amongst the RGB channels and a threshold. It captures the roads, but also many other features.\nOn the bottom left is the hough transform image, which captures the long linear nature of roads but does not successfully detect every pixel.\nOn the bottom middle is the original image after gaussian blurring with \\(\\sigma = 3\\). Our thought process here was that there might be some noise in the image leading random pixels to have the same R, G, and B values as roads. By blurring the image, we hoped to account for this by giving some weight to the values of nearby pixels. We include gaussian blur with \\(\\sigma = 1\\), \\(\\sigma = 3\\), and \\(\\sigma = 5\\) for red, green and blue, hoping that our model might learn from multiple blurring radii.\nOn the bottom right is our image after the log of gaussian filter has been applied to the red, green and blue channels. We hopes to pick up on the width/frequency of roads with this filter, so we included this filter for \\(\\sigma = 0.5\\), \\(\\sigma = 0.6\\), and \\(\\sigma = 0.8\\).\n\n\nPrincipal Components Analysis\nPrincipal Component Analysis (PCA) was utilized to assess the possibility of cutting down on the number of features input into the various models. Currently there are about 27 filtered and calculated features extracted from each 1500x1500x3 RGB image. When scaling this process, the ability to cut down on preprocessing steps and decrease the size of the image input into the models would provide large increases in the speed and storage needed to run this process. This is where PCA has the potential to help, PCA is a commonly used tool for dimensionality reduction across data science. PCA is especially helpful when a dataset has multiple potentially correlated and redundant features and it is unclear which features are most important. In this situation, many of the features generated from our satellite images may provide very similar information or are highly correlated. PCA would allow us to use less layers while retaining the maximum amount of variance.\n\n\nK-Means Clustering\nK-Means Clustering is a basic unsupervised machine learning algorithm that groups data points into k clusters, the value of k can be chosen by the user. The centroids of each cluster are initialized randomly as three data points and all points are assigned to the cluster nearest to them. Then the mean coordinates of all the points in each cluster is calculated and this value is now the new centroid for that cluster. Then the process is repeated until a maximum number of interactions is reached or the centroids spot changing significantly between each interaction. Although there were only two categories in this study; road and background, k was chosen to be three. This is because it was found by (Maurya, et al) to keep the clusters containing the roads from including other non-road features.\nK-Means clustering was run on features extracted from the full 1500x1500 images, and two sets of features were used. First was all features discussed in the previous section and the second was the top 5 principal components accessed by PCA. We will compare the results of these two approaches to K-Means clustering and then compare the results of K-Means in general to the results from our other models.\n\n\nRandom Forest\nRandom forest is a highly flexible supervised machine learning algorithm that can perform both regression and classificiation and can take both continuous and categorical data as input. Random forest is an ensemble learning method that works by training a specified number of decision trees on the training data. Each decision tree is trained using a random subset of training data and features, and essentially is composed of a root node and a number of internal/decision nodes. Each data point is passed to the root of the decision tree, and at each decision node, either travels to the node’s left or right child depending on the value of one feature of the data point. The data point is passed through the tree until it reaches a terminal node, at which point it is classified or given a predicted value.\nIn the random forest model, classification predictions are created based on a majority vote of the decision trees, while regression predictions are created based on averaging the output of the decision trees. For our work, we consider each pixel as an observation and we use the random forest model to classify pixels as either road or non-road. Due to computational constraints, we did not train a model using the entirety of multiple images – we don’t even train a model on a single image. Each image has \\(2,250,000\\) pixels, and we ran this part of our analysis on our personal computers. Instead, we created our first model by training on the RGB channels of a subset of one image. In the second model, we trained on the RBG channels plus all of the additional layers mentioned above. We noticed that our model tended to produce far more accurate predictions on non-road pixels than road pixels, so we then sampled an image for an equal number of pixels of each class and trained both RGB and additional layer models on that input. Then, we attempted to prevent overfitting by fitting a random forest on the additional layers input reduced to 5 features via PCA. Finally, we trained both RGB and additional layers models on sampled data from multiple images. In the results section, we compare the performance of all of our models.\n\n\nU-Net\nA U-Net model is a convolutional neural network architecture which is designed for segmentation. The architecture is characterized by it’s symmetrical “U-shaped” design composed of an encoder path and a decoder path. The encoder path takes as input the original image and consists of multiple iterations of convolutional layers followed by max pooling layers. This path is designed to take the context and identify key features and reduce the spatial dimensions between each layer. The decoder path is unique to U-Net neural networks, as it attempts to construct a mask using the features learned in the encoder path and regaining the spatial dimensions lost. It consists of a series of up-sampling layers (often using transposed convolutions or interpolation) followed by concatenation with feature maps from the corresponding contracting path. Throughout this process, a skip connection is made between each layer of the two paths to preserve the high resolution between each layer. The final layer of the U-Net typically uses a 1x1 convolution followed by an activation function (e.g., sigmoid or softmax) to produce the final segmentation mask or output. This layer condenses the information learned by the network into the desired output format, such as a binary mask for semantic segmentation tasks.\nOur U-Net model is comprised of 10 layers: 5 encoding layers and 5 decoding layers. We were limited by resources like time and memory, so instead of fitting the model with 1500 x 1500 pixel images, we used 128 x128 pixels. We also divided each 1500 x 1500 into 128 x 128 images, increasing the training size tremendously. Images and masks without roads were removed from the dataset. The model is optimized using the Adam algorithm with a learning rate of 0.01. Our loss is calculated using the Binary Cross-Entropy function. Like previous models, we measured the strength of our model using the dice coefficient."
  },
  {
    "objectID": "posts/road-segmentation/index.html#results",
    "href": "posts/road-segmentation/index.html#results",
    "title": "Road Segmentation",
    "section": "Results",
    "text": "Results\n\nPCA\nRead in our PCA example image and label and crop them\n\n\nCode\npca_img = skio.imread(\"../data/MA_roads/tiff/train/10528735_15.tiff\")\npca_img_label = skio.imread(\"../data/MA_roads/tiff/train_labels/10528735_15.tif\")\n\nx1, x2 = 1200, 1300\ny1, y2 = 300, 375\npca_img_cropped = pca_img[y1:y2, x1:x2, :]\npca_img_label_cropped = pca_img_label[y1:y2, x1:x2]\n\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(pca_img_cropped)\naxes[1].imshow(pca_img_label_cropped, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThis image will be used to demostrate what is being done with PCA and to assess how well the dimension reduction and variance maximization of PCA group of road points and background points into distnguishable clusters. A much smaller image is used so that the data points don’t just appear as a large mass of the size of graph that is able to be shown. For this example, we picked a straight forward patch with a perpedicular road intersection.\nGet all filters of the image\n\n\nCode\npca_filters = compute_features(pca_img_cropped)\n\n\nShow top 5 principal components\n\n\nCode\npca_layers = pca_filters.reshape(pca_filters.shape[0] * pca_filters.shape[1], pca_filters.shape[2])\n# Standardize the features\nscaler = StandardScaler()\npca_layers_scaled = scaler.fit_transform(pca_layers)\n\n# Initialize PCA and fit the scaled data\npca_5 = PCA(n_components=5)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\npca_5_comps = pca_5.fit_transform(pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio_5 = pca_5.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(6, 4))\nplt.bar(range(1, len(explained_variance_ratio_5) + 1), explained_variance_ratio_5, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nThis graph shows the percent of variance explained by the top 5 principal components, it can be interpretted that since the top component explains about 45% of variance, the second component explains about 15%, and so on. Therefore, in reducing our dimensions to just 5 we still maintain around 70-80% of the variance that our previous 27 features had.\nIn order to visualize the new dimensinos that PCA can output, we will use the top 3 components to plot our road and background points in 3D space\n\n\nCode\n# Initialize PCA and fit the scaled data\npca_3 = PCA(n_components=3)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\npca_3_comps = pca_3.fit_transform(pca_layers_scaled)\n\n\nDisplay labelled data points in 3D\n\n\nCode\nfig = plt.figure(1, figsize=(6, 6))\nplt.clf()\n\nax = fig.add_subplot(111, projection=\"3d\", elev=15, azim=90)\nax.set_position([0, 0, 0.95, 1])\n\nX = pca_3_comps\ny = pca_img_label_cropped.ravel()\n\nfor name, label in [(\"Background\", 0), (\"Road\", 255)]:\n    ax.text3D(\n        X[y == label, 0].mean(),\n        X[y == label, 1].mean() + 1.5,\n        X[y == label, 2].mean(),\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\n# y = np.choose(y, [0, 255]).astype(float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Accent', edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThere is some clear distinction between the road and backround points but also still a lot of overlap. This shows that the problem of segmenting roads will be difficult but possible.\n\n\nK-Means Clustering\nLoad in test image, this image will be used to test k-Means and future methods\n\n\nCode\ntest_image = skio.imread(\"../data/MA_roads/tiff/train/21929005_15.tiff\")\ntest_image_label = skio.imread(\"../data/MA_roads/tiff/train_labels/21929005_15.tif\")\ntest_image_label_bool = (test_image_label==255)\n\n\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize = (10,15))\n\naxes[0].imshow(test_image)\naxes[1].imshow(test_image_label, cmap='gray')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nGet all feature layers and PCA top-5 layers for input into K-Means\n\n\nCode\ntest_filters = compute_features(test_image)\ntest_layers = test_filters.reshape(test_filters.shape[0] * test_filters.shape[1], test_filters.shape[2])\n\n# Standardize the features\ntest_layers_scaled = scaler.fit_transform(test_layers)\n\n# Initialize PCA and fit the scaled data\ntest_pca_5 = PCA(n_components=5)\n# layers_pca = pca_10.fit_transform(pca_layers_scaled)\ntest_pca_5_layers = test_pca_5.fit_transform(test_layers_scaled)\n\n\nRun K-Means on all features\n\n\nCode\nkmeans_all_layers = KMeans(n_clusters=3, verbose=1).fit(test_layers)\n\n\nInitialization complete\nIteration 0, inertia 156575021617.0.\nIteration 1, inertia 115025038683.5734.\nIteration 2, inertia 112581278537.2893.\nIteration 3, inertia 112197579571.15659.\nIteration 4, inertia 112101897492.05617.\nIteration 5, inertia 112071542003.54475.\nIteration 6, inertia 112060482467.27419.\nIteration 7, inertia 112056114456.59085.\nIteration 8, inertia 112054313068.1214.\nIteration 9, inertia 112053524278.15723.\nIteration 10, inertia 112053178792.97005.\nConverged at iteration 10: center shift 0.13958334970474479 within tolerance 0.2913024911899612.\n\n\nReshape our segmented image, identify which layer is the layer with the roads, and print metrics\n\n\nCode\nsegmented_image_all_layers = kmeans_all_layers.labels_.reshape((test_image.shape[0], test_image.shape[1]))\nroad_cluster_num = identify_road_cluster(segmented_image_all_layers, test_image_label_bool)\nroad_cluster = (segmented_image_all_layers==road_cluster_num)\naccuracy_metrics(test_image_label_bool.ravel(), road_cluster.ravel())\n\n\nConfusion matrix:\n [[  96616   43649]\n [ 888145 1221590]]\nOverall accuracy: 0.586 \nPrecision: 0.098 \nRecall 0.689 \nDICE: 0.172\n\n\n\n\nCode\n# Show cluster and original image\nfig, axes = plt.subplots(1, 2, figsize=(10, 10), sharex=True, sharey=True)\n\naxes[0].imshow(road_cluster, cmap='gray')\naxes[0].set_title('Road Cluster')\n\naxes[1].imshow(test_image)\naxes[1].set_title('Original Image')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nRun K-Means on the PCA filters\n\n\nCode\nkmeans_pca = KMeans(n_clusters=3, verbose=1).fit(test_pca_5_layers)\n\n\nInitialization complete\nIteration 0, inertia 28462864.121999323.\nIteration 1, inertia 22772599.926497176.\nIteration 2, inertia 22365365.74884916.\nIteration 3, inertia 22305926.751492783.\nIteration 4, inertia 22285522.619277447.\nIteration 5, inertia 22274295.569714464.\nIteration 6, inertia 22267485.489484854.\nIteration 7, inertia 22263139.295963943.\nIteration 8, inertia 22260413.28828707.\nIteration 9, inertia 22258742.78713832.\nIteration 10, inertia 22257703.835843332.\nIteration 11, inertia 22257070.42794912.\nConverged at iteration 11: center shift 0.0003099553591653962 within tolerance 0.00041374644497123064.\n\n\n\n\nCode\nsegmented_image_pca = kmeans_pca.labels_.reshape((test_image.shape[0], test_image.shape[1]))\npca_road_cluster_num = identify_road_cluster(segmented_image_pca, test_image_label_bool)\npca_road_cluster = (segmented_image_pca==pca_road_cluster_num)\naccuracy_metrics(test_image_label_bool.ravel(), pca_road_cluster.ravel())\n\n\nConfusion matrix:\n [[  93505   46760]\n [ 414199 1695536]]\nOverall accuracy: 0.795 \nPrecision: 0.184 \nRecall 0.667 \nDICE: 0.289\n\n\n\n\nCode\n# Show cluster and original image\nfig, axes = plt.subplots(1, 2, figsize=(10, 10), sharex=True, sharey=True)\n\naxes[0].imshow(pca_road_cluster, cmap='gray')\naxes[0].set_title('Road Cluster')\n\naxes[1].imshow(test_image)\naxes[1].set_title('Original Image')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nA clear visual difference can be seen with the K-Means clustering run on the PCA top-5 components. The resulting road clusters are more uniform, while the results with using all features were quite grainy. This observation impacts the metrics as well, with PCA layers having similar recall to all layers but better precision and therefore DICE coefficient.\n\n\nRandom Forest Results\n\nInitial RGB Model\nRecall that our initial random forest model uses a small subset of our training image, as displayed earlier. First, we train our model.\n\n\nCode\n# Flatten images\ntrain_small_rgb = small_rgb.reshape(small_rgb.shape[0]*small_rgb.shape[1], 3)\ny_train = small_ans.reshape(small_ans.shape[0]*small_ans.shape[1])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel1 = RF.fit(train_small_rgb, y_train)\n\n# Predictions on training data\nmodel1_pred = model1.predict(train_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_train, model1_pred)\n\n\nConfusion matrix:\n [[  8986   4037]\n [  1545 105432]]\nOverall accuracy: 0.953 \nPrecision: 0.853 \nRecall 0.69 \nDICE: 0.763\n\n\nWhile we have a really good overall accuracy rate, we are correctly predicting only 69% of the actual road pixels. With a precision of 0.85, about 85% of our road predictions are actually roads.\n\n\nCode\n# Convert predictions to image\ntrain_preds = model1_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\n\nVisually, our solution looks alright, but it obviously has room for improvement. Let’s see what our results look like on the testing data.\n\n\nCode\n# Flatten images\ntest_small_rgb = small_rgb_test.reshape(small_rgb_test.shape[0]*small_rgb_test.shape[1], 3)\ny_test = small_ans_test.reshape(small_ans_test.shape[0]*small_ans_test.shape[1])\n\n# Predictions on testing data\nmodel1_test_pred = model1.predict(test_small_rgb)\n\n# Confusion matrix\naccuracy_metrics(y_test, model1_test_pred)\n\n\nConfusion matrix:\n [[   605   6498]\n [  4762 108135]]\nOverall accuracy: 0.906 \nPrecision: 0.113 \nRecall 0.085 \nDICE: 0.097\n\n\nWhile we still have a good overall accuracy rate, our predictions of roads is substantially worse. We have only classified 8.5% of the road pixels correctly, and only 11.3% of our road predictions were actually roads.\n\n\nCode\n# Convert predictions to image\ntest_preds = model1_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 2, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\");\n\n\n\n\n\n\n\n\n\nOur model did NOT generalize well! This looks terrible!\n\n\nInitial Additional Layers Model\nNow, let’s try training on the same region, but incorporating all of the features described above.\n\n\nCode\n# Train model\n\n# Flatten image\ntrain_small_rgb_layers = small_rgb_layers.reshape(small_rgb_layers.shape[0]*small_rgb_layers.shape[1], small_rgb_layers.shape[2])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel2 = RF.fit(train_small_rgb_layers, y_train)\n\n# Predictions on training data\nmodel2_pred = model2.predict(train_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_train, model2_pred)\n\n\nConfusion matrix:\n [[ 13023      0]\n [     0 106977]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nNow we have virtually perfect results! Let’s look at an image of the output.\n\n\nCode\n# Convert predictions to image\ntrain_preds = model2_pred.reshape(small_ans.shape[0], small_ans.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(train_preds, ax = ax[0])\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(train_preds==small_ans, ax = ax[2])\nax[2].set_title(\"Errors (look closely)\");\n\n\n\n\n\n\n\n\n\nYep, can’t even find the errors without looking closely at the difference between the two images. Let’s evaluate our results on the testing data!\n\n\nCode\n# Create additional features\nsmall_rgb_test_layers = compute_features(small_rgb_test)\n\n# Flatten image\ntest_small_rgb_layers = small_rgb_test_layers.reshape(small_rgb_test_layers.shape[0]*small_rgb_test_layers.shape[1], small_rgb_layers.shape[2])\n\n# Predictions on testing data\nmodel2_test_pred = model2.predict(test_small_rgb_layers)\n\n# Confusion matrix\naccuracy_metrics(y_test, model2_test_pred)\n\n\nConfusion matrix:\n [[   321   6782]\n [  1839 111058]]\nOverall accuracy: 0.928 \nPrecision: 0.149 \nRecall 0.045 \nDICE: 0.069\n\n\n\n\nCode\n# Convert predictions to image\ntest_preds = model2_test_pred.reshape(small_ans_test.shape[0], small_ans_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (10, 6))\n\nskio.imshow(test_preds, ax = ax[0], cmap = \"gray\")\nax[0].set_title(\"Predicted Labels\")\n\nskio.imshow(small_ans_test, ax = ax[1])\nax[1].set_title(\"Actual Solution\")\n\nskio.imshow(test_preds==small_ans_test, ax = ax[2])\nax[2].set_title(\"Errors\");\n\n\n\n\n\n\n\n\n\nAdding these filters to our model had negligible impact on our results. It improved the accuracy from 0.906 to 0.928 and the precision from 0.113 to 0.149, but the recall dropped from 0.085 to 0.045. This means that of the pixels that actually represent roads, we are only correctly classifying 4.7% of them. With perfect results on our training data and pitiful results on our testing data, it appears that incorporating these features in our training data led to severe overfitting! Visually, our results look slightly less random, even though the performance metrics are worse.\n\n\nSampling for Overfitting: RGB\nWe have fed a substantial amount of data, which ought to contain some useful information regarding roads, to our model In our training solution, this data was in fact useful, leading to virtually 100% accuracy. However, on the testing data for both the RGB model and the model with additional layers, our model correctly predicted less than 10% of our roads. Perhaps this means that our model is overfit to our training data. Since the vast majority of pixels in our training data represent non-roads, perhaps our model is overfit to the particularities of the non-road pixels in our training data. One way to address this issue is to randomly select an equal number of pixels of both classes, and then train the model on those pixels. Let’s try randomly picking 5000 road pixels and 5000 non-road pixels for our training data and 5000 of each for our testing data and evaluating our model’s performance.\nFirst, let’s use this method on a model with just RGB layers.\n\n\nCode\n# Flatten training images\ntrain_rgb = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\ny_train = ans.reshape(ans.shape[0]*ans.shape[1])\n\n# Subset training data by label\ny_train_true = y_train[y_train]\ny_train_false = y_train[~y_train]\ntrain_rgb_true = train_rgb[y_train]\ntrain_rgb_false = train_rgb[~y_train]\n\n# Sample indices of each label\ntrue_indices = sample_without_replacement(y_train_true.shape[0], 10000)\nfalse_indices = sample_without_replacement(y_train_false.shape[0], 10000)\n\n# Create modified training data\ny_train_mod = np.concatenate([y_train_true[true_indices[:5000]], y_train_false[false_indices[:5000]]])\ntrain_rgb_mod = np.concatenate([train_rgb_true[true_indices[:5000]], train_rgb_false[false_indices[:5000]]])\n\n# Create modified testing data\ny_test_mod = np.concatenate([y_train_true[true_indices[5000:]], y_train_false[false_indices[5000:]]])\ntest_rgb_mod = np.concatenate([train_rgb_true[true_indices[5000:]], train_rgb_false[false_indices[5000:]]])\n\n\n\n\nCode\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel3 = RF.fit(train_rgb_mod, y_train_mod)\n\n# Predictions on training data\nmodel3_pred = model3.predict(train_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model3_pred)\n\n\nConfusion matrix:\n [[4960   40]\n [  73 4927]]\nOverall accuracy: 0.989 \nPrecision: 0.985 \nRecall 0.992 \nDICE: 0.989\n\n\nWhile this model does not have 100% training accuracy like the additional layers model, it has improved significantly over the original RGB model. Most notably, the training recall has improved from less than 70% to roughly 99%. Let’s see if we maintain this performance when we make predictions on our testing data.\n\n\nCode\n# Predictions on testing data\nmodel3_test_pred = model3.predict(test_rgb_mod)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model3_test_pred)\n\n\nConfusion matrix:\n [[3794 1206]\n [1267 3733]]\nOverall accuracy: 0.753 \nPrecision: 0.75 \nRecall 0.759 \nDICE: 0.754\n\n\nOur results are encouraging! Our overall accuracy, precision, and recall are all approximatly 0.75. In the original RGB model, the overall accuracy was over 90%, while the precision and recall were roughly 10%. By balancing the amount of training data in each class, we were able to balance the different accuracy metrics, improving our predictions of roads at the expense of our predictions of non-roads. Perhaps if we incorporate our additional layers into the model, these balance improvements will translate to balanced and higher accuracy metrics.\nWhile our results above are encouraging, our training and testing data were both drawn from the same image, so our model may have overtrained to this image. Let’s form predictions and compute accuracy metrics on a different image.\n\n\nCode\n# Flatten testing images\nflat_rgb_test = rgb_test.reshape(rgb_test.shape[0]*rgb_test.shape[1], 3)\ny_test = ans_test.reshape(ans_test.shape[0]*ans_test.shape[1])\n\n# Predictions on testing data\nmodel3_test_pred_2 = model3.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model3_test_pred_2)\n\n\nConfusion matrix:\n [[ 100346   39919]\n [ 344802 1764933]]\nOverall accuracy: 0.829 \nPrecision: 0.225 \nRecall 0.715 \nDICE: 0.343\n\n\nSurprisingly, the overall accuracy is higher in the testing image than in the training image! The recall is still over 70%, indicating that we are capturing most pixels representing roads correctly. With a much lower precision, we must be predicting road pixels frequently where there are not actually roads.\nSince we are working with an entire image, we can inspect our results!\n\n\nCode\n# Convert predictions to image\ntest_preds = model3_test_pred_2.reshape(rgb_test.shape[0], rgb_test.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nIt looks like we tend to label pixels as roads when they in reality represent other human features like buildings. We also exaggerate the width of some roads.\n\n\nSampling for Overfitting: Additional Layers\nNow let’s try the same sampling technique but after producing all of our features.\n\n\nCode\n# Create additional features\ntrain_rgb_mod_layers = compute_features(rgb)\n\n# Flatten training image with extra layers\ntrain_rgb_2 = train_rgb_mod_layers.reshape(train_rgb_mod_layers.shape[0]*train_rgb_mod_layers.shape[1], train_rgb_mod_layers.shape[2])\n\n# Subset training data by label\ntrain_rgb_true_2 = train_rgb_2[y_train]\ntrain_rgb_false_2 = train_rgb_2[~y_train]\n\n# Create modified training data\ntrain_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[:5000]], train_rgb_false_2[false_indices[:5000]]])\n\n# Create modified testing data\ntest_rgb_mod_2 = np.concatenate([train_rgb_true_2[true_indices[5000:]], train_rgb_false_2[false_indices[5000:]]])\n\n\n\n\nCode\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel4 = RF.fit(train_rgb_mod_2, y_train_mod)\n\n# Predictions on training data\nmodel4_pred = model4.predict(train_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model4_pred)\n\n\nConfusion matrix:\n [[5000    0]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nOur training results are literally perfect. Does this translate to our testing data?\n\n\nCode\n# Predictions on testing data\nmodel4_test_pred = model4.predict(test_rgb_mod_2)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model4_test_pred)\n\n\nConfusion matrix:\n [[4102  898]\n [1062 3938]]\nOverall accuracy: 0.804 \nPrecision: 0.794 \nRecall 0.82 \nDICE: 0.807\n\n\nIt appears that there were some errors on our testing data. Going from the RGB model to the additional layers model, our overall accuracy improved from 0.755 to 0.812, the precision improved from 0.749 to 0.796, and the recall improved from 0.767 to 0.84. These are the most accurate road predictions yet!\nWhile our training and testing data contained none of the same pixels, they were both drawn from the same image, so it is possible that they were overtrained to our particular image of choice. Perhaps a more valid testing metric would involve testing our model on pixels from a different image. Let’s form predictions and compute accuracy metrics on the entirety of another image.\n\n\nCode\n# Create additional features\ntest_rgb_layers_3 = compute_features(rgb_test)\n\n# Flatten testing images\ntest_rgb_3 = test_rgb_layers_3.reshape(test_rgb_layers_3.shape[0]*test_rgb_layers_3.shape[1], test_rgb_layers_3.shape[2])\n\n\n\n\nCode\n# Predictions on testing data\nmodel4_test_pred_2 = model4.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model4_test_pred_2)\n\n\nConfusion matrix:\n [[  92296   47969]\n [ 249036 1860699]]\nOverall accuracy: 0.868 \nPrecision: 0.27 \nRecall 0.658 \nDICE: 0.383\n\n\nSimilar to the RGB model, the results on the testing image were largely similar to the results on the previous image, except for the precision dropping by over 50% The overall accuracy is over 85%, but the recall is now 65.9% and the precision is now 26.6%. While this is certainly not perfect, the precision and recall are still a substantial improvement over the models without sampling. However, the recall was actually slightly higher in the sampled RGB model, indicating that the RGB model generalized better in terms of predicting road pixels. Perhaps there are tactics we can use to combat overfitting.\nAlso, since we are now working with a complete image, we can once again inspect a full image illustrating our predictions versus the truth.\n\n\nCode\n# Convert predictions to image\ntest_preds = model4_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nOnce again, our model tends to incorrectly predict roads where there are other human features like buildings, and it exaggerates the width of some roads.\n\n\nPCA for Overfitting\nIt appears that our additional layers model with sampled training data is overfit to our training data, as we have excellent performance on the training data but subpar performance on the new image. Perhaps our model suffers from overfitting because we have constructed so many features, many of which are similar to one another. To help combat this issue, we fit our model again below, but after applying Principal Component Analysis and selecting the most important components.\nFirst, we apply Principal Component Analysis to our training data and plot the percent variance explained by each component. Note that PCA is only applicable for continuous features, so we cannot include binary features such as canny edges in this model.\n\n\nCode\n# Add layers to model\ntrain_pca_layers = compute_features(rgb, include_categorical = False)   \n\n# Flatten training image with extra layers\ntrain_pca_layers_flat = train_pca_layers.reshape(train_pca_layers.shape[0]*train_pca_layers.shape[1], train_pca_layers.shape[2])\n\n# Standardize the features\nscaler = StandardScaler()\ntrain_pca_layers_scaled = scaler.fit_transform(train_pca_layers_flat)\n\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=train_pca_layers.shape[2])\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Plotting the explained variance ratio\nplt.figure(figsize=(8, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.show()\n\n\n\n\n\n\n\n\n\nApparently, over 50% of the variance in our data can be explained by the first component! The second component only accounts for about 11.5% of the variance in the data, and the numbers continue to drop after that.\n\n\nCode\nexplained_variance_ratio[0:5].sum()\n\n\n0.843420909342875\n\n\nThe first 5 components account for over 84% of the variation in our data. Let’s try only retaining the first 5 components for our model and seeing whether our performance improves.\n\n\nCode\n# Initialize PCA and fit the scaled data\npca = PCA(n_components=5)\nlayers_pca = pca.fit_transform(train_pca_layers_scaled)\n\n# Subset training data by label\nlayers_pca_true = layers_pca[y_train]\nlayers_pca_false = layers_pca[~y_train]\n\n# Create modified training data\nlayers_pca_mod_train = np.concatenate([layers_pca_true[true_indices[:5000]], layers_pca_false[false_indices[:5000]]])\n\n# Create modified testing data\nlayers_pca_mod_test = np.concatenate([layers_pca_true[true_indices[5000:]], layers_pca_false[false_indices[5000:]]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel5 = RF.fit(layers_pca_mod_train, y_train_mod)\n\n# Predictions on training data\nmodel5_pred = model5.predict(layers_pca_mod_train)\n\n# Confusion matrix\naccuracy_metrics(y_train_mod, model5_pred)\n\n\nConfusion matrix:\n [[4999    1]\n [   0 5000]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual with our additional layers, our model’s performance is virtually perfect on the training data. Let’s take a look at the testing performance.\n\n\nCode\n# Predictions on testing data\nmodel5_test_pred = model5.predict(layers_pca_mod_test)\n\n# Confusion matrix\naccuracy_metrics(y_test_mod, model5_test_pred)\n\n\nConfusion matrix:\n [[3820 1180]\n [1379 3621]]\nOverall accuracy: 0.744 \nPrecision: 0.735 \nRecall 0.764 \nDICE: 0.749\n\n\nOn the testing data, all of our model’s performance metrics are lower than its non-PCA counterpart, although not by that much. Let’s test on an entirely new image.\n\n\nCode\n# Add layers to model\ntest_pca_layers = compute_features(rgb_test, include_categorical = False)\n\n# Flatten testing images\ntest_pca_layers_flat = test_pca_layers.reshape(test_pca_layers.shape[0]*test_pca_layers.shape[1], 22)\n\n# Standardize the features\ntest_pca_layers_scaled = scaler.fit_transform(test_pca_layers_flat)\n\n# Project onto principal components\nlayers_pca_test = pca.transform(test_pca_layers_scaled)\n\n\n\n\nCode\n# Predictions on testing data\nmodel5_test_pred_2 = model5.predict(layers_pca_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model5_test_pred_2)\n\n\nConfusion matrix:\n [[  75268   64997]\n [ 536637 1573098]]\nOverall accuracy: 0.733 \nPrecision: 0.123 \nRecall 0.537 \nDICE: 0.2\n\n\nAgain, on the testing image, all of our model’s performance metrics are lower than its non-PCA counterpart. In this scenario, it appears that the components explaining very little variation in the data were actually somewhat useful for predictions. Note that we tried this with a variety of number of retained components, and we found that the model’s performance improved as we increased the number of components.\nBelow, we inspect the image of our predictions.\n\n\nCode\n# Convert predictions to image\ntest_preds = model5_test_pred_2.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nVisually, our predictions appear somewhat worse that those from our non-PCA additional layers model. There is overall a lot more noise in our predictions, and interestingly, we are not predicting a road in much of the massive highway.\n\n\nSampling from Multiple Images: RGB\nPCA did not help us generalize to new testing data, but perhaps there is another approach we could take. Earlier, we created our training data by sampling from a single image. Perhaps we could sample from multiple images, mitigating bias from working with a single image. Below, we select 10 images and sample from them to train our model.\n\n\nCode\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,3))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n    \n    # Flatten training images\n    rgb_flat = rgb.reshape(rgb.shape[0]*rgb.shape[1], 3)\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel6 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel6_pred = model6.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model6_pred)\n\n\nConfusion matrix:\n [[48194  1806]\n [ 2559 47441]]\nOverall accuracy: 0.956 \nPrecision: 0.95 \nRecall 0.964 \nDICE: 0.957\n\n\nThese metrics are actually somewhat lower than the training metrics for the RGB model sampled from a single image. But our true question is whether the model generalizes better to the testing data – more specifically, to our new testing image.\n\n\nCode\n# Predictions on testing data\nmodel6_test_pred = model6.predict(flat_rgb_test)\n\n# Confusion matrix\naccuracy_metrics(y_test, model6_test_pred)\n\n\nConfusion matrix:\n [[ 110650   29615]\n [ 384361 1725374]]\nOverall accuracy: 0.816 \nPrecision: 0.224 \nRecall 0.789 \nDICE: 0.348\n\n\nIn the RGB model trained on sampling data from one image, our results were as follows.\n\nOverall accuracy: 0.829\nPrecision: 0.225\nRecall 0.715\nDICE: 0.343\n\nThis model has slightly decreased in all metrics except for the recall. Considering the additional computational power required to train this model, it does not appear to be advantageous over the other model.\n\n\nCode\n# Convert predictions to image\ntest_preds = model6_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nThe output image visually looks quite similar to that when trained on data sampled from a single image.\n\n\nSampling from Multiple Images: Additional Layers\nFirst, we train our model.\n\n\nCode\n# Store id's of images\nimgs = [\"10828735_15\", \"10228675_15\", \"10228705_15\", \"10228720_15\", \"10228735_15\", \"10528675_15\", \"10528750_15\", \"10978720_15\", \"11128825_15\", \"12028750_15\"]\n\n# Initialize arrays to store training data\ny_train = np.array([])\nrgb_train = np.zeros((0,27))\n\n# Sample from each image\nfor img in imgs:\n    rgb = skio.imread(\"../data/MA_roads/tiff/train/\" + img + \".tiff\")\n    ans = skio.imread(\"../data/MA_roads/tiff/train_labels/\" + img + \".tif\") &gt; 0\n\n    # Create additional layers\n    rgb_layers = compute_features(rgb)\n    \n    # Flatten training images\n    rgb_flat = rgb_layers.reshape(rgb_layers.shape[0]*rgb_layers.shape[1], rgb_layers.shape[2])\n    ans_flat = ans.reshape(ans.shape[0]*ans.shape[1])\n    \n    # Subset training data by label\n    ans_true = ans_flat[ans_flat]\n    ans_false = ans_flat[~ans_flat]\n    rgb_true = rgb_flat[ans_flat]\n    rgb_false = rgb_flat[~ans_flat]\n    \n    # Sample indices of each label\n    true_indices = sample_without_replacement(ans_true.shape[0], 5000)\n    false_indices = sample_without_replacement(ans_false.shape[0], 5000)\n    \n    # Create modified training data\n    y_train = np.concatenate([y_train, ans_true[true_indices], ans_false[false_indices]])\n    rgb_train = np.concatenate([rgb_train, rgb_true[true_indices], rgb_false[false_indices]])\n\n# Create model\nRF = RandomForestClassifier()\n\n# Fit and output the performance of the model\nmodel7 = RF.fit(rgb_train, y_train)\n\n# Predictions on training data\nmodel7_pred = model7.predict(rgb_train)\n\n# Confusion matrix\naccuracy_metrics(y_train, model7_pred)\n\n\nConfusion matrix:\n [[49999     1]\n [    1 49999]]\nOverall accuracy: 1.0 \nPrecision: 1.0 \nRecall 1.0 \nDICE: 1.0\n\n\nPer usual when working with all of our layers, we have virtually perfect results on our training data. Next, we test the model on our usual testing image.\n\n\nCode\n# Predictions on testing data\nmodel7_test_pred = model7.predict(test_rgb_3)\n\n# Confusion matrix\naccuracy_metrics(y_test, model7_test_pred)\n\n\nConfusion matrix:\n [[ 115371   24894]\n [ 321743 1787992]]\nOverall accuracy: 0.846 \nPrecision: 0.264 \nRecall 0.823 \nDICE: 0.4\n\n\nThis is the best performance we have seen so far. The biggest difference between this model and the additional layers model with training data sampled from a single image is that the recall has increased from 0.658 to 0.823.\nFinally, we visually inspect our output.\n\n\nCode\n# Convert predictions to image\ntest_preds = model7_test_pred.reshape(test_rgb_layers_3.shape[0], test_rgb_layers_3.shape[1])\n\n# Create figure\nfig, ax = plt.subplots(1, 3, figsize = (12, 6))\nskio.imshow(rgb_test, ax = ax[0])\nax[0].set_title(\"Testing Image\")\nskio.imshow(test_preds, ax = ax[1])\nax[1].set_title(\"Testing Predictions\")\nskio.imshow(ans_test, ax = ax[2])\nax[2].set_title(\"Testing Solution\");\n\n\n\n\n\n\n\n\n\nThis is definitely the cleanest output we have seen so far, which is reflective of our model’s high recall value. The model continues to erroneously label human features that are not roads as roads, and it has labelled virtually the entire highway as road instead of just its centerlines. Differentiating between the centerline and the road surface seems to be a very difficult problem.\n\n\nComparison of Random Forest Results\nWe fit a lot of random forest models, so we summarized their accuracy metrics in the following table. Note that our testing metrics listed here are the results when we tested on an entirely new image.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nRGB\nMore Layers\nRGB Sampled\nMore Layers Sampled\nPCA Sampled\nRGB Multiple Images\nMore Layers Multiple Images\n\n\n\n\nTraining Accuracy\n0.953\n1.0\n0.989\n1.0\n1.0\n0.956\n1.0\n\n\nTraining Precision\n0.853\n1.0\n0.985\n1.0\n1.0\n0.95\n1.0\n\n\nTraining Recall\n0.69\n1.0\n0.992\n1.0\n1.0\n0.964\n1.0\n\n\nTraining Dice\n0.763\n1.0\n0.989\n1.0\n1.0\n0.957\n1.0\n\n\nTesting Accuracy\n0.906\n0.928\n0.829\n0.868\n0.733\n0.816\n0.846\n\n\nTesting Precision\n0.113\n0.149\n0.225\n0.27\n0.123\n0.224\n0.264\n\n\nTesting Recall\n0.085\n0.045\n0.715\n0.658\n0.537\n0.789\n0.823\n\n\nTesting Dice\n0.097\n0.069\n0.343\n0.383\n0.2\n0.348\n0.4\n\n\n\nFor a quick summary of each model’s performance, we look at the testing Dice score. We noticed a major bump in performance when we started sampling equal portions of training data, with the dice coefficient of both the RGB and additional layers models jumping from less than 0.1 to above 0.3. We attempted to account for overfitting by introducing PCA, but this cut our Dice coefficient in half. Finally, creating our training data by sampling from multiple images resulted in the highest performance, particularly in the additional layers model, where the Dice coefficient jumped to 0.4. Our final model was our best performing random forest model by every metric except testing accuracy.\n\n\n\nU-Net Results\nBecause of the high computational demans of the U-Net model, we did not re-run the model in this notebook. For full documentation of our code, please see our GitHub repository.\nWe managed to get solid predictions for the small 128 x 128 images. The precision got up to 0.445, the recall went to 0.938, and the dice coefficient was 0.604. We wanted to compare our results with the previous models, so we ran our predictions on the test set (also divided into 128 x 128 pixel images), and reconstructed the 128 x 128 masks to get 1408 x 1408 masks which resemble the original image. These final predictions were less accurate than the original 128 x 128 masks with a precision metric of 0.06, a recall of 0.197, and a dice coefficient of 0.092. Individually the predictions made by the U-Net were pretty accurate, but when reconstructed to the greater original image size, the cumulative prediction must have lost enough information to lower its accuracy metrics.\n\n\nOverall Result Comparison\nWhich method worked best: k-means, random forest, or U-Net? We include the following table to facilitate comparison of results. We trained many different models in the preceding sections, but we only report the best performing model for each method in the table below.\n\n\n\nModel\nK-Means\nRandom Forest\nU-Net\n\n\n\n\nTesting Precision\n0.187\n0.264\n0.06\n\n\nTesting Recall\n0.661\n0.823\n0.197\n\n\nTesting Dice\n0.291\n0.4\n0.092\n\n\n\nEssentially, with increasing complexity of the algorithm and computational requirements came increased performance. K-Means clustering required the least computational power but also achieved the worst results, achieving a Dice coefficient of 0.291 after we reduced the number of features using PCA. Random forest required more computational power, especially when we began sampling training data from multiple images, achieving an improved Dice score of 0.4. Finally, our U-Net model achieved even better results when we considered small subsets of our testing images with a dice score of 0.604, but poor results when applied to the entire image with a Dice score of 0.092. The U-Net models in the literature and Kaggle datasets have achieved by far the best results. Unfortunately, achieving such high performance with U-Net requires substantial computational power for model training. Overall, we conclude that U-Net can achieve the best performance when appropriate resources are available. In the absence of high performance computing capabilities, random forest may suffice.\nAdditionally, it is interesting to note that we had mixed results with the effectiveness of PCA. For K-Means Clustering, using the top-5 components helped the model perform better, but for Random Forest, using PCA led to decreased performance. PCA was not applicable to deep neural networks."
  },
  {
    "objectID": "posts/road-segmentation/index.html#accessibility",
    "href": "posts/road-segmentation/index.html#accessibility",
    "title": "Road Segmentation",
    "section": "Accessibility",
    "text": "Accessibility\nDue the high requirements to run these models, especially U-Net, and the large amount of data needed, these methods are not very accessible to many individuals. Luckily this is not a task that many individuals would look to perform but more a task that would be performed by large organization like governments. Considering accessibility at this scale brings in the question of access to large datasets and high performance computers, as many countries may not have access to such resources and therefore would not be able to implement these methods on a large scale.\nThere are some positive accessibility points with these algorithms as well, being that no one has to physically be in the place where you hope to segment roads. This means that in areas with difficult to access roads, one can still segment the road network. Some examples of when this could be extremely helpful are climate disasters and war time aid. With climate disasters, roads may be destroyed like during the floods in Vermont last summer. Knowing what roads were destroyed without having to go out into the field where danger from flood is still high could be extremely helpful for disaster response. In the example of war, especially in places that are being heavily bombed, it may be hard to get humanitarian aid workers into certain areas due to the destruction of roads. These algorithms could help assess with roads are still open without having to put anyone on the field and in danger.\nThere are some other accessibility points such as ability to interpret the image segmentation. In an area where the visual interpretation is essential, making sure color blind inclusive colors are used is very important."
  },
  {
    "objectID": "posts/road-segmentation/index.html#ethical-considerations",
    "href": "posts/road-segmentation/index.html#ethical-considerations",
    "title": "Road Segmentation",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nAs mentioned above, some countries and organizations may not have the resources to train large models like U-Net. Resources like data centers and HPCs are essential and many places may not have access to them. Looking deeper, if a country like the US trains a UNet model for segmenting roads on a dataset including road in the US, that model could be used by other countries but it may be much less accurate if there road infrastructure looks different than that in the US. Furthermore, if large labeled datasets of roads only exist with roads from developed countries, this would hinder any country that may have less built infrastructure as the training data will not align with the data being input into the model.\nFurther ethical considerations around data across fields is the exploitative nature in which many labeled datasets are created. Large tech companies often out-source their data labeling overseas and pay people very little for the data that makes all this possible. They make huge profits off of the models that are only possible with this labeled data and the associated labor.\nGoing into remote sensing and the high resolution of satellite images, privacy concerns may be raised. These satellite images will contain sections consisting of private problems and invasion of personal privacy should always be a consideration when using remote sensing on inhabited areas."
  },
  {
    "objectID": "posts/road-segmentation/index.html#reflection",
    "href": "posts/road-segmentation/index.html#reflection",
    "title": "Road Segmentation",
    "section": "Reflection",
    "text": "Reflection\n\nSchedule and Obstacles\nWe each stayed on schedule for our tasks in general, but some tasks were much more difficult than expected and we ended up deciding we would save them for future work. Some tasks just ended up being more time consuming, like training the U-Net. Lots of time was spent waiting for a model to train just have an error or horrible predictions. Looking back, we should have learned how to connect to Middlebury’s ADA cluster to train our model. We ended up ditching LiDAR data because it was more difficult to process than we realized and would put an even heavily burden on the amount of training data we were using. We also felt like we already had a plethora of topics to discuss and compare, so it wasn’t necessary to add even another aspect to the project.\n\n\nFuture Work\nFuture work we are interested in pursuing is of course integrating LiDAR data into each of our models and comparing the metrics on how well roads were segmented. We are very curious to see if this additional data would help our models and are interested in learning about LiDAR data. Another path of future work is performing more data engineering and augmentation on our training data for input into U-Net. We know from reading and seeing models trained on Kaggle that very accurate segmentations are possible and we had a lot of room for improvement with our model."
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html",
    "href": "posts/COVID-19-spatial-accessibility/index.html",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "",
    "text": "In Rapidly Measuring Spatial Accessibility of COVID-19 Healthcare Resources: A Case Study of Illinois, USA, Kang et al. use the Enhanced Two-Step Floating Catchment Area (E2SFCA) method with parallel processing to assess the accessibility of ICU beds and ventilators to vulnerable populations – defined as individuals of 50 years of age or older – and COVID-19 patients in Illinois. The goal of their study was to provide policymakers with regularly updated information concerning the spatial capability of key treatment resources to COVID-19 patients and vulnerable populations. The authors found that access to ventilators and ICU beds was unevenly distributed throughout Illinois, and they published updated analyses daily in an online, interactive webpage called WhereCovid.\nWe seek to reproduce Kang et al.’s study for a few reasons. First, the global pandemic is a pressing issue, and public policy decisions regarding the pandemic ought to be based upon reputable research. Reproducing this study either confirms its findings, contributing to its validity as a basis for public policy, or overturns its findings, improving the basis of knowledge from which the government designs public policy. There are also intellectual and pedagogical motives for conducting a reproduction of this study. Intellectually, a reproduction confirms the validity of the researchers’ spatial techniques; and pedagogically, conducting a reproduction allows students to see how geospatial studies are conducted and encourages students think critically about their reputability."
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#introduction",
    "href": "posts/COVID-19-spatial-accessibility/index.html#introduction",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "",
    "text": "In Rapidly Measuring Spatial Accessibility of COVID-19 Healthcare Resources: A Case Study of Illinois, USA, Kang et al. use the Enhanced Two-Step Floating Catchment Area (E2SFCA) method with parallel processing to assess the accessibility of ICU beds and ventilators to vulnerable populations – defined as individuals of 50 years of age or older – and COVID-19 patients in Illinois. The goal of their study was to provide policymakers with regularly updated information concerning the spatial capability of key treatment resources to COVID-19 patients and vulnerable populations. The authors found that access to ventilators and ICU beds was unevenly distributed throughout Illinois, and they published updated analyses daily in an online, interactive webpage called WhereCovid.\nWe seek to reproduce Kang et al.’s study for a few reasons. First, the global pandemic is a pressing issue, and public policy decisions regarding the pandemic ought to be based upon reputable research. Reproducing this study either confirms its findings, contributing to its validity as a basis for public policy, or overturns its findings, improving the basis of knowledge from which the government designs public policy. There are also intellectual and pedagogical motives for conducting a reproduction of this study. Intellectually, a reproduction confirms the validity of the researchers’ spatial techniques; and pedagogically, conducting a reproduction allows students to see how geospatial studies are conducted and encourages students think critically about their reputability."
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#important-links",
    "href": "posts/COVID-19-spatial-accessibility/index.html#important-links",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "Important Links",
    "text": "Important Links\n\nOriginal publication\nOriginal code repository\nReproduction repository"
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#materials-and-methods",
    "href": "posts/COVID-19-spatial-accessibility/index.html#materials-and-methods",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "Materials and Methods",
    "text": "Materials and Methods\nThe Kang et al. study draws on four datasets:\n\nA hospital dataset provided by the Illinois Department of Health, which contains information about the number of ICU beds and ventilators at each hospital.\nA COVID-19 dataset also provided by the Illinois Department of Health, with information regarding the number of COVID-19 cases in each Zip Code in the state.\nA residential dataset from the 2018 American Community Survey 5 year table detailing the population and demographic composition of each tract in Illinois.\nA road network dataset queried from OpenStreetMap using the Python package, OSMnx.\n\nThe provided research notebook includes only the data for the City of Chicago, because it is computationally burdensome for users to conduct this reproduction on the entire state of Illinois. In order to deal with boundary issues (i.e. sometimes the fastest route to a hospital in Chicago uses streets outside the city), a past GEOG 323 class revised the original methodology, extending the road network 15 miles past the boundaries of Chicago. However, the population data provided by the authors contained information exclusively for the tracts within Chicago. Residents of the Chicago suburbs can, and likely do, take advantage of the services provided by the hospitals physically within the city. For this reason, we know that a more accurate analysis would incorporate population information of Chicago’s suburbs.\nIn our class’s reproduction of this analysis, we seek to remedy this issue by extending the pool of demographic information to include the tracts in all of the counties neighboring Cook county, which is the county where Chicago is located. We did not address the geographic extent of the COVID-19 case data.\nThe computational resources available for the original study and our reproduction included a CyberGIS server and the programming language Python. Specifically, the study was conducted in a Jupyter notebook using the virtual computing environment, CyberGISX, a cyberinfrastructure project which performs computations on a bank of supercomputers at the University of Illinois Urbana-Champaign. Required Python packages include numpy, pandas, geopandas, networkx, OSMnx, shapely, matplotlib, tqdm, and multiprocessing."
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#our-additions-to-the-code",
    "href": "posts/COVID-19-spatial-accessibility/index.html#our-additions-to-the-code",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "Our Additions to the Code",
    "text": "Our Additions to the Code\nTo address our concern that individuals who live outside of Chicago might also use the hospital services within Chicago, we reconfigured the pre-processing of residential data in order to include households in the suburbs. Once we adjusted the input data, we simply re-ran the analysis to generate new results. Since the road network in the Jupyter notebook already included roads within a 15 mile buffer, we knew that the network analysis would work with our new residential dataset. Furthermore, since the code joins the centroids of census tracts that intersect catchment areas, we know that including superfluous residential information in our input dataset will not bring superfluous residential information into our results; residences that are located outside of the catchment areas simply are not counted. For this reason, we extended the pool of demographic information drastically, such that it includes the tracts in all of the counties neighboring Cook county.\nOur additions to the code can be found in /procedure/code/04-Class-Reanalysis.ipynp under the “Population and COVID-19 Cases Data by County” subheading and our new figures are under /results/figures/reproduction and are copied below for convenience:\n# Load data for tract geometry\ntract_geom = gpd.read_file('./data/raw/public/ReanalysisClass/cb_2018_17_tract_500k.shp')\ntract_geom.head()\n\n# Load data for Census Demographics\ntract_dem = pd.read_csv('./data/raw/public/ReanalysisClass/real_data_census_illinois.csv', sep=\",\", skiprows = [1, 1])\ntract_dem.head()\n\n# Extract the following columns: S0101_C01_001E, S0101_C01_012E, S0101_C01_013E, S0101_C01_014E, S0101_C01_015E, S0101_C01_016E, S0101_C01_017E, S0101_C01_018E, S0101_C01_019E\nat_risk_csv = tract_dem[[\"GEO_ID\", \"NAME\", \"B01001_001E\", \"B01001_016E\", \"B01001_017E\", \"B01001_018E\", \"B01001_019E\", \"B01001_020E\", \"B01001_021E\", \"B01001_022E\", \"B01001_023E\", \"B01001_024E\", \"B01001_025E\", \"B01001_040E\", \"B01001_041E\", \"B01001_042E\", \"B01001_043E\", \"B01001_044E\"]]\n#Note: after a certain number of column names, atom becomes convinced that you're done with your code chunk. For this reason, I left out the last few columns in the code above, but they ought to be included when running the code.\n\n# Find the number of columns in dataframe\nlen(at_risk_csv.columns)\n\n# Sum all of the counts for individuals who are 50 years or older\nat_risk_csv['OverFifty'] = at_risk_csv.iloc[:, 3:23].sum(axis = 1)\n\n# create new pop column with a more useful name\nat_risk_csv['TotalPop'] = at_risk_csv['B01001_001E']\n\n# drop columns to clean the data set\nat_risk_csv = at_risk_csv.drop(at_risk_csv.columns[2:23], axis =1)\n\n# rename col to join\nnewnames = {\"GEO_ID\":\"AFFGEOID\"}\nat_risk_csv = at_risk_csv.rename(columns = newnames)\n\n# check projection\nprint(tract_geom.crs)\n\n# transform CRS\ntract_geom = tract_geom.to_crs(epsg=4326)\n\n# check projection\nprint(tract_geom.crs)\n\n# select tracts in Cook County and in counties adjacent to Cook County\ntract_geom = tract_geom.loc[(tract_geom[\"COUNTYFP\"] == '031')|\n                            (tract_geom[\"COUNTYFP\"] == '089')|\n                            (tract_geom[\"COUNTYFP\"] == '197')|\n                            (tract_geom[\"COUNTYFP\"] == '043')|\n                            (tract_geom[\"COUNTYFP\"] == '097')|\n                            (tract_geom[\"COUNTYFP\"] == '111')]\n\n# perform an inner join based on AFFGEOID columns\natrisk_data = tract_geom.merge(at_risk_csv, how='inner', on='AFFGEOID')"
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#results-and-discussion",
    "href": "posts/COVID-19-spatial-accessibility/index.html#results-and-discussion",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nUpon running the code with our updated residential dataset, we generated new figures, some of which differ significantly from the original results. Since we did not address the spatial extent of the COVID-19 case data, I am excluding COVID-19 accessibility maps from this report. The following maps reveal the accessibility of ICU beds and ventilators to vulnerable populations in Chicago. Darker blue represents higher spatial accessibility and lighter blue represents lower spatial accessibility.\n\nAccessibility of ICU Beds to Vulnerable Populations\n\nOriginal FigureUpdated Figure\n\n\n\n\n\nAccessibility of ICU Beds to vulnerable populations, as produced by GEOG 323 Spring 2021.\n\n\n\n\n\n\n\nAccessibility of ICU Beds to vulnerable populations, updated fall 2021.\n\n\n\n\n\nNote that the extents of the darker blues are much smaller in the updated figure, especially in the northwest and southwest. The new figure also includes a section of light blue in the northwest, which was a darker shade in the original figure. We will discuss these results further after reviewing the second set of figures.\n\n\nAccessibility of Ventilators to Vulnerable Populations\n\nOriginal FigureUpdated Figure\n\n\n\n\n\nAccessibility of ventilators to vulnerable populations, as produced by GEOG 323 Spring 2021.\n\n\n\n\n\n\n\nAccessibility of ventilators to vulnerable populations, updated fall 2021.\n\n\n\n\n\nSimilar to the first set of figures, the extents of the darker blues are smaller in the updated figure than in the original figure, especially in the northwest and southwest. The new figure also includes a section of light blue in the northwest, where the original figure had been darker.\nFor both ICU beds and ventilators, the original and updated figures are similar on the east side, but differ significantly in the northwest and southwest. These differences makes sense. Because Chicago borders Lake Michigan on the east and it takes time to drive to hospitals, including adjacent counties in the residential dataset will not increase the population accessing hospitals in Chicago’s east side. With a similar number of people accessing hospital services, the spatial accessibility of those services remains similar for individuals who live in eastern Chicago. However, our adjusted residential dataset does impact the accessibility of hospitals for residents in western, northern, and southern Chicago. The hospitals in these parts of Chicago are less isolated from suburban residents, as they can drive to these hospitals in less time. Incorporating those suburban residents into our analysis increases the perceived demand for hospital services in western, northern, and southern Chicago. With more people accessing hospital services, it is more difficult for any one individual to access those services, and the spatial accessibility measure mapped by our figures declines accordingly.\nThe differences between the original and updated figures highlight the inaccuracies that boundary effects introduce to Kang et al.’s results. In their paper, Kang et al. include hospitals within 15 miles of the city, but not the residents. Since residents outside of the city also use hospitals within the city, the authors appear to have neglected an important boundary effect. Our updated analysis accounts for this issue and illustrates that the surrounding populations significantly impact the spatial accessibility of healthcare resources within the city.\nIf you would like more information regarding the processes and results of this reproduction, please see my complete reproduction repository here."
  },
  {
    "objectID": "posts/COVID-19-spatial-accessibility/index.html#conclusions",
    "href": "posts/COVID-19-spatial-accessibility/index.html#conclusions",
    "title": "Spatial Accessibility of Healthcare Resources",
    "section": "Conclusions",
    "text": "Conclusions\nAt the end of the day, we were able to reproduce the study and make minor improvements to the code. This would not have been possible for a group of undergraduate students to accomplish in a couple of afternoons had we not been provided the Jupyter notebook on CyberGISX. The Jupyter notebook illustrates exactly how the authors addressed their research questions and provides some information as to the motivations for their choices, making it possible to review their code and methodology in a manner that is impossible for most research papers. Cudos to the authors for their foresight in publishing their work in a cutting-edge, reproducible environment.\nConducting the reproduction, however, also introduced me to the limitations and errors in their work. Discovering that their analysis of Chicago included hospitals but neglected the road networks and population outside of the city was surprising and somewhat eye-opening. The authors of the study are some of our top geospatial researchers, and they still made mistakes. If anything, this reproduction drew my attention to the importance of reproducing academic studies. All of us, even those at the top of the field, make mistakes, and a thorough peer review process is critical to addressing those errors.\nAnother key takeaway is that undocumented pre-processing of data poses significant barriers to reproducibility. While the authors performed some manipulations on their data simply to format it for the study, they do not document those manipulations in their code. For this reason, when we extended the geographic extent of the residential database, we had no model to work off of and had to develop our own method.\nOverall, Kang et al.’s study on spatial accessibility of COVID-19 healthcare resources is reproducible, and their Jupyter notebook on CyberGISX provides the public with all of the information necessary to computationally reproduce their analysis in Chicago. There are, however, a couple areas in which the work could be improved. Future work on the notebook could include documenting their data preprocessing and adding more comments to their code to make it easier to assess their methodology. Additionally, to account for the fact that residents outside of Chicago (which we added in this revision) could also access the hospitals outside of the city that are included in this analysis, the road network ought to be extended even further. Since hospital catchment areas are 30 minutes of driving time, extending the road network 60 miles past the boundary of Chicago would be adequate. Further than that distance, even an individual traveling at the maximum speed limit in a straight line would not be included in any catchment areas, so they would be irrelevant to our analysis. This Jupyter notebook is already an incredibly valuable tool for teaching and learning the methods of reproducible GIS, and continual work on the notebook will only continue to improve its functionality."
  },
  {
    "objectID": "posts/landcover-classification/index.html",
    "href": "posts/landcover-classification/index.html",
    "title": "High Resolution Landcover Classification",
    "section": "",
    "text": "The Lemon Fair Insect Control District is a municipal project of Bridport, Cornwall, and Weybridge seeking to control local mosquito populations. By applying larvicides in areas with high concentrations of mosquito eggs, the Lemon Fair Insect Control District reduces the presence of these pests and mitigates associated health risks. In this portfolio problem, we analyzed imagery of one section of the Lemon Fair River in order to assist the Lemon Fair Insect Control District with two tasks:\n\nIdentifying areas that ought to be targeted for mosquito prevention\nIdentifying the optimal source of imagery for this type of work"
  },
  {
    "objectID": "posts/landcover-classification/index.html#introduction",
    "href": "posts/landcover-classification/index.html#introduction",
    "title": "High Resolution Landcover Classification",
    "section": "",
    "text": "The Lemon Fair Insect Control District is a municipal project of Bridport, Cornwall, and Weybridge seeking to control local mosquito populations. By applying larvicides in areas with high concentrations of mosquito eggs, the Lemon Fair Insect Control District reduces the presence of these pests and mitigates associated health risks. In this portfolio problem, we analyzed imagery of one section of the Lemon Fair River in order to assist the Lemon Fair Insect Control District with two tasks:\n\nIdentifying areas that ought to be targeted for mosquito prevention\nIdentifying the optimal source of imagery for this type of work"
  },
  {
    "objectID": "posts/landcover-classification/index.html#image-acquisition-comparison",
    "href": "posts/landcover-classification/index.html#image-acquisition-comparison",
    "title": "High Resolution Landcover Classification",
    "section": "Image Acquisition & Comparison",
    "text": "Image Acquisition & Comparison\nIn this portfolio problem, I comparatively analyzed 4 sources of imagery of the Lemon Fair River: aerial photos from Vermont’s Open Data Portal, satellite images taken by Planet Labs, drone footage taken in 2021, and drone footage taken in 2022.\nI first downloaded images from VT Open Data Portal’s website. Using the Vermont Orthoimagery Finder, I found the most recent high resolution color imagery taken in April. On the main page, I selected the appropriate dataset and downloaded 6 adjacent images taken in April 2017, cross-referencing with the provided shapefile of the study area to ensure that the images covered the entire region.\nNext, I downloaded imagery from Planet Labs. To do so, I navigated to Planet Explorer, specified my area of interest by uploading a shapefile of the study area, and specified the date range of April 2021 using the left sidebar. From the list of suitable images and image collections, I chose a single image that covered the entire study site. This eliminated any preprocessing work associated with stitching images together, and Planet Labs actually clipped the provided image to my study area.\nThe other two images were generated by flying drones directly over the study site. The drone image from 2021 was generated and provided by Bill Hegman using a borrowed drone. The drone image from 2022 was generated using the Geography Department’s drone during last week’s lab, when Bill Hegman took our class to the study site for a hands-on field trip. After lab, Bill stitched the many images taken by the drone into one image. I downloaded both drone images directly from our class Canvas page.\nIn order to efficiently compare the value of my four image sources, I generated the following table displaying important characteristics of each image.\n\nTable 1. Spatial, spectral, and temporal characteristics of each source of imagery.\n\n\n\n\n\n\n\n\n\n\nSource\nSpatial Extent\nResolution\nYear\nBands\nFrequency of Imaging\n\n\n\n\nVT Open Data Portal\nSmall, but merged images to cover entire study area.\n30 cm\n2017\nRGB + NIR\nOnce every 5 years, when VT commissions a plane.\n\n\nPlanet Labs\nLarge. Came from Planet Labs clipped to study area.\n3 m\n2021\nRGB + NIR\nDaily (not always great quality)\n\n\nDrone 2021\nSmall. Does not cover study area.\n6 cm\n2021\nRGB + NIR + 8 more\nAs needed, with borrowed drone.\n\n\nDrone 2022\nEven smaller. Does not cover study area.\n2.1 cm\n2022\nRGB\nAs needed, easy because we own the drone.\n\n\n\nMy initial impression is that drones would work best for this work, because they offer the best spatial and spectral properties and if you own one, you can fly it whenever works best. While the aerial imagery from VT Open Data Portal has good technical specifications, they only photograph the region every 5 years. Flooded areas of standing water change from year to year depending on differing flood conditions, so aerial imagery would often fail to provide the most up-to-date information. While Planet Labs photographs the entire earth every day, their images are often covered by clouds and with a 3 meter pixel resolution, they might fail to pick up on small areas of standing water.\nLet’s see how each image source performs in a classification model before deciding which one is best-suited for the Lemon Fair Insect Control District."
  },
  {
    "objectID": "posts/landcover-classification/index.html#pre-processing-analysis-performed",
    "href": "posts/landcover-classification/index.html#pre-processing-analysis-performed",
    "title": "High Resolution Landcover Classification",
    "section": "Pre-Processing & Analysis Performed",
    "text": "Pre-Processing & Analysis Performed\nThe first step of my analysis was to create a new map in ArcGIS Pro and import all of my images into the project. Next, I merged the six VT Open Data Portal images together into one image suitable for classification using the mosaic to raster tool. At this point, I was ready to perform my analysis. Because Mosquitoes tend to lay their eggs in standing and slow-moving water, I chose to identify locations of still water, which most often appeared in my images as irrigation channels, ponds, and the flooding of the Lemon Fair.\nOne at a time, I selected each image in ArcGIS Pro and navigated to the “classification wizard”. I configured my classifier as a supervised, object-oriented analysis and modified the default schema to include only the classes relevant to each image, which wound up being some combination of standing water, farmland, trees with leaves, flowing water, trees without leaves, shrubs, and wetlands. After using the default parameters to generate the segmented image, I identified 20 training samples for each class. I selected the Support Vector Machine as my classifier, using all segment attributes in my model. I saved my classified dataset as a new layer, and I chose not to merge or reclassify any regions so that I could fairly compare the performance of each image’s classification."
  },
  {
    "objectID": "posts/landcover-classification/index.html#results-and-interpretation",
    "href": "posts/landcover-classification/index.html#results-and-interpretation",
    "title": "High Resolution Landcover Classification",
    "section": "Results and Interpretation",
    "text": "Results and Interpretation\nBelow, each classified layer is displayed next to its corresponding true color image, followed by a reflection on the strengths and/or weaknesses of that image source for classification. The legend next to the first set of maps applies to all classified figures.\n\n\n\nFigure 1. True color image from 2022 drone footage (left) and corresponding classified image (right).\n\n\nAs you can tell, the 2022 drone footage performed extremely poorly. Classification does not appear to be any better than random chance. Classification of regions is both blocky and pixelated, and no areas were correctly identified as standing water.\n\n\n\nFigure 2. True color image from 2021 drone footage (left) and corresponding classified image (right).\n\n\nIn stark contrast, the 2021 drone imagery performed remarkably well. Most ponds and irrigation channels were correctly classified as standing water. One aspect I found impressive was that flowing water was never erroneously classified as standing water; I, personally, would imagine that it would be difficult to distinguish between those two classes of water. That said, the model did make some errors. The L-shaped lake in the middle of the image was misclassified as flowing water, and some irrigation channels were misclassified as trees without leaves (although to be fair, there were trees without leaves along these irrigation channels). Overall, I am very impressed with the performance of this drone, especially juxtaposed with the drone from 2022.\n\n\n\nFigure 3. True color mosaicked image from VT Open Data (left) and corresponding classified image (right).\n\n\nThe VT Open Data image performed better than the 2022 drone footage, but worse than the 2021 drone footage. Unfortunately, several agricultural fields and shrub areas were misclassified as standing water. Perhaps they were relatively wet areas and thus shared similar spectral characteristics. Additionally, several sections of road and several sections of the Lemon Fair River are misclassified as standing water, perhaps because of object shapes similar to standing water, and one pond was misclassified as flowing water. That said, many irrigation channels and ponds were correctly classified, and this classification would be salvageable were I to go in with the reclassification tool to correct mistakes.\n\n\n\nFigure 4. True color image from Planet Labs (left) and corresponding classified image (right).\n\n\nFinally, despite its courser spatial resolution, the Planet Labs image performed better than the 2022 drone image. It correctly classified portions irrigation channels and some ponds, but failed to capture other parts. Additionally, one section of the forest in the southwest corner of the study region was misclassified as standing water, as well as parts of the flowing Lemon Fair River. While this image far outperformed the 2021 drone image, I believe that its 3m pixel resolution was inadequate to identify some narrow parts of irrigation channels."
  },
  {
    "objectID": "posts/landcover-classification/index.html#recommendations-conclusion",
    "href": "posts/landcover-classification/index.html#recommendations-conclusion",
    "title": "High Resolution Landcover Classification",
    "section": "Recommendations & Conclusion",
    "text": "Recommendations & Conclusion\nDespite performing an identical analysis on each image, the performance of each image differed greatly, ranging from results that look basically random to results that are highly accurate. The clear winner in terms of classification accuracy is the 2021 drone footage, which correctly identifies almost all instances of standing water.\nWith a pixel resolution of mere centimeters and 12 multispectral bands, the 2021 drone simply has the best technical specifications for the job, allowing it to pick up on small objects that are essentially invisible at coarser resolutions. Drones also have the benefit of versatility: if you own a drone, then you can fly it whenever you want, which is far preferable to chartering a plane.\nThat said, drones are not without their drawbacks. First, operating a drone requires some degree of technical expertise. Second, it can take a long time, and many batteries, to photograph a study site. And third, the technical specifications vary widely between drones, allowing some drones to perform far better than others (as we saw with the 2022 drone footage). My recommendation to the Lemon Fair Insect Control District is to carefully evaluate their drone options and select one that minimizes these drawback and works best for them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hey there! My name is Liam and I’m a recent graduate from Middlebury College with a B.A. in geography and mathematics. I currently work as a Data Science Intern at the Rocky Mountain Inventory & Monitoring Network, where I write code and develop R packages to manage long term ecological monitoring data. Other professional interests include remote sensing, cartography, conservation, and sustainability.\nI grew up in Rochester, NY with my four siblings, parents, and grandma, and I’m currently working remotely from Middlebury, VT. When I’m not at work or spending time with family, you can find me rock climbing, standing on my hands, or playing piano.\nThis website serves as my digital portfolio, displaying the products of my education and work experience. To get in touch, please drop me a line at 1liam1smith1@gmail.com."
  }
]