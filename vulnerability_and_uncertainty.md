---
layout: page
title: Vulnerability and Uncertainty
---
Introduction: mention *Vulnerability modeling for sub-Saharan Africa: An operationalized approach in Malawi*.

Aleatoric uncertainty refers to the inherent uncertainty in models, due to randomness of input parameters.
Epistemic uncertainty refers to an inadequate understanding of the process which one is modeling.
While aleatoric uncertainty is inevitable, epistemic uncertainty can be reduced with practices of good science and improved data.
Additionally, while aleatoric uncertainty impacts just the input of a vulnerability model, epistemic uncertainty propogates error throughout the entire research process.

**Stages of modeling:**
![Figure 1 from Tate's paper](assets/Index_construction_flowchart.png)

**Model structure:** in this stage, an index developer decides whether to follow a *deductive*, *hierarchical*, or *inductive* design.
*Deductive* designs are based on theory and typically include 10 or fewer normalized variables.
*Hierarchical* designs are similar; they typically include a greater number of indicators, which are also chosen based on theory, and which are grouped into subindices.
Both deductive and hierarchical models depend on expert opinion and critical thought, and there is no internal proof of validity.
*Inductive* designs are more robust, using data and statistical/regression analysis to decide which variables are more indicative of the outcome of interest.
Malcomb et al opt for a hierarchical design of 18 variables organized into three subindices as outlined below:
![Malcomb et al's Model Structure](assets/Malcomb_Model_Structure.png)

**Indicator set:** the way in which one selects their indicators greatly impacts the validity of their vulnerability index as certain methods are more likely to introduce bias and error than others.
*Deductive* methods select indicators based on theory; *normative* methods rely on value judgements and expert opinion; *inductive* methods rely on data and statistical analysis of real-world results; *non-substantial* methods rely on the characteristics of data alone; and *practical* methods select indicators based on what one can access and afford.
Malcomb et al rely heavily on the normative method, conducting an extensive process of interviews with experts and locals in Malawi.
In their paper, they also cite a number of studies about the validity of certain indicators, leading me to believe that they cross-checked the feedback from Malawians with academic work on the issue.
On one hand it makes sense to hear from locals and experts about the factors influencing their day-to-day lives, but on the other hand, using this as their principal method of indicator selection undoubtably introduced bias into their model.
A more robust method of indicator selection might involve a combination of the normative method and a data-driven method.
Malcomb et al appear to also take advantage of the practical method.
The reality is that data with sufficient spatial granularity is hard to come by in Sub-Saharan Africa.
For this reason, in order to find information on flood and drought risk in Malawi, the authors used a global dataset from UNEP/GRID.
The rasters are not even provided in a local projection and could likely be recreated with greater specificity.
The fact that they rely on 3 data sources for 18 variables is also indicative of the lack of comprehensive data in the area.
A more substantial search for indicators might have considered methods to create more detailed livelihood zones, flood rasters, and drought rasters oneself.

**Analysis scale:** this step occurs simultaneously with the previous step, and refers to selecting the geographic scale (like state, country, or county) on which the analysis will be conducted.
Of course, the relationships between indicators in different locations is subject to the Modifiable Areal Unit Problem, and it may not always make sense to aggregate data to larger scales.
Malcomb et al select an analysis level of Traditional Authority, a geographic scale for which they have data and one which makes sense in local context, because Traditional Authorities are often tasked with dealing with disaster incidents.
While it makes sense to conduct a vulnerability analysis at the appropriate government subdivision, the methodology that the authors use to transform all of their data to the Traditional Authority level introduces substantial error.
Of all of their data inputs, the one with the finest spatial granularity is one of the raster layers.
Unfortunately, to present their results as a raster in figure 5, the authors choose the dimensions of that finest layer as the spatial granularity for that figure.
That is a critical mistake.
One cannot take data for a larger area and arbitrarily decide that every point within that area has the same features as the area as a whole.
This ignores the nuance that in reality, every subdividision of an area is slightly different from each other.

**Measurement error:** this stage refers to the fact that error in the measurement of one's index may propogate through their vulnerability index calculations. **LOOK AT MALCOMB"S METADATA**

((((Malcomb et al conduct an extensive process of interviewing experts in order to identify the factors local to Malawi that make one most prone to climate change induced hazards.
In concept, their selection of indicator's seems reasonable, but when one digs into the metadata, it becomes apparent that their process was not perfect.
As it turns out, the "Exposition to Drought Events" indicator, which makes up 20% of the overall index, is not a measure of the likelihood of an area experiencing drought.
Rather, it has to do with the number of people in that area who might be exposed to drought, so it exaggerates the drought risk for cities and reduces the drought risk for rural areas.
**CITE and HASH OUT?**)))



**Transformation:** after measuring data, one must decide how to transform their data to be suitable for analysis.
One important question is whether vulnerability is better represented with total populations of  individuals at risk or relative proportions of individuals at risk.
There are a few different ways of calculating relative proportions of individuals at risk, which include percentage of the population and the density of at-risk individuals within a certain area.
Generally speaking, it is best practice to create these indices using relative proportions.
Using counts of people will inevitably exaggerate the vulnerability for urban areas, which tends to be misleading unless each area of consideration contains a similar numbers of residents.
Unfortunately, Malcomb et al fail to transform a number of their indicators before applying their weighted sum.
For example, they consider the number of orphans in each household, when it might be more meaningful to consider the proportion of each household consisting of orphans.

**Normalization:** vulnerability models typically bring many variables, which have different units, into one number which indicates a region's overarching vulnerability.
In order to adjust for the fact that different variables have different units and scales, one must normalize their indicators onto a common scale before considering them together.
One popular method of scaling is called linear scaling, which uses the following formula to assign every observation a value between 0 and 1: (observation - min)/(max - min).
Another method of scaling uses the z-score, calculated as (observation - mean)/(standard deviation).
Furthermore, some indicators in a vulnerability index may have a trend opposite of what is desired; that is, when their values increase, the vulnerability of an area decreases.
In order to prevent the effects of one indicator from cancelling out the effects of another indicator, such indicators with opposite trends ought to undergo a directionality adjustment before normalization.
Directionality adjustments typically involve multiplying by -1 or taking the reciprocal of each point.
Malcomb et al apply a normalization scheme of linear scaling and multiply that value by 5 in order to generate rankings for all indicators on a scale up to 5.
This is a valid methodology for quantitative variables; however, many of their indicators (sex of household head, the type of cooking fuel, the urban/rural setting, and whether the household has access to a radio or phone) are not quantitative.
How does one take a categorical or binary variable and convert it onto a scale from 0 to 5?
The process is nonsensical.
In the transformation section, the authors could have transformed these data to make them conform to a quantitative scale suitable for linear scaling.
For example, they could have determined the rate of households that own a radio.
Unfortunately, they neglected -- or at least failed to document how they addressed -- this issue.
Malcomb et al also provide no information about directionality adjustments.
It is quite possible that the authors accounted for the conflicting directionality of some of their variables (like wealth index score and electricity), but with no mention of that in their paper that is impossible for us to know and reproduce.

**Weighting:** perhaps the most subjective decision when designing a vulnerability index is selecting the weights to apply to each indicator.
Malcomb et al again apply a normative strategy for selecting weights, relying on interviews with Malawians to determine which factors most greatly impact one's vulnerability to climate change.
It is difficult to assess the validity of their weighting without having conducted the interviews in Malawi myself, but I would feel more comfortable with their weights had the authors conducted some sort of statistical analysis in order to select them.

**Aggregation:** this stage refers to the method one uses to combine all of their indicators into a single index.
The most common method of aggregation, and the method employed by Malcomb et al, is known as *additive aggregation*, which is implemented as a weighted sum of the indicators.
This method considers each indicator invidually, ignoring any interaction between indicators.
It is also considered a substitutable method, because an especially high value of one indicator can mask low values of other indicators.
*Multiplicative aggregation* offers a different methodology for aggregation, which seeks to address issues of interaction and substitution by multiplying each of the normalized indicators together instead of summing them.
There are several other methods of aggregation that attempt to address these issues as well.
It is unclear whether the authors of the paper considered the ways in which other methods of aggregation could have accounted for the shortcomings of additive aggregation.

**Uncertainty and sensitivity analysis validation:** this step seeks to identify and reduce uncertainty in the model's assumptions, but unfortunately, it is often neglected in the creation of vulnerability indices.
In Malcomb et al's paper, they do not address this issue mathematically.
Instead, they articulate how their index was developed based on the input of local experts in Malawi, attempting to build credibility through their extensive use of local interviews.
While it totally makes sense to use expert opinion in order to develop a vulnerability index, that alone is insufficient to validate one's methodology.
Monte Carlo simulations and statistical tests would go a long way towards validating their methodology and identifying any bias in the inclusion of their experts' opinions.


**References**
- Malcomb
- Tate
- Hinkel
